{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/header_18.png)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow y Keras\n",
    "\n",
    "TensorFlow es una biblioteca para cómputo simbólico sobre conjuntos de datos representados como tensores, que resulta ser de gran utilidad para diferentes técnicas de análisis de datos, destacándose las técnicas de aprendizaje automático como las redes neuronales.\n",
    "\n",
    "Keras, por otra parte, es una biblioteca para la implementación de redes neuronales profundas que utiliza bibliotecas de más bajo nivel como Tehano, Microsoft Cognitive Toolkit, PlaidML y TensorFlow. Keras ofrece una interfaz más amistosa al usuario, para el desarrollo de aplicaciones en redes neuronales profundas, que las que ofrecen las bibliotecas *backend*. \n",
    "\n",
    "A continuación, utilizamos Keras/TensorFlow para problemas de clasificación (con los datos de flores iris y los datos de diabetes de los indios Pima) y de regresión (con los datos del atractor de Lorenz)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjunto de datos de flores iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos de entrada:\n",
      "[[4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]]\n",
      "\n",
      "Valores de clase:\n",
      "[0 0 0 0 0 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#import os\n",
    "#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "iris_data = load_iris()\n",
    "\n",
    "X = iris_data['data']\n",
    "y = iris_data['target']\n",
    "names = iris_data['target_names']\n",
    "feature_names = iris_data['feature_names']\n",
    "\n",
    "print(\"Datos de entrada:\\n{}\\n\\nValores de clase:\\n{}\"\n",
    "      .format(X[45:55], y[45:55]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la preparación de los datos se utilizan dos procedimientos. En el caso de las variables de entrada ('*sepal length* (cm)', '*sepal width* (cm)', '*petal length* (cm)' y '*petal width* (cm)') se estandarizan los datos. En el caso de la salida, se trata de una variable categórica, esto es, la variable toma uno de un conjunto finito de valores (0-'*setosa*' 1-'*versicolor*' 2-'*virginica*'). Es conveniente utilizar una codificación 'OneHot', esto es, el valor categórico se representa mediante un vector cuya longitud corresponde al número de posibles valores de la variable categórica, con todas las posiciones en cero, excepto la que corresponde al índice del valor en el conjunto valores.\n",
    "\n",
    "<img src=\"images/onehot.png\" width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos de entrada:\n",
      "[[-1.26418478 -0.13197948 -1.34022653 -1.18381211]\n",
      " [-0.90068117  1.70959465 -1.22655167 -1.3154443 ]\n",
      " [-1.50652052  0.32841405 -1.34022653 -1.3154443 ]\n",
      " [-0.65834543  1.47939788 -1.2833891  -1.3154443 ]\n",
      " [-1.02184904  0.55861082 -1.34022653 -1.3154443 ]\n",
      " [ 1.40150837  0.32841405  0.53540856  0.26414192]\n",
      " [ 0.67450115  0.32841405  0.42173371  0.3957741 ]\n",
      " [ 1.2803405   0.09821729  0.64908342  0.3957741 ]\n",
      " [-0.41600969 -1.74335684  0.13754657  0.13250973]\n",
      " [ 0.79566902 -0.59237301  0.47857113  0.3957741 ]]\n",
      "\n",
      "Valores de clase:\n",
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Métodos de codificación y estandarización de datos\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Estandarizar los datos\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convertir cada elemento en una lista de un elemento con np.newaxis \n",
    "# y codificar tipo OneHot\n",
    "Y = OneHotEncoder().fit_transform(y[:, np.newaxis]).toarray()\n",
    "\n",
    "print(\"Datos de entrada:\\n{}\\n\\nValores de clase:\\n{}\"\n",
    "      .format(X_scaled[45:55], Y[45:55]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación dividimos los datos en subconjuntos de entrenamiento y prueba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Método para generar subconjuntos de datos de entrenamiento y prueba \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dividir los datos en subconjuntos de entrenamiento y prueba... \n",
    "# random_state=n para repetitibilidad\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_scaled, Y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La biblioteca *Keras* ofrece una buena interfaz para diseñar redes neuronales profundas en base a diferentes '*backends*'. En este caso utilizamos *TensorFlow*.\n",
    "\n",
    "El modelo más simple ofrecido por Keras es el *Sequential*, el cual permite definir redes neuronales *feedforward*. Para ello, se especifica la estructura de cada capa oculta, en forma secuencial hasta la capa de salida. El tipo más común de capa oculta es la capa 'totalmente conectada', que se define mediante la clase *Dense*. El constructor de esta clase recibe diversos parámetros, siendo los más importante el número de neuronas en la capa y la función de activación de estas neuronas. La capa de entrada, cuya única función es recibir las señales de entrada y transferirlas a la primera capa oculta, se especifica mediante el argumento 'input_dim=$n$' en la especificación de la primera capa oculta, siendo $n$ el número de variables de entrada. \n",
    "\n",
    "En la siguiente celda se construye una red neuronal *feedforward* completamente conectada, con una capa de entrada de 4 neuronas que recibirán las cuatro variables de entrada de cada instancia en el conjunto de datos. La capa de salida contará con 3 neuronas cada una responsable de identificar una clase (en un esquema *OneHot*). Adicionalmente, la red neuronal tendrá dos capas ocultas entre la capa de entrada y la capa de salida, cada una de ellas con 10 neuronas. En cada capa hay una neurona adicional que emite una señal de sesgo (*bias*). <br><br>\n",
    "\n",
    "<img src=\"images/neuron10.png\" width=500><br>\n",
    "\n",
    "Las neuronas en la capa de salida utilizarán la función de activación *softmax*, mientras que las neuronas en las capas internas utilizarán la función de activación *tanh*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Creación de un modelo \"en blanco\"\n",
    "model = Sequential([\n",
    "    Dense(10, input_dim=4, activation='tanh'),\n",
    "    Dense(10, activation='tanh'),\n",
    "    Dense(3, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez definida la estructura de la red neuronal, es necesario compilarlo. Como en el caso genérico de desarrollo de software, este es un paso de optimización que permite convertir la descripción previa en un conjunto de operaciones matriciales más adecuado para su ejecución. Los dos parámetros más importantes para compilar un modelo secuencial de red neuronal en Keras son la función objetivo ('*loss*') y el método de optimización del error ('*optimizer*'). El problema de clasificación de flores iris es un problema multiclase, por lo que se utiliza la función objetivo '*categorical_crossentropy*'. En cuanto al optimizador, una opción común es utilizar el método estocástico de descenso de gradiente ('*sgd*'). Otro argumento utilizado típicamente es '*metrics*', que parmite definir las métricas a utilizar durante el entrenamiento y la fase de prueba, en este caso se utilizará la exactitud ('*accuracy*'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 10)                50        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 33        \n",
      "=================================================================\n",
      "Total params: 193\n",
      "Trainable params: 193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compilación del modelo\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "# Descripción del modelo\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez compilado el modelo de red neuronal, se entrena la red con un conjunto de ejemplos específicos. el método '*fit*' permite especificar, además de los datos de entrada y los correspondientes valores objetivo, otros argumentos entre los cuales destaca '*epochs*', que especifica el número de iteraciones de entrenamiento sobre el total de los datos. Otro argumento útil es '*verbose*', que especifica si se deberán imprimir los resultdos después de cada época (1) o deberá realizarse le entrenamiento en silencio (0).\n",
    "\n",
    "En la siguiente celda se realiza el entrenamiento de la red neuronal definida anteriormente, utilizando 100 épocas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x10cc4a748>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenamiento del modelo \n",
    "model.fit(X_train, Y_train, epochs=100, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación utilizamos la red neuronal ya entrenada para clasificar los datos de prueba y observamos los resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salida de la red neuronal\n",
      "[[0.01208622 0.15381525 0.8340985 ]\n",
      " [0.06618087 0.79039335 0.14342585]\n",
      " [0.9170157  0.06742304 0.01556122]\n",
      " [0.0167618  0.18662404 0.7966141 ]\n",
      " [0.9121316  0.08154988 0.00631859]\n",
      " [0.00975841 0.07330248 0.9169391 ]\n",
      " [0.91727525 0.07563473 0.00708998]\n",
      " [0.10508515 0.3938408  0.5010741 ]\n",
      " [0.06096775 0.533455   0.40557727]\n",
      " [0.11669735 0.6787578  0.20454489]]\n"
     ]
    }
   ],
   "source": [
    "# Casificación de los datos de prueba\n",
    "Y_pred = model.predict(X_test)\n",
    "print(\"Salida de la red neuronal\\n{}\".format(Y_pred[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puede observarse, cada renglón de salida presenta 3 valores, de acuerdo a la codificación '*OneHot*'. Cada valor describe la probabilidad de que el vector pertenezca a cada una de las posibles clases. De acuerdo a lo deseado, una de las columnas presenta un valor cercano a 1 y las otras dos columns un valor cercano a cero, como puede verificarse redondeando los datos: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salida, redondeada, de la red neuronal\n",
      "[[0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Salida, redondeada, de la red neuronal\\n{}\"\n",
    "      .format(Y_pred.round()[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, se evalúa la calidad de la red neuronal, utilizando los datos de prueba y las herramientas ofrecidas por la biblioteca '*sklearn*'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       0.94      0.83      0.88        18\n",
      "           2       0.77      0.91      0.83        11\n",
      "\n",
      "   micro avg       0.91      0.91      0.91        45\n",
      "   macro avg       0.90      0.91      0.91        45\n",
      "weighted avg       0.92      0.91      0.91        45\n",
      "\n",
      "[[16  0  0]\n",
      " [ 0 15  3]\n",
      " [ 0  1 10]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Obtener el valor más alto (el resultado más probable)\n",
    "y_test_class = np.argmax(Y_test, axis=1)\n",
    "y_pred_class = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "print(classification_report(y_test_class, y_pred_class))\n",
    "print(confusion_matrix(y_test_class, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como era de esperar, el resultado de la clasificación es particularmente bueno. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjunto de datos de diabetes de los indios Pima\n",
    "\n",
    "El conjunto de datos de diabetes presenta una serie de complicaciones que dificultan la identificación de patrones en ellos. El primer problema es que se trata de un conjunto de datos reducido para la complejidad del problema (768 muestras en total). El segundo problema es la presencia de una gran cantidad de valores faltantes, enmascardos como ceros. En algunos casos, como las variables '*imc*', '*gl2h*', '*pad*' y '*ept*', la identificación de los valores en cero como valores faltantes es evidente, en otros casos es menos claro. La preparación de los datos para el entrenamiento de una red neuronal, en este caso, incluye una imputación de valores basados en la media para la variable '*imc*', '*gl2h*' y '*pad*' y eliminación de datos con valores en cero en '*ept*'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar los datos\n",
    "dfPID = pd.read_csv(\"Data sets/Pima Indian Data Set/pima-indians-diabetes.data\", \n",
    "                 names = ['emb', 'gl2h', 'pad', 'ept', 'is2h', 'imc', 'fpd', 'edad', 'class'])\n",
    "\n",
    "# Limieza de valores faltantes\n",
    "dfPID.loc[dfPID['ept'] == 0,'ept'] = np.nan\n",
    "dfPID.loc[dfPID['imc'] == 0,'imc'] = dfPID['imc'].mean()\n",
    "dfPID.loc[dfPID['gl2h'] == 0,'gl2h'] = dfPID['gl2h'].mean()\n",
    "dfPID.loc[dfPID['pad'] == 0,'pad'] = dfPID['pad'].mean()\n",
    "dfPID = dfPID.dropna()\n",
    "\n",
    "# Formar vectores de características y normalizar\n",
    "df_pure = dfPID[list(['emb', 'gl2h', 'pad', 'ept', 'is2h', 'imc', 'fpd', 'edad'])]\n",
    "XPID_scaled = StandardScaler().fit_transform(df_pure)\n",
    "\n",
    "# Valores de salida\n",
    "df_class = dfPID[list(['class'])].values.ravel()\n",
    "\n",
    "X_trainPID, X_testPID, y_trainPID, y_testPID = train_test_split(\n",
    "    df_pure, df_class, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso de los datos de diabetes de los indios Pima, existen dos clases (0-'*sano*' y 1-'*diabético*') que, por lo tanto, pueden reducirse a una sóla clase ('*sano*'): el individo pertenece a la clase o no. De esta manera, sólo se requiere una neurona en la capa de salida. La definición de un modelo de red neuronal con Keras puede construirse agregando capas de manera secuencial, en lugar de proporcionar todas las capas de una sola vez:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "modelPID = Sequential()\n",
    "modelPID.add(Dense(15, input_dim=8, activation='relu'))\n",
    "modelPID.add(Dense(10, activation='relu'))\n",
    "modelPID.add(Dense(5, activation='relu'))\n",
    "modelPID.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otro elemento que puede pre-definirse es el optimizador a utilizar en la compilación del modelo. Esto permite especificar parámetros de entrenamiento como la tasa de aprendizaje y el momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a3ceb5908>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar el módulo de optiización SGD\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Definición del optimizador\n",
    "sgd = SGD(lr=0.001, momentum=1.5e-4)\n",
    "\n",
    "# Compilación del modelo\n",
    "modelPID.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# Ajuste del modelo\n",
    "modelPID.fit(X_trainPID, y_trainPID, epochs=1500, verbose=0, batch_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, se realiza la evaluación de calidad del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179/179 [==============================] - 0s 2ms/step\n",
      "Exactitud: 73.18%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.75      0.80       130\n",
      "           1       0.51      0.67      0.58        49\n",
      "\n",
      "   micro avg       0.73      0.73      0.73       179\n",
      "   macro avg       0.68      0.71      0.69       179\n",
      "weighted avg       0.76      0.73      0.74       179\n",
      "\n",
      "[[98 32]\n",
      " [16 33]]\n"
     ]
    }
   ],
   "source": [
    "# Evalución del modelo\n",
    "scores = modelPID.evaluate(X_testPID, y_testPID)\n",
    "print (\"Exactitud: %.2f%%\" %(scores[1]*100))\n",
    "\n",
    "y_predPID = modelPID.predict(X_testPID).round().ravel()\n",
    "print(classification_report(y_testPID, y_predPID))\n",
    "print(confusion_matrix(y_testPID, y_predPID))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puede observarse que, aunque la exactitud global es razonablemente alta, al evaluar la calidad a través de la matriz de confusión, los resultados son menos atractivos, particularmente en lo que se refiere a la cantidad de falsos negativos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjunto de datos del atractor de Lorenz\n",
    "\n",
    "Para el análisis del atractor de Lorenz, se utilizarán datos calculados previamente y almacenados en un archivo de valores separados por coma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1970-01-01 00:00:00.000000000</th>\n",
       "      <td>12.629611</td>\n",
       "      <td>14.975338</td>\n",
       "      <td>-13.859515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-01 00:00:00.001000019</th>\n",
       "      <td>12.655508</td>\n",
       "      <td>15.487868</td>\n",
       "      <td>-13.630292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-01 00:00:00.002000039</th>\n",
       "      <td>12.686236</td>\n",
       "      <td>15.998124</td>\n",
       "      <td>-13.394778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-01 00:00:00.003000060</th>\n",
       "      <td>12.721724</td>\n",
       "      <td>16.506214</td>\n",
       "      <td>-13.152910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-01 00:00:00.004000079</th>\n",
       "      <td>12.761905</td>\n",
       "      <td>17.012235</td>\n",
       "      <td>-12.904620</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       X          Y          Z\n",
       "1970-01-01 00:00:00.000000000  12.629611  14.975338 -13.859515\n",
       "1970-01-01 00:00:00.001000019  12.655508  15.487868 -13.630292\n",
       "1970-01-01 00:00:00.002000039  12.686236  15.998124 -13.394778\n",
       "1970-01-01 00:00:00.003000060  12.721724  16.506214 -13.152910\n",
       "1970-01-01 00:00:00.004000079  12.761905  17.012235 -12.904620"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cargar los datos del atractor de Lorenz, con la opción 'index_col=0' \n",
    "# para inidicar que la primera columna es el índice de los datos y\n",
    "# 'parse_dates=True' para indicar identificar valores tipo fecha\n",
    "lorenz_df = pd.read_csv('Data sets/Lorenz.csv', index_col=0, parse_dates=True)\n",
    "display(lorenz_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La forma más usual de analizar los datos del atractor de Lorenz es mediante el análisis de series de tiempo. En este caso, lo que se busca es predecir el valor de la variable de interés (típicamente, sólo se utilizan como entrada al modelo los valores previos de la propia variable, por lo tanto, en el caso del sistema de Lorenz se analiza sólo una componente):\n",
    "\n",
    "<img src=\"images/neuron12.png\" width=\"50%\"> <br>\n",
    "\n",
    "En las siguientes celdas se buscará predecir datos de la componente 'Z' del sistema de datos, siguiendo el enfoque típico de análisis de series de tiempo. \n",
    "\n",
    "El primer paso es separar este conjunto de datos en datos de entrenamiento y datos de prueba: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Establecer un punto de separación de datos de entrenamiento y datos de prueba\n",
    "split_date = pd.Timestamp('1970-01-01 00:00:40')\n",
    "\n",
    "train = lorenz_df.Z.loc[:split_date]\n",
    "test = lorenz_df.Z.loc[split_date:]\n",
    "\n",
    "plt.figure(figsize=(12, 3))\n",
    "ax = train.plot()\n",
    "test.plot(ax=ax)\n",
    "plt.legend(['train', 'test']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, es conveniente cambiar la escala de los valores en el conjunto de datos al rango [-1, 1]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "train_sc = scaler.fit_transform(train.values.reshape(-1, 1))\n",
    "test_sc = scaler.transform(test.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo del análisis de series de tiempo, es realizar la predicción de una variable en términos de un conjunto de datos previos. Las técnicas convencionales de reconocimiento de patrones requieren recibir como entradas las \"variables dependientes\", esto es las variables que determinan el valor de la variable independiente. De manera que, para utilizar estas técnicas sin modificación, es necesario generar un conjunto de 'casos', cada uno con un vector de entrada y un valor de salida. El vector de entrada se construye tomando una ventana de valores previos al valor objetivo, de manera que, si el valor a predecir es $y = z(t)$, entonces el vector de entrada es $\\vec X = \\left(z(t-n), \\ldots z(t-2), z(t-1) \\right)$, siendo $n$ el número de datos previos a considerar:\n",
    "\n",
    "<img src=\"images/neuron13.png\" width=\"90%\"> <br>\n",
    "\n",
    "En la siguiente celda se construyen los casos de entrenamiento, seleccionando los valores de salida y los vectores de entrada, a partir de la serie de tiempo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamaño de la ventana de datos previos\n",
    "win_size = 12\n",
    "\n",
    "train_size = len(train_sc) - win_size\n",
    "X_train = np.array([train_sc[i:i+win_size].ravel() for i in range(train_size) ])\n",
    "y_train = np.array([train_sc[i+win_size] for i in range(train_size) ]).ravel()\n",
    "\n",
    "test_size = len(test_sc) - win_size\n",
    "X_test = np.array([test_sc[i:i+win_size].ravel() for i in range(test_size) ])\n",
    "y_test = np.array([test_sc[i+win_size] for i in range(test_size)]).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La creación del modelo de red neuronal es similar al utilizado en los casos previos, utilizando el tamaño de la ventana de datos previos como longitud de la entrada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "# create the model\n",
    "modelLorenz = Sequential()\n",
    "modelLorenz.add(Dense(12, input_dim=win_size, activation='relu'))\n",
    "modelLorenz.add(Dense(18, activation='relu'))\n",
    "modelLorenz.add(Dense(9, activation='relu'))\n",
    "modelLorenz.add(Dense(3, activation='relu'))\n",
    "modelLorenz.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "modelLorenz.compile(loss='mean_squared_error', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La *detención temprana* es una técnica de regularización cuyo objetivo es evitar el sobreentrenamiento de un método de aprendizaje automático, como las redes neuronales. Este método realiza un proceso de monitoreo sobre algún indicador de calidad y detiene el entrenamiento cuando este indicador empieza a degradarse. El monitoreo puede realizarse sobre los datos de entrenamiento o sobre un conjunto de datos de evaluación.\n",
    "\n",
    "En este ejemplo, se utiliza una condición de detención temprana basada en el error cuadrático medio sobre datos de evaluación ('*val_mse*'). De acuerdo a los parámetros establecidos, el algoritmo de entrenamiento se detendrá después de 5 épocas sin observar mejora en el '*val_mse*' (*patience=5*) y el algoritmo de entrenamiento deberá emplear el 30% de los datos para esta validación ('validation_split=0.3'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Detener el entrenamiento después de n épocas sin mejora\n",
    "early_stop = EarlyStopping(monitor='val_mse', patience=5, verbose=1)\n",
    "\n",
    "history = modelLorenz.fit(X_train, y_train, epochs=500, batch_size=10, verbose=0, \n",
    "                          callbacks=[early_stop], validation_split=0.3, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, para evaluar la calidad del modelo final, se utiliza el indicador $R^2$ o *coeficiente de determinación*. El coeficiente $R^2$ es una medida estadística sobre la calidad de la aproximación obtenida por un algoritmo de regresión, con respecto a los valores reales. Un valor de $R^2=1$ indica que las predicciones se ajustan perfectamente a los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeficiente de determinación R2 en el conjunto de datos: 0.530\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y_pred = modelLorenz.predict(X_test).ravel()\n",
    "print(\"Coeficiente de determinación R2 en el conjunto de datos: {:0.3f}\"\n",
    "      .format(r2_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
