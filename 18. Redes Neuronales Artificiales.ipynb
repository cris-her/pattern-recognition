{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/header_18.png)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Técnicas de clasificación: Redes Neuronales\n",
    "\n",
    "\n",
    "### Las metáforas en aprendizaje automático\n",
    "**<font style=\"color:#0011d2\">Metáfora</font>**\n",
    "<p style=\"background-color:#e8e8e8; color:#008100\">Del lat. metaphŏra, y este del gr. μεταφορά metaphorá.\n",
    "\n",
    "1. <font style=\"color:#006ec3\">f. *Ret.*</font> Traslación del sentido recto de una voz a otro figurado, en virtud de una comparación tácita, como en las perlas del rocío, la primavera de la vida o refrenar las pasiones. ([RAE](http://dle.rae.es/?id=P4sce2c))<br>\n",
    "\n",
    "Las metáforas son uno de los recursos más utilizados como figuras literarias. Una metáfora se refiere a un significado o a la identidad atribuida a un sujeto por medio de otro. ([Figuras Literarias](http://figurasliterarias.org/content/metáfora))<br>\n",
    "\n",
    "![](images/metaphors.jpg)<br>\n",
    "\n",
    "El uso de una metáfora no se entiende como una comparación fiel, en un sentido directo. <br>\n",
    "\n",
    "En el caso de **aprendizaje automático**, se utilizan metáforas con diversos sistemas naturales/sociales para desarrollar sistemas artificiales cuyo funcionamiento recuerda \"*de alguna manera*\" al funcionamiento de sus contrapartes metafóricas.<br>\n",
    "\n",
    "El uso de metáforas para el desarrollo de sistemas artificiales se originó en el trabajo de Arturo Rosenblueth y Norbert Wiener. Su colaboración dió origen a la cibernética y, de ahí, a la inteligencia artificial. Algunas referencias importantes que documentan esta colaboración son:\n",
    "\n",
    "* [Behavior, Purpose and Teleology](http://cleamc11.vub.ac.be/Books/Wiener-teleology.pdf), 1943; por Arturo Rosenblueth, Norbert Wiener y Julian Bigelow. El documento que inició todo.\n",
    "* [Arturo Rosenblueth y Norbert Wiener: dos científicos en la historiografía de la educación contemporánea](http://cleamc11.vub.ac.be/Books/Wiener-teleology.pdf)\n",
    "* [Coloquio: Norbert Wiener y Arturo Rosenblueth: un encuentro interdisciplinario](http://computo.ceiich.unam.mx/webceiich/deptoDifusion/32PreMed/12-05-B2.pdf)\n",
    "* [Analogías entre hombre y máquina. El Grupo Cibernética y algunas de sus ideas fundacionales](http://www.con-temporanea.inah.gob.mx/node/42)\n",
    "* [interview: Heinz von Foerster](https://web.stanford.edu/group/SHR/4-2/text/interviewvonf.html)\n",
    "* [Perspectives on the history of the cybernetics movement: The path to current research through the contributions of Norbert Wiener, Warren McCulloch, and John von Neumann](http://www.univie.ac.at/constructivism/archive/fulltexts/3763.html)\n",
    "\n",
    "![](images/rosenblueth-wiener.jpg)<br>\n",
    "\n",
    "El primer resultado importante del uso de metáforas biológicas para el desarrollo de sistemas automatizados fue la Neurona de McCulloch-Pitts que daría origen a las redes neuronales. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neurona biológica\n",
    "\n",
    "La neurona es la célula base del sistema nervioso. Su característica más importante es la excitabilidad eléctrica de su membrana plasmática. Esta peculiaridad le ha permitido desarrollar una especialización en la recepción de estímulos eléctricos y la transmisión de esta información en forma de impulsos eléctricos. Los estímulos pueden provenir de otras neuronas o de otros tipos de células, por ejemplo células sensoriales. El impulso nervioso es envido a otras neuronas o hacia otros tipos de células, por ejemplo, hacia las fibras musculares. <br><br>\n",
    "\n",
    "![](images/Complete_neuron_cell_diagram_es.png)<br>\n",
    "\n",
    "Podemos simplificar la estructura de la neurona de la siguiente manera:<br>\n",
    "\n",
    "![](images/neuron0.png)\n",
    "\n",
    "La membrana celular juega un papel muy importante en la funcionalidad de la neurona. Como en todas la células vivas, la membrana tiene una función estructural al separar físicamente el citoplasma y sus componentes intracelulares del ambiente extracelular. \n",
    "\n",
    "<img src=\"images/0312_Animal_Cell_and_Components.jpg\" width=400>\n",
    "\n",
    "La membrana también juega un papel importante en la nutrición de la célula debido a que es selectivamente permeable y, por lo tanto, es capaz de regular lo que entra y sale de la célula. \n",
    "\n",
    "Esta separación de componentes tiene un efecto colateral interesante: dado que los materiales que entran y salen de la célula son típicamente iones y dado que la membrana es selectivamente permeable, es usual encontrar una diferencia de potencial eléctrico dentro y fuera de la célula, lo que se conoce como *potencial en reposo*.\n",
    "\n",
    "<img src=\"images/11-08_RestMemPoten.jpg\" width=600>\n",
    "\n",
    "La característica morfológica más distintiva de una neurona como célula, es el conjunto de \"extensiones\" de su membrana: las dendritas y los axones. Las dendritas tienen la función de recibir los estímulos provenientes de otras neuronas en forma de neurotransmisores químicos y convertirlos en potenciales eléctricos en la membrana.\n",
    "\n",
    "<img src=\"images/synapsis.png\" width=600>\n",
    "\n",
    "<img src=\"images/640px-Neurotransmitters.jpg\" width=600>\n",
    "\n",
    "Cuando el potencial eléctrico en la membrana receptora es suficientemente alto, es decir, cuando se alcanza un cierto umbral de excitación en la membrana, se modifica la permeabilidad de ésta provocando un flujo de iones hacia adentro y afuera de la célula. \n",
    "\n",
    "Este movimiento de iones a través de la membrana genera una corriente eléctrica que a su vez provoca un tren de cambios locales de permeabilidad a lo largo de todo el axón. \n",
    "\n",
    "<img src=\"images/action_potential.jpg\" height=200px>\n",
    "\n",
    "Este tren de *potenciales de acción* constituye el impulso eléctrico que, de esta manera, \"viaja\" desde el cuerpo celular hasta las *sinapsis* en las dendritas terminales en el axón. \n",
    "\n",
    "![](images/neuron1.png)\n",
    "\n",
    "Al final del recorrido, las señales eléctricas provocan la liberación de substancias químicas (los neurotransmisores) que tendrán la función de excitar las dendritas de las neuronas vecinas, logrando de esta manera que la señal viaje a lo largo de un determinado camino por el sistema nervioso.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neurona de McCulloch-Pitts\n",
    "\n",
    "El primer modelo de neurona artificial fue propuesto por [Warren S. McCulloch](https://en.wikipedia.org/wiki/Warren_Sturgis_McCulloch) (neurocientífico) y [Walter Pitts](https://en.wikipedia.org/wiki/Walter_Pitts) (matemático) en 1943 en el artículo \"[A logical calculus of the ideas immanent in nervous activity](http://www.minicomplexity.org/pubs/1943-mcculloch-pitts-bmb.pdf)\". Esta neurona es denominada **neurona de McCulloch-Pitts**, *neuronas MCP*, o *unidad/compuerta lógica de umbral*.\n",
    "\n",
    "![](images/mcp-neuron.png)\n",
    "\n",
    "Este modelo implementa la metáfora neuronal de la siguiente manera:\n",
    "\n",
    "1. Las señales provenientes del ambiente o de otras neuronas se modelan como el conjunto de entradas $\\{x_1, x_2, \\ldots, x_n\\}$. \n",
    "2. Cada una de las señales de entrada ejerce un estímulo en la neurona que depende de la actividad sináptica, la que a su vez codifica qué tanta importancia da la neurona a una determinada señal. En el modelo, esta importancia se representa mediante el conjunto de *pesos sinápticos* $\\{w_1, w_2, \\ldots, w_n\\}$.\n",
    "3. Los estímulos provenientes de cada entrada (la señal ponderada por el peso), son integrados en el cuerpo celular. Dado que, en este caso, se ignoran los estados preliminares de la neurona, esta suma ponderada representa la activación de la neurona en el tiempo actual. Así, la activación de la neurona $j$ en el tiempo $t$ está dada por: $$a_j(t) = \\sum_{i=1}^n w_i x_i$$\n",
    "4. La salida de la neurona es un proceso \"todo o nada\": si la activación supera un cierto valor umbral $\\theta$ la neurona se activa y transmite un impulso (salida $y=1$), en caso contrario no *dispara*. De esta manera, la salida de la neurona $j$ en el tiempo $t$ está dada por: <br>\n",
    "$$y_j(t) = \n",
    "\\begin{cases} \n",
    "0 \\quad \\text{ if } \\quad a_j(t) < \\theta\\\\ \n",
    "1 \\quad \\text{ if } \\quad a_j(t) \\ge \\theta \n",
    "\\end{cases}\n",
    "$$<br>\n",
    "lo cual puede reescribirse como <br>\n",
    "$$y_j(t) = \n",
    "\\begin{cases} \n",
    "0 \\quad \\text{ if } \\quad \\mathbf{w}_j\\cdot \\mathbf{x}_j(t) < \\theta\\\\ \n",
    "1 \\quad \\text{ if } \\quad \\mathbf{w}_j\\cdot \\mathbf{x}_j(t) \\ge \\theta \n",
    "\\end{cases}\n",
    "$$<br>\n",
    "siendo $\\mathbf{w}_j$ y $\\mathbf{x}_j(t)$ el vector de pesos (constante) de la neurona $j$ y el vector de entrada a la neurona en el tiempo $t$.\n",
    "\n",
    "La neurona de McCulloch-Pitts puede, entonces, representarse de la siguiente manera:\n",
    "\n",
    "![](images/neuron2.png)\n",
    "\n",
    "El problema ahora se reduce a escoger adecuadamente los pesos $w_i$.\n",
    "\n",
    "Originalmente, sin un procedimiento adecuado para ajustar los pesos sinápticos, particularmente para neuronas en una red, la neurona de McCulloch-Pitts era apenas una curiosidad y un primer resultado que validaba las posibilidades del uso de metáforas para el desarrollo de sistemas artificiales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regla de Hebb\n",
    "\n",
    "La [teoría Hebbiana](http://lcn.epfl.ch/~gerstner/PUBLICATIONS/Gerstner-Plasticity2011.pdf) propuesta por Donald Hebb en 1949 ofrece una explicación del aprendizaje en términos de la adaptación de las neuronas durante su actividad. De acuerdo con Hebb, una actividad o señal que se repite regularmente y que tiene un efecto establizador en la célula, tiende a fortalecerse, transformando una memoria de corto plazo en un *engrama*, esto es, una estructura de interconexión neuronal estable.\n",
    "\n",
    "Hebb describe este proceso de la siguiente manera: \n",
    "\n",
    ">When an axon of cell A is near enough to excite a cell B and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both cells such that A's efficiency, as one of the cells firing B, is increased.\n",
    "\n",
    "Es decir, la *importancia* que la neurona B presta a las señales provenientes de la neurona A se intensifica con cada interacción exitosa. Esta teoría recibe también el nombre de Regla de Hebb o teoría de la Sinápsis Hebbiana. \n",
    "\n",
    "Esta regla puede expresarse, matemáticamente de la siguiente manera: Supongamos que $y_k$ es la salida de la neurona postsináptica $k$ dada la señal $x_{i}$ proveniente de la neurona presináptica $i$, entonces, si $\\eta$ es la tasa de aprendizaje, la actualización del peso que la neurona $k$ da a la entrada proveniente de la neurona $i$ está dada por:\n",
    "\n",
    "$$\\Delta w_{ki}=\\eta x_{i} y_k$$ \n",
    "\n",
    "![](images/neuron3.png)\n",
    "\n",
    "La regla de Hebb, utilizada para actualizar los pesos de la neurona de McCulloch-Pitts, abre la puerta a sistemas neuronales efectivos.\n",
    "\n",
    "\n",
    "### El perceptrón\n",
    "\n",
    "El **[perceptrón](https://en.wikipedia.org/wiki/Perceptron)** es un modelo de neurona obtenido al combinar el modelo de la neurona de McCulloch-Pitts con una extensión de la regla de Hebb. La regla de entrenamiento consta de los siguientes pasos:\n",
    "\n",
    "1. Inicializar los pesos con valores aleatorios pequeños.\n",
    "2. Para cada vector de entrenamiento $\\mathbf{x}_j$ con valor de clase $y_j$ (el valor de salida esperado): \n",
    "    1. Calcular el valor de salida  $y^*_j$ de acuerdo con la función de activación\n",
    "    2. Actualizar los pesos de acuerdo a la regla de Hebb modificada:<BR>\n",
    "    $$\\Delta \\mathbf{w} = \\eta (y_j - y^*_j) \\mathbf{x}_j$$\n",
    "    \n",
    "\n",
    "A continuación mostramos el algoritmo de entrenamiento del perceptrón. Analizamos el caso de clasificación de los datos de la flor Iris. Dado que el preceptrón tiene sólo dos salidas (0 y 1). sólo podemos clasificar en dos clases por lo que, para este ejercició, utilizaremos sólo los primeros 100 datos del conjunto de datos *Iris Data Set*. Utilizamos los primeros 50 datos (revueltos) para entrenar el perceptrón, imprimimos sólo los casos donde hubo error y la consecuente modificación de los pesos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   target  \n",
       "0     0.0  \n",
       "1     0.0  \n",
       "2     0.0  \n",
       "3     0.0  \n",
       "4     0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Creación de un Dataframe\n",
    "iris_data = load_iris()\n",
    "iris_df= pd.DataFrame(data= np.c_[iris_data['data'], iris_data['target']],\n",
    "                      columns= iris_data['feature_names'] + ['target'])\n",
    "display(iris_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etiqueta de clase de cada vector ejemplo\n",
    "Y = iris_df.iloc[50:, 4].values\n",
    "\n",
    "# Vector de características\n",
    "X = iris_df.iloc[50:, 0:4].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector\tTarget\toutput\terror\tw\n",
      "5 \t 2.0 \t 1 \t 1.0 \t [0.064 0.027 0.053 0.019]\n",
      "7 \t 2.0 \t 1 \t 1.0 \t [0.131 0.06  0.11  0.04 ]\n",
      "8 \t 2.0 \t 1 \t 1.0 \t [0.199 0.092 0.169 0.063]\n",
      "9 \t 2.0 \t 1 \t 1.0 \t [0.268 0.124 0.226 0.086]\n",
      "11 \t 2.0 \t 1 \t 1.0 \t [0.326 0.152 0.277 0.11 ]\n",
      "12 \t 2.0 \t 1 \t 1.0 \t [0.393 0.182 0.329 0.133]\n",
      "14 \t 2.0 \t 1 \t 1.0 \t [0.453 0.204 0.379 0.148]\n",
      "15 \t 2.0 \t 1 \t 1.0 \t [0.52  0.235 0.435 0.172]\n",
      "16 \t 2.0 \t 1 \t 1.0 \t [0.582 0.269 0.489 0.195]\n",
      "17 \t 2.0 \t 1 \t 1.0 \t [0.645 0.297 0.54  0.21 ]\n",
      "18 \t 2.0 \t 1 \t 1.0 \t [0.714 0.328 0.594 0.231]\n",
      "20 \t 2.0 \t 1 \t 1.0 \t [0.771 0.353 0.644 0.251]\n",
      "22 \t 2.0 \t 1 \t 1.0 \t [0.843 0.389 0.705 0.276]\n",
      "26 \t 2.0 \t 1 \t 1.0 \t [0.92  0.417 0.772 0.296]\n",
      "29 \t 2.0 \t 1 \t 1.0 \t [0.993 0.446 0.835 0.314]\n",
      "30 \t 2.0 \t 1 \t 1.0 \t [1.07  0.476 0.896 0.337]\n",
      "31 \t 2.0 \t 1 \t 1.0 \t [1.135 0.506 0.951 0.355]\n",
      "32 \t 2.0 \t 1 \t 1.0 \t [1.204 0.537 1.002 0.378]\n",
      "33 \t 2.0 \t 1 \t 1.0 \t [1.264 0.567 1.05  0.396]\n",
      "34 \t 2.0 \t 1 \t 1.0 \t [1.325 0.597 1.099 0.414]\n",
      "35 \t 2.0 \t 1 \t 1.0 \t [1.39  0.627 1.151 0.434]\n",
      "36 \t 2.0 \t 1 \t 1.0 \t [1.455 0.659 1.202 0.454]\n",
      "40 \t 2.0 \t 1 \t 1.0 \t [1.519 0.687 1.258 0.476]\n",
      "41 \t 2.0 \t 1 \t 1.0 \t [1.595 0.717 1.324 0.497]\n",
      "42 \t 2.0 \t 1 \t 1.0 \t [1.644 0.742 1.369 0.514]\n",
      "44 \t 2.0 \t 1 \t 1.0 \t [1.707 0.771 1.425 0.532]\n",
      "46 \t 2.0 \t 1 \t 1.0 \t [1.774 0.804 1.482 0.557]\n",
      "47 \t 2.0 \t 1 \t 1.0 \t [1.838 0.836 1.535 0.58 ]\n",
      "50 \t 2.0 \t 1 \t 1.0 \t [1.909 0.866 1.594 0.601]\n",
      "Pesos finales:  [1.909 0.866 1.594 0.601]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Datos para entrenamiento y prueba\n",
    "train_data, test_data, train_y, test_y = train_test_split(X, Y, test_size=.5)\n",
    "\n",
    "# Tasa de aprendizaje\n",
    "eta=0.01\n",
    "\n",
    "# Vector de pesos inicial\n",
    "w = np.zeros(X.shape[1])\n",
    "\n",
    "# Entrenamiento\n",
    "print(\"Vector\\tTarget\\toutput\\terror\\tw\")\n",
    "vector = 1\n",
    "for xi, target in zip(train_data, train_y) :\n",
    "    activation = np.dot(xi, w)\n",
    "    output = np.where(activation >= 0.0, 1, 0)\n",
    "    error = target - output\n",
    "    w += eta * error * xi\n",
    "    if(error != 0):\n",
    "        print(vector, \"\\t\", target, \"\\t\", output, \"\\t\", error, \"\\t\", w)\n",
    "    vector += 1\n",
    "    \n",
    "print (\"Pesos finales: \", w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es importante notar que los valores finales de los pesos dependerá de sus valores iniciales y de la secuencia en que se van presentando los vectores de entrenamiento. En la siguiente tabla se presentan los pesos finales para diversas corridas:\n",
    "\n",
    "Corrida | Pesos finales  \n",
    "----| ----|\n",
    "1   | [-0.012 -0.044  0.078  0.035]\n",
    "2   | [-0.022 -0.052  0.076  0.036]\n",
    "3   | [-0.014 -0.057  0.081  0.033]\n",
    "4   | [-0.020 -0.046  0.064  0.027]\n",
    "5   | [-0.017 -0.049  0.074  0.028]\n",
    "\n",
    "Los valores específicos de los pesos carecen de importancia y lo importante es el desempeño del clasificador.\n",
    "\n",
    "A continuación utilizamos los siguientes 50 vectores para probar la eficacia del entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 vectores mal clasificados de 50 (42.0%)\n"
     ]
    }
   ],
   "source": [
    "# Prueba\n",
    "errores = 0\n",
    "for xi, target in zip(test_data, test_y) :\n",
    "    activation = np.dot(xi, w)\n",
    "    output = np.where(activation >= 0.0, 1, 0)\n",
    "    if (target != output) :\n",
    "        errores += 1\n",
    "print(\"{} vectores mal clasificados de {} ({}%)\".format(errores, len(test_data), \n",
    "                                                        errores/len(test_data)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repetimos el experimento, ahora con los datos de diabetes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Importar los datos\n",
    "dfPID = pd.read_csv(\"Data sets/Pima Indian Data Set/pima-indians-diabetes.data\", \n",
    "                 names = ['emb', 'gl2h', 'pad', 'ept', 'is2h', 'imc', 'fpd', 'edad', 'class'])\n",
    "\n",
    "# Eliminar datos con valores faltantes\n",
    "dfPID.loc[dfPID['pad'] == 0,'pad'] = np.nan\n",
    "dfPID.loc[dfPID['ept'] == 0,'ept'] = np.nan\n",
    "dfPID.loc[dfPID['is2h'] == 0,'is2h'] = np.nan\n",
    "dfPID.loc[dfPID['imc'] == 0,'imc'] = np.nan\n",
    "dfPID = dfPID.dropna()\n",
    "\n",
    "# Formar vectores de características y normalizar\n",
    "df_pure = dfPID[list(['emb', 'gl2h', 'pad', 'ept', 'is2h', 'imc', 'fpd', 'edad'])]\n",
    "std_data = StandardScaler().fit_transform(df_pure)\n",
    "df_pure = pd.DataFrame(std_data)\n",
    "\n",
    "# Valores de salida\n",
    "df_class = dfPID[list(['class'])].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 vectores mal clasificados de 130 (28.46153846153846%)\n",
      "\n",
      "Reporte de clasificación\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.69      0.77        89\n",
      "           1       0.53      0.78      0.63        41\n",
      "\n",
      "   micro avg       0.72      0.72      0.72       130\n",
      "   macro avg       0.70      0.73      0.70       130\n",
      "weighted avg       0.76      0.72      0.73       130\n",
      "\n",
      "\n",
      "Matriz de confusión\n",
      " [[61 28]\n",
      " [ 9 32]]\n"
     ]
    }
   ],
   "source": [
    "X_trainPID, X_testPID, y_trainPID, y_testPID = train_test_split(\n",
    "    df_pure.values, df_class, test_size=0.33)\n",
    "\n",
    "# Tasa de aprendizaje\n",
    "eta=0.01\n",
    "\n",
    "# Vector de pesos inicial\n",
    "wPID = np.zeros(X_trainPID.shape[1])\n",
    "\n",
    "# Entrenamiento\n",
    "for xi, target in zip(X_trainPID, y_trainPID):\n",
    "    activation = np.dot(xi, wPID)\n",
    "    output = np.where(activation >= 0.0, 1, 0)\n",
    "    error = target - output\n",
    "    wPID += eta * error * xi\n",
    "    \n",
    "# Prueba\n",
    "errores = 0\n",
    "outputs = []\n",
    "for xi, target in zip(X_testPID, y_testPID) :\n",
    "    activation = np.dot(xi, wPID)\n",
    "    output = np.where(activation >= 0.0, 1, 0)\n",
    "    outputs.append(output)\n",
    "    if (target != output) :\n",
    "        errores += 1\n",
    "print(\"{} vectores mal clasificados de {} ({}%)\".format(errores, len(y_testPID), \n",
    "                                                        errores/len(y_testPID)*100))\n",
    "print(\"\\nReporte de clasificación\\n\", classification_report(y_testPID, outputs))\n",
    "print(\"\\nMatriz de confusión\\n\", confusion_matrix(y_testPID, outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que los resultados para problemas más complejos no es satisfactorio. Este resultado no es realmente una sorpresa dado que una neurona como la planteada es, simplemente, un clasificador lineal. A continuación los resultados de 5 corridas con diferentes particiones de datos:\n",
    "\n",
    "Corrida | Resultados  \n",
    "----| ----|\n",
    "1   | 26 vectores mal clasificados de 79 (32.91%)\n",
    "2   | 31 vectores mal clasificados de 79 (39.24%)\n",
    "3   | 37 vectores mal clasificados de 79 (46.84%)\n",
    "4   | 55 vectores mal clasificados de 79 (69.62%)\n",
    "5   | 18 vectores mal clasificados de 79 (22.78%)\n",
    "\n",
    "Como puede observarse, el porcentaje de error, en las corridas presentadas, va desde 22.78% hasta 69.62%. Esta variabilidad en la capacidad de clasificación es debida a las siguientes características del proceso de entrenamiento del perceptrón:\n",
    "\n",
    "1. El error cometido por el perceptrón durante el entrenamiento, el cual se utiliza para realizar el ajuste de pesos, es de caracter \"todo-o-nada\": Solo se toma en cuenta si la clasificación fue correcta en términos del valor discreto de clase, pero no se toman en cuenta los aciertos que estuvieron cerca de fallar y se magnifican los errores cometidos por un margen pequeño (ambos casos correspondientes a puntos muy cercanos al hiperplano definido por los pesos actuales).\n",
    "2. El perceptrón corrije los pesos cada que encuentra un vector que se clasifica erróneamente, lo que lo hace susceptible a valores atípicos.\n",
    "\n",
    "El resultado es un hiperplano no óptimo, altamente dependiente de la secuencia de presentación de los vectores de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación realizamos una segunda ronda de entrenamiento con los mismos datos de entrenamiento, barajeados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84 vectores de entrenamiento mal clasificados de 263 (31.93916349809886%)\n",
      "49 vectores mal clasificados de 130 (37.69230769230769%)\n",
      "\n",
      "Matriz de confusión\n",
      " [[53 36]\n",
      " [13 28]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "# 2a ronda de entrenamiento\n",
    "vectores = 0\n",
    "shuffled_data = shuffle(list(zip(X_trainPID, y_trainPID)))\n",
    "for xi, target in shuffled_data:\n",
    "    activation = np.dot(xi, wPID)\n",
    "    output = np.where(activation >= 0.0, 1, 0)\n",
    "    error = target - output\n",
    "    wPID += eta * error * xi\n",
    "    if (target != output) :\n",
    "        vectores += 1\n",
    "print(\"{} vectores de entrenamiento mal clasificados de {} ({}%)\".\n",
    "      format(vectores, len(y_trainPID), vectores/len(y_trainPID)*100))\n",
    "    \n",
    "# Prueba\n",
    "errores = 0\n",
    "outputs = []\n",
    "for xi, target in zip(X_testPID, y_testPID) :\n",
    "    activation = np.dot(xi, wPID)\n",
    "    output = np.where(activation >= 0.0, 1, 0)\n",
    "    outputs.append(output)\n",
    "    if (target != output) :\n",
    "        errores += 1\n",
    "print(\"{} vectores mal clasificados de {} ({}%)\".format(errores, len(y_testPID), \n",
    "                                                        errores/len(y_testPID)*100))\n",
    "print(\"\\nMatriz de confusión\\n\", confusion_matrix(y_testPID, outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación utilizamos todos los datos para continuar el entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122 vectores de entrenamiento mal clasificados de 393 (31.043256997455472%)\n",
      "56 vectores mal clasificados de 130 (43.07692307692308%)\n",
      "\n",
      "Matriz de confusión\n",
      " [[45 44]\n",
      " [12 29]]\n"
     ]
    }
   ],
   "source": [
    "# 3a ronda de entrenamiento... con todos los datos\n",
    "vectores = 0\n",
    "for xi, target in zip(df_pure.values, df_class):\n",
    "    activation = np.dot(xi, wPID)\n",
    "    output = np.where(activation >= 0.0, 1, 0)\n",
    "    error = target - output\n",
    "    wPID += eta * error * xi\n",
    "    if (target != output) :\n",
    "        vectores += 1\n",
    "print(\"{} vectores de entrenamiento mal clasificados de {} ({}%)\".\n",
    "      format(vectores, len(df_class), vectores/len(df_class)*100))\n",
    "    \n",
    "# Prueba\n",
    "errores = 0\n",
    "outputs = []\n",
    "for xi, target in zip(X_testPID, y_testPID) :\n",
    "    activation = np.dot(xi, wPID)\n",
    "    output = np.where(activation >= 0.0, 1, 0)\n",
    "    outputs.append(output)\n",
    "    if (target != output) :\n",
    "        errores += 1\n",
    "print(\"{} vectores mal clasificados de {} ({}%)\".format(errores, len(y_testPID), \n",
    "                                                        errores/len(y_testPID)*100))\n",
    "print(\"\\nMatriz de confusión\\n\", confusion_matrix(y_testPID, outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado apenas mejora o incluso empeora en diferentes corridas. El plano de clasificación alcanzó su máxima eficacia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaline y la regla delta\n",
    "\n",
    "#### El modelo Adaline\n",
    "\n",
    "La siguiente modificación al modelo de neurona se obtiene observando que en la regla de Hebb, la modificación en la neurona A se realiza a partir de la estimulación obtenida debido a la señal proveniente de la neurona B y no al disparo de una respuesta (\"When an axon of cell A is near enough to excite a cell B...\"). Este ajuste da lugar al modelo de neurona Adaline.\n",
    "\n",
    "La **neurona lineal adaptativa** o **Adaline** (*Adaptive Linear Neuron*) puede describirse de acuerdo a la siguiente estructura:\n",
    "\n",
    "![](images/neuron6.png)\n",
    "\n",
    "En esta neurona, la función de activación se divide en una función de activación lineal continua, concretamente la función identidad y una función de salida o cuantificador que es la responsable de generar la salida discreta (la salida es, en muchos casos seleccionada como [-1,1] por conveniencia). De esta manera, el cálculo del error y la actualización de los pesos sinápticos se realiza a partir de la diferencia entre la salida esperada y la suma ponderada de las entradas:<br><br>\n",
    "\n",
    "![](images/neuron7.png)\n",
    "\n",
    "$$error = y_j - \\sum_{i=1}^n w_i x_{ji} = y_j - \\mathbf{w}\\cdot \\mathbf{x}_j(t)$$\n",
    "\n",
    "Esta medida de error es la distancia de cada vector al hiperplano de clasificación. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 vectores mal clasificados de 130 (25.384615384615383%)\n",
      "\n",
      "Matriz de confusión\n",
      " [[66 23]\n",
      " [10 31]]\n"
     ]
    }
   ],
   "source": [
    "# Vector de pesos inicial\n",
    "wPID = np.zeros(X_trainPID.shape[1])\n",
    "\n",
    "# Entrenamiento\n",
    "for xi, target in zip(X_trainPID, y_trainPID):\n",
    "    activation = np.dot(xi, wPID)\n",
    "    error = target - activation\n",
    "    wPID += eta * error * xi\n",
    "    \n",
    "# Prueba\n",
    "errores = 0\n",
    "outputs = []\n",
    "for xi, target in zip(X_testPID, y_testPID) :\n",
    "    activation = np.dot(xi, wPID)\n",
    "    output = np.where(activation >= 0.0, 1, 0)\n",
    "    outputs.append(output)\n",
    "    if (target != output) :\n",
    "        errores += 1\n",
    "print(\"{} vectores mal clasificados de {} ({}%)\".format(errores, len(y_testPID), \n",
    "                                                        errores/len(y_testPID)*100))\n",
    "print(\"\\nMatriz de confusión\\n\", confusion_matrix(y_testPID, outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### El sesgo\n",
    "El sesgo (*bias*) es otra modificación que suele hacerse a la neurona artificial, inspirada en el clasificador lineal y con una contraparte biológica en el potencial de activación o potencial umbral de la neurona. En la siguiente gráfica se muestra el potencial eléctrico de una neurona biológica:\n",
    "\n",
    "![](images/potencial-de-accion.png)\n",
    "\n",
    "Como puede observarse, la neurona siempre mantiene un potencial eléctrico (de reposo), independientemente de que no reciba una entrada y genera un impulso (un potencial de acción), únicamente en aquellos casos en los que la excitación de la membrana sobrepasa un cierto potencial. Esto equivale, en la neurona artificial, a modificar el valor de activación para el cual la neurona artificial arroja un 1, o bien, a introducir un valor de *entrada* independiente de las señales provenientes de las otras neuronas. Este término se denomina *Bias*. \n",
    "\n",
    "Desde una perspectiva del clasificador lineal, observemos que el término $\\mathbf{w}\\cdot \\mathbf{x}_j(t)$ describe una línea que pasa por el origen. Esta condición (de pasar por el origen) restringe la capacidad del clasificador:<br><br>\n",
    "\n",
    "![](images/linear-plane-bias.png)\n",
    "\n",
    "Al agregar un término independiente aumenta la felxibilidad del clasificador. \n",
    "\n",
    "De manera que ahora, la estructura de la neurona artificial puede representarse de la siguiente manera:<br>\n",
    "\n",
    "![](images/neuron8.png)\n",
    "\n",
    "Donde el bias se integra al modelo como un \"nodo\" de entrada constante (típicamente con valor 1) y un peso $w_0$ adicional que permite ajustar la posición del hiperplano clasificador. \n",
    "\n",
    "#### La regla delta\n",
    "Otra ventaja de la función de activación lineal en la neurona Adaline, con respecto a la función escalón del perceptrón, es que esta función es derivable. Esto significa que podemos definir una función de error debido a los pesos $E(\\mathbf{w})$ que podemos minimizar para optimizar el valor de los pesos $\\mathbf{w}$. Normalmente se utiliza el error cuadrático medio:\n",
    "\n",
    "$$E(\\mathbf{w}) = \\frac{1}{2} \\sum_{j=1}^{T}(y_j-a_j)^2$$\n",
    "\n",
    "siendo $E$ el error global, $T$ el núnero total de muestras de entrenamiento, $y_j$ el valor de clase del vector de entrenamiento $j$ (el valor objetivo) y $a_j\\in \\mathbb{R}$ la salida de la función de activación. El término $\\frac{1}{2}$ es por conveniencia al derivar.\n",
    "\n",
    "Uno de los métodos más utilizados en redes neuronales es el denominado **descenso de gradiente**. La lógica tras este método es que al tratar de alcanzar el valor de $\\mathbf{w}$ que minimiza la función de error, la pendiente de la función también disminuye:\n",
    "\n",
    "![](images/gradient-descent.png)\n",
    "\n",
    "El gradiente es un vector que apunta en dirección hacia donde crece la pendiente de una superficie. Entonces, para alcanzar el mínimo de la función de error, en este caso $E(\\mathbf{w})$, es necesario \"moverse\" a lo largo de $\\mathbf{w}$ en dirección contraria al gradiente de $E$, esto es:\n",
    "\n",
    "$$\\Delta \\mathbf{w} = - \\eta \\nabla E(\\mathbf{w}) = - \\eta \\left(\\frac{\\partial E(\\mathbf{w})}{\\partial w_0},\\ldots \\frac{\\partial E(\\mathbf{w})}{\\partial w_n} \\right)$$\n",
    "\n",
    "donde\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial E(\\mathbf{w})}{\\partial w_i}  &= \\frac{\\partial }{\\partial w_i} \\frac{1}{2} \\sum_{j=1}^{T}(y_j-a_j)^2\\\\\n",
    "&= \\frac{1}{2} \\sum_{j=1}^{T}\\frac{\\partial }{\\partial w_i} (y_j-a_j)^2 \\\\\n",
    "&= \\frac{1}{2} \\sum_{j=1}^{T} 2 (y_j-a_j) \\frac{\\partial }{\\partial w_i} (y_j-a_j) \\\\\n",
    "&= \\sum_{j=1}^{T} (y_j-a_j) \\frac{\\partial }{\\partial w_i} \\left(y_j-\\sum_{\\iota=1}^n w_{\\iota} x_{j\\iota}\\right) \\\\\n",
    "&= \\sum_{j=1}^{T} (y_j-a_j) \\frac{\\partial }{\\partial w_i} \\left(-w_{i} x_{ji}\\right) \\\\\n",
    "&= -\\sum_{j=1}^{T} (y_j-a_j) x_{ji}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "De manera que, para buscar iterativamente el conjunto de pesos que minimiza el error en la neurona Adaline, actualizamos los pesos de acuerdo a la siguiente regla:\n",
    "\n",
    "$$\\Delta \\mathbf{w} = \\eta \\sum_{j=1}^{T} (y_j-a_j) \\mathbf{x_j}$$\n",
    "\n",
    "o bien\n",
    "\n",
    "$$\\mathbf{w} = \\mathbf{w} + \\eta \\sum_{j=1}^{T} (y_j-a_j) \\mathbf{x_j}$$\n",
    "\n",
    "donde el lado izquierdo de la ecuación representa, iterativamente, el nuevo valor de $\\mathbf{w}$, dados los valores en el lado derecho.\n",
    "\n",
    "Si bien esta regla de entrenamiento tiene un aspecto parecido a la regla de entrenamiento para el perceptrón, pueden observarse dos diferencias importantes:\n",
    "1. La primera diferencia es el error se mide a partir de una función continua y no a partir de valores escalón.\n",
    "2. La actualización de los pesos se realiza a partir del error acumulado en todos los vectores de entrenamiento y no vector a vector, como es en el entrenamiento del perceptrón.\n",
    "\n",
    "En los siguientes ejemplos se analiza nuevamente el problema de las flores Iris (con sólo 2 clases), utilizando el clasificador Adaline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 vectores mal clasificados de 50 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "# Re-etiquetar las clase de cada vector ejemplo en [-1,1]\n",
    "yAd = np.where(Y == 0, -1, 1)\n",
    "\n",
    "# Normalizar los vectores de características\n",
    "XAd = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Tasa de aprendizaje\n",
    "eta=0.01\n",
    "# Número de iteraciones\n",
    "niter = 20\n",
    "\n",
    "# Vector de pesos inicial\n",
    "wAd = np.zeros(XAd.shape[1] + 1)\n",
    "# Número de errores\n",
    "errors = []\n",
    "# Función de costo\n",
    "costs = []\n",
    "\n",
    "# Datos para entrenamiento y prueba\n",
    "train_dataAd, test_dataAd, train_yAd, test_yAd = train_test_split(XAd, yAd, test_size=.5)\n",
    "\n",
    "# Entrenamiento\n",
    "for i in range(niter):\n",
    "    output = np.dot(train_dataAd, wAd[1:]) + wAd[0]\n",
    "    errors = train_yAd - output\n",
    "    wAd[1:] += eta * train_dataAd.T.dot(errors)\n",
    "    wAd[0] += eta * errors.sum()\n",
    "\n",
    "# Prueba\n",
    "errores = 0\n",
    "for xi, target in zip(test_dataAd, test_yAd) :\n",
    "    activation = np.dot(xi, wAd[1:]) + wAd[0]\n",
    "    output = np.where(activation >= 0.0, 1, -1)\n",
    "    if (target != output) :\n",
    "        errores += 1\n",
    "print(\"{} vectores mal clasificados de {} ({}%)\".format(errores, len(test_dataAd), \n",
    "                                                        errores/len(test_dataAd)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos a continuación la clasificación mediante Adaline sobre los datos de diabetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 vectores mal clasificados de 130 (19.230769230769234%)\n",
      "\n",
      "Matriz de confusión\n",
      " [[78 10]\n",
      " [15 27]]\n"
     ]
    }
   ],
   "source": [
    "# Re-etiquetar las clase de cada vector ejemplo en [-1,1]\n",
    "df_classAd = np.where(df_class == 0, -1, 1)\n",
    "\n",
    "X_trainPIDAd, X_testPIDAd, y_trainPIDAd, y_testPIDAd = train_test_split(\n",
    "    df_pure.values, df_classAd, test_size=0.33)\n",
    "\n",
    "# Tasa de aprendizaje\n",
    "eta=0.0001\n",
    "\n",
    "# Número de iteraciones\n",
    "niter = 500\n",
    "\n",
    "# Vector de pesos inicial\n",
    "wPIDAd = np.zeros(X_trainPIDAd.shape[1] + 1)\n",
    "\n",
    "# Number of misclassifications\n",
    "errors = []\n",
    "\n",
    "# Entrenamiento\n",
    "for i in range(niter):\n",
    "    output = np.dot(X_trainPIDAd, wPIDAd[1:]) + wPIDAd[0]\n",
    "    errors = y_trainPIDAd - output\n",
    "    wPIDAd[1:] += eta * X_trainPIDAd.T.dot(errors)\n",
    "    wPIDAd[0] += eta * errors.sum()\n",
    "\n",
    "# Prueba\n",
    "errores = 0\n",
    "outputs = []\n",
    "for xi, target in zip(X_testPIDAd, y_testPIDAd) :\n",
    "    activation = np.dot(xi, wPIDAd[1:]) + wPIDAd[0]\n",
    "    output = np.where(activation >= 0.0, 1, -1)\n",
    "    outputs.append(output)\n",
    "    if (target != output) :\n",
    "        errores += 1\n",
    "print(\"{} vectores mal clasificados de {} ({}%)\".format(errores, len(X_testPIDAd), \n",
    "                                                        errores/len(X_testPIDAd)*100))\n",
    "print(\"\\nMatriz de confusión\\n\", confusion_matrix(y_testPIDAd, outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método descrito anteriormente se conoce también como \"*método de descenso de gradiente por lotes*\": la modificación de los pesos (el aprendizaje) se realiza sólo después de acumular los errores para el *lote* completo de los vectores de entrenamiento. Esto es así debido a que es la solución arrojada por el proceso de optimización. Existen otros métodos para optimizar los pesos, siendo la alternativa más directa el llamado \"*método de descenso de gradiente estocástico*\" o de \"*aprendizaje en línea*\". En este método la actualización de los pesos se realiza un vector a la vez (como en el caso del perceptrón). Esta versión suele tener mejor tiempo de convergencia y es útil para mantener la red actualizada con los datos que va recibiendo. Sin embargo, esta es una aproximación al proceso de optimización del error. El resultado es típicamente *oscilatorio* y depende de como se van presentando los ejemplos de entrenamiento. \n",
    "\n",
    "A continuación la aplicación del método de descenso de gradiente estocástico a los datos de diabetes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 vectores mal clasificados de 130 (19.230769230769234%)\n",
      "\n",
      "Matriz de confusión\n",
      " [[78 10]\n",
      " [15 27]]\n"
     ]
    }
   ],
   "source": [
    "# Tasa de aprendizaje\n",
    "eta=0.0001\n",
    "\n",
    "# Número de iteraciones\n",
    "niter = 500\n",
    "\n",
    "# Vector de pesos inicial\n",
    "wPIDAdS = np.zeros(X_trainPIDAd.shape[1] + 1)\n",
    "\n",
    "# Number of misclassifications\n",
    "errors = []\n",
    "# Cost function\n",
    "costs = []\n",
    "\n",
    "# Entrenamiento\n",
    "for i in range(niter):\n",
    "    for xi, target in zip(X_trainPID, y_trainPID):\n",
    "        activation = np.dot(xi, wPIDAdS[1:]) + wPIDAdS[0]\n",
    "        output = np.where(activation >= 0.0, 1, 0)\n",
    "        error = target - output\n",
    "        wPIDAdS[1:] += eta * xi.dot(error)\n",
    "        wPIDAdS[0] += eta * error\n",
    "\n",
    "# Prueba\n",
    "errores = 0\n",
    "outputs = []\n",
    "for xi, target in zip(X_testPIDAd, y_testPIDAd) :\n",
    "    activation = np.dot(xi, wPIDAd[1:]) + wPIDAd[0]\n",
    "    output = np.where(activation >= 0.0, 1, -1)\n",
    "    outputs.append(output)\n",
    "    if (target != output) :\n",
    "        errores += 1\n",
    "print(\"{} vectores mal clasificados de {} ({}%)\".format(errores, len(X_testPIDAd), \n",
    "                                                        errores/len(X_testPIDAd)*100))\n",
    "print(\"\\nMatriz de confusión\\n\", confusion_matrix(y_testPIDAd, outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La función de activación\n",
    "\n",
    "Otra línea de desarrollo de los modelos neuronales es la selección de la función de activación de la neurona: ni la respuesta escalón ni la respuesta identidad (lineal) son respuestas \"naturales\". Una respuesta escalón implica la \"aparición instantánea\" de una respuesta, pasando de una respuesta nula (o con un valor base) a un nuevo valor (lo cual requiere una cantidad de energía infinita. Una respuesta lineal implicaría que la neurona responde a cualquier estímulo, sin discriminar el ruido y sin permitir que se integren señales pequeñas pero sostenidas para provocar una respuesta. A continuación se muestran un par de gráficas de respuestas de neuronas [VTA](https://en.wikipedia.org/wiki/Ventral_tegmental_area) (tomada de [A-type K+ Current of Dopamine and GABA Neurons in the Ventral Tegmental Area. Susumu Koyama y Sarah B. Appel, Journal of Neurophysiology, Vol. 96(2), 2006, DOI: 10.1152/jn.01318.2005](http://jn.physiology.org/content/96/2/544)), donde se aprecia que la respuesta a la presentación de voltajes de entrada tiene un comportamiento no lineal, tanto para la corriente (imagen a la izquierda) como en la conductancia (derecha). <br><br>\n",
    "\n",
    "![](images/f1.jpg)<br><br>\n",
    "\n",
    "Con el fin de explotar un poco más la metáfora neural y desarrollar modelos más variados se han definido una diversidad de funciones de activación, siendo las principales la función gaussiana, la función sigmoide y la tangente hiperbólica, mostradas a continuación.<br><br>\n",
    "\n",
    "![](images/activation-functions.png)\n",
    "\n",
    "De manera que, en general, podemos expresar la salida de la neurona como\n",
    "\n",
    "$$y_j = g \\left( a_j \\right)$$\n",
    "\n",
    "siendo \n",
    "\n",
    "$$a_j = \\sum_{i=1}^n w_i x_i - \\theta $$\n",
    "\n",
    "la activación de la neurona.\n",
    "\n",
    "### Función de activación Relu\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El *perceptrón multicapa* y el algoritmo *backpropagation*\n",
    "\n",
    "En términos de la metáfora neuronal, la función de razonamiento no se logra con una sola neurona, sino con un conglomerado, interconectado, de neuronas. La organización de las neuronas en el cerebro humano aún es tema de debate entre los especialistas en neurociencias, sin embargo, se han identificado estructuras que han sido aceptada ampliamentes. La estructura más simple, desde un punto de vista estructural, es el circuito neuronal. El término **circuito neuronal** hace referencia, típicamente, a un conjunto de componentes interconectadas que ofrecen, de manera conjunta, una función cerebral específica. Estos circuitos no se encuentran presentes al momento de nacer ni son estáticas, por el contrario, están sujetos a una constante evolución a lo largo de la vida del individuo. \n",
    "\n",
    "![](images/neuron4.jpg)\n",
    "\n",
    "Una arquitectura que se acerca a la metáfora neuronal es la siguiente:\n",
    "\n",
    "![](images/full_topo.png)\n",
    "\n",
    "En este caso, se trata de una *[red neuronal de impulsos](https://en.wikipedia.org/wiki/Spiking_neural_network)* *[completamente conectada](https://en.wikipedia.org/wiki/Network_topology#Mesh)*. Sin embargo, una opción mucho más común es el modelo conocido como **multiperceptrón multicapa** o, más simplemente, **perceptrón multicapa**, un tipo especial de \"*red neuronal alimentada hacia adelante*\" (red *feed-forward*) formada por una colección de *perceptrones* organizados en una sola dirección, desde una primera *capa de entrada* que recibe todos los estímulos a la red (siguiendo la metáfora, estos estímulos pueden ser las señales de las neuronas sensoras) y transfieren estas señales a un conjunto de capas, avanzando hasta la *capa de salida*. La capa de salida arroja la o las salidas de la red neuronal (en la metáfora se trataría, por ejemplo, de las señales hacia las neuronas motoras). \n",
    "\n",
    "![](images/neuron5.png)\n",
    "\n",
    "El entrenamiento de una red neuronal multicapa consiste en ajustar los pesos sinápticos entre las neuronas, de tal manera que las salidas de las neuronas en la capa de salida correspondan a una cierta salida  prevista, asociada a los datos de entrada. La salida en cada neurona depende del conjunto de entradas y los pesos asignados a cada conexión. Como en las neuronas individuales, los pesos se configuran de acuerdo a los ejemplos que se le presentan durante el entrenamiento, de manera que las salidas esperadas en las capas previas a la capa de salida para una entrada específica son desconocidas. En este caso no es posible utilizar la regla de Hebb, utilizada para entrenar el perceptrón ni la regla delta utilizada en el modelo Adaline, ya que en ambos casos se requiere conocer, para cada neurona, además de las salidas, las entradas y las salidas esperadas. De manera que podemos ver la red neuronal como una *caja negra*. Por ello, estas capas intermedias (entre la capa de entrada y la capa de salida) se denominan *capas ocultas*.<br><br>\n",
    "\n",
    "![](images/neuron9.png)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algoritmo *backpropagation*\n",
    "\n",
    "El algoritmo *backpropagation* es el mecanismo de entrenamiento para redes neuronales *feed-forward* más utilizado. La idea es básica es: 1) Las entradas a la red se propagan desde la entrada a la salida a través de *impulsos* que pasan de una neurona a la siguiente; 2) cada neurona colabora para generar la salida en la red, estimulando adecuadamente a las neuronas con las que se conecta; 3) puesto que originalmente los pesos en cada neurona no son los adecuados, cada neurona comete un \"error\" que, al acumularse, dan origen al error de la red, es decir, el error en la red es consecuencia del funcionamiento inadecuado del conjunto completo de neuronas. El algoritmo de retropropagación del error consiste, entonces, en evaluar el error en la capa de salida y propagar el error hacia la primera capa oculta, tratando de corregir los errores en las neuronas correspondientes.\n",
    "\n",
    "La salida de la neurona $k$ en la capa de salida (capa $o$) es $y^*_{kj}$ mientras que la salida esperada es $y_{kj}$, de manera que el error en la salida de esta neurona es:\n",
    "\n",
    "$$\\text{error}_k^{(o)} = y_{kj} - y^*_{kj}$$\n",
    "\n",
    "Para las neuronas antecesoras, las neuronas de las capas ocultas, no hay valores esperados, por lo que no se puede definir un error por esta vía. Sin embargo, es lógico asumir que el error en las neuronas de salida es generado, parcialmente por los valores de entrada, $x_i^{(o)}$ que recibe y que corresponden a las salidas $z_i^{(L)}$ de las neuronas en la capa previa $L$.\n",
    "\n",
    "En general, la salida de la $k$-ésima neurona en la capa $l$ está dada por\n",
    "\n",
    "$$z^{(l)}_k = g \\left( a^{(l)}_k \\right)$$\n",
    "\n",
    "donde, $g(\\cdot)$ es la función de activación y $a^{(l)}_k$ es la activación de la neurona. \n",
    "\n",
    "$$a^{(l)}_k = \\left(\\sum_{i=1}^{n^{(l-1)}}w^{(l)}_{ki} x^{(l)}_i \\right) - \\theta^{(l)}_k$$\n",
    "\n",
    "siendo $w^{(l)}_{ki}$ el peso sináptico entre la $i$-ésima neurona en la capa $l-1$ y la $k$-ésima neurona en la capa $l$. $n^{(l-1)}$ es el número de neuronas en la capa $l-1$, lo cual corresponde al número de entradas en la cada neurona de la capa $l$. $x^{(l)}_i$ es la $i$-ésima entrada a la capa $l$ y que corresponde a la salida de la neurona $i$ en la capa $l-1$, $z^{(l-1)}_i$ o al rasgo $i$ del vector de entrada si $l$ es la primrea capa oculta. $\\theta^{(l)}_k $ es el valor de bias o valor umbral de la neurona $k$ de la capa $l$. \n",
    "\n",
    "$w^{(l)}_{ki}$ se actualiza de forma iterativa, mediante la ecuación\n",
    "\n",
    "$$w^{(l)}_{ki}(j+1) = w^{(l)}_{ki}(j) + \\Delta w^{(l)}_{ki}(j)$$\n",
    "\n",
    "con $\\Delta w^{(l)}_{ki}(j)$, el cambio en el peso de la neurona $k$ para la entrada $i$ debido al ejemplo $j$, definido de la siguiente manera:\n",
    "\n",
    "$$\\Delta w^{(l)}_{ki}(j) = \\eta\\ \\delta^{(l)}_{k}(j)\\ x^{(l)}_{i}(j) + \\alpha\\ \\Delta w^{(l)}_{ki}(j-1)$$\n",
    "\n",
    "$\\delta^{(l)}_{k}$ es la señal de error de la neurona $k$ en la capa $l$ y se define como\n",
    "\n",
    "$$\\delta^{(l)}_{k}(j)=\\left\\{\n",
    "      \\begin{array}{ll}\n",
    "         g^\\prime\\!\\!\\left(a^{(o)}_k \\right) \\left( y_k - z^{(o)}_k \\right) &\n",
    "                         \\textrm{Para neuronas en la capa de salida} \\\\\n",
    "                         & \\\\\n",
    "         g^\\prime\\!\\!\\left(a^{(l)}_k \\right) \\sum_{m=1}^{n^{(l+1)}} \\delta^{(l+1)}_{m} w^{(l+1)}_{mk} &\n",
    "                         \\textrm{Para neuronas en las capas anteriores a la de salida}\n",
    "      \\end{array} \\right.\n",
    "$$\n",
    "\n",
    "Para una función sigmoidea de salida, por ejemplo, tendremos\n",
    "\n",
    "$$\n",
    "g(a) = \\frac{1}{1+e^{-Ga}}\n",
    "$$\n",
    "\n",
    "donde $G$ es la ganancia de la red. La derivada de esta función de activación es:\n",
    "\n",
    "$$g'(a) = G\\ g(a)\\left(1-g(a)\\right)$$\n",
    "\n",
    "El segundo término en la expresión para la actualización de los pesos es simple de analizar: $\\Delta w^{(l)}_{ki}(j-1)$ es el cambio en el peso debido al ejemplo anterior $j-1$; $\\alpha$, por otra parte, es una constante en $[0, 1]$. Es decir, el cambio en el peso dado el ejemplo actual, va a agregar una proporción del cambio inmediato anterior:\n",
    "\n",
    "![](images/backpropagation.momentum.png)\n",
    "\n",
    "$\\alpha$ se denomina momento y su objetivo es reducir las fluctuaciones en la dirección de cambio de los pesos, al hacer que el cambio en los pesos en el paso actual dependa del cambio previo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, presentamos los resultados de la red neuronal *feed-forward* entrenada con el método *backpropagation* sobre los datos de prueba. Utilizamos un método del módulo *neural_network* de la biblioteca *sklearn*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Iris Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 vectores mal clasificados de 50 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-4, activation='tanh', #  identity logistic\n",
    "                    hidden_layer_sizes=(3, 2), random_state=1, \n",
    "                    learning_rate_init=0.001, max_iter=500)\n",
    "clf.fit(train_dataAd, train_yAd)                         \n",
    "\n",
    "# Prueba\n",
    "errores = 0\n",
    "for xi, target in zip(test_dataAd, test_yAd) :\n",
    "    output = clf.predict(xi.reshape(1, -1))\n",
    "    if (target != output) :\n",
    "        errores += 1\n",
    "print(\"{} vectores mal clasificados de {} ({}%)\".format(errores, len(test_yAd), \n",
    "                                                        errores/len(test_yAd)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pima Indians Diabetes Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 vectores mal clasificados de 130 (23.076923076923077%)\n",
      "\n",
      "Matriz de confusión\n",
      " [[67 21]\n",
      " [ 9 33]]\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', alpha=1.5e-4, activation='logistic', #identity tanh relu\n",
    "                    hidden_layer_sizes=(5, 7, 3), random_state=1, \n",
    "                    learning_rate_init=0.001, max_iter=5000)\n",
    "clf.fit(X_trainPIDAd, y_trainPIDAd)                         \n",
    "\n",
    "# Prueba\n",
    "errores = 0\n",
    "outputs = []\n",
    "for xi, target in zip(X_testPIDAd, y_testPIDAd) :\n",
    "    output = clf.predict(xi.reshape(1, -1))\n",
    "    outputs.append(output)\n",
    "    if (target != output) :\n",
    "        errores += 1\n",
    "print(\"{} vectores mal clasificados de {} ({}%)\".format(errores, len(X_testPIDAd), \n",
    "                                                        errores/len(X_testPIDAd)*100))\n",
    "classification_report, \n",
    "print(\"\\nMatriz de confusión\\n\", confusion_matrix(y_testPIDAd, outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
