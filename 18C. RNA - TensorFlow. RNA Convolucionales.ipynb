{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/header_18.png)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes neuronales convolucionales\n",
    "\n",
    "Las redes convolucionales son una combinación de un perceptrón multicapa, con capas de pre-procesamiento de los datos de entrada. Estas capas de entrada mejoran y simplifican los datos que serán presentados al perceptrón multicapa. Esta arquitectura es particularmente útil para reconocimiento de patrones en imágenes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La inspiración biológica\n",
    "\n",
    "El perceptrón multicapa es un buen clasificador y método de aprendizaje automático basado en una metafora fisiológica. Sin embargo, la arquitectura que presenta este modelo es muy simplista: La red neuronal se conforma mediante un conjunto, típicamente grande, de neuronas básicamente idénticas, organizadas en capas secuenciales que tampoco se diferencian entre sí. La forma de resolver un problema es, básicamente, la misma sin importar el tipo de problema de que se trate. Las redes neuronales convolucionales se derivan de una metáfora más amplia de la estructura cerebral, específicamente aquella especializada en la comprensión de la información visual.\n",
    "\n",
    "### Estructura cerebral en el humano\n",
    "\n",
    "Actualmente, no existe una teoría unificada acerca de la organización y funcionamiento del cerebro que sea universalmente aceptada. Sin embargo, existe un consenso amplio en torno a la idea de que las diferentes regiones en el cerebro se especializan funcionalmente. Anatómicamente, el cerebro se describe en dos hemisferios (derecho e izquierdo), divididos en 6 lóbulos, cuatro de ellos fácilmente identificables (el lóbulo frontal, el lóbulo temporal, el lóbulo parietal y el lóbulo occipital) y otros dos (el lóbulo límbico y el lóbulo insular) localizados más al interior del cerebro.<br><br>\n",
    "\n",
    "<img src=\"images/brain_2.png\" width=50%><br>\n",
    "\n",
    "Estas regiones anatómicas han sido asociadas a funciones específicas, una interpretación sujeta a controversia, pero que sigue siendo ampliamente aceptada:\n",
    "\n",
    "* Hemisferio derecho: Se considera responsable de controlar el movimiento de lado izquierdo del cuerpo, así como asiento de la creatividad, la intuición y el pensamiento holístico.\n",
    "\n",
    "* Hemisferio izquierdo: Se considera responsable de controlar el movimiento de lado derecho del cuerpo, así como asiento del pensamiento lógico-matemático y el lenguaje.\n",
    "\n",
    "* Lóbulo frontal: Se encuentra localizado al frente del cerbro y delimitado posteriormente por el surco central y en la parte inferior por el surco lateral. Se considera responsable de controlar el movimiento voluntario de partes específicas del cuerpo, así como de funciones mentales superiores como la planificación y la toma de decisiones. También se considera responsable de la articulación fluida del habla.\n",
    "\n",
    "* Lóbulo parietal: Se localiza en la parte central superior del cerebro. Se asume responsable de procesar información sensorial relacionada con el gusto, la temperatura, el tacto y la propiocepción. También se le responsabiliza (de acuerdo a la hipótesis de las dos corrientes de la visión) de la interpretación visual del movimiento (trayectoria dorsal). \n",
    "\n",
    "* Lóbulo occipital: Localizado en la parte posterior del cerebro, contiene la mayor parte de la corteza visual en los mamíferos y se le considera responsable del procesamiento de la información visual proveniente de los ojos.\n",
    "\n",
    "* Lóbulo temporal: Se encuentra localizado por abajo del surco lateral. Se le considera responsable de interpretar la información proveniente de los sentidos, particularmente del oído y la vista, comprensión del lenguaje y la asociación de emociones.\n",
    "\n",
    "* Lóbulo límbico: Se encuentra localizado sobre las caras interiores de cada hemisferio cerebral, en la parte más interna y abarca porciones de los lóbulos frontal, parietal y temporal. Se le responsabiliza del olfato y de influir fuertemente en las emociones.\n",
    "\n",
    "* Lóbulo insular: Consiste de la corteza cerebral en la parte profunda del surco lateral. Se le relaciona con la conciencia y se asume juega un papel importante en las emociones y el control de la homeostasis del cuerpo.\n",
    "\n",
    "\n",
    "### Comprensión de la información visual\n",
    "\n",
    "Una de las teorías de especialización local más influyentes y más cuestionadas en los últimos 30 años es la relacionada con la hipótesis de las dos trayectorías. De acuerdo con esta hipótesis, la información visual (al igual que la auditiva) viaja por dos trayectorias independientes:\n",
    "\n",
    "* La corriente ventral, que lleva la información visual desde el lóbulo occipital al lóbulo tenporal con el fin de identificar patrones (formas), y \n",
    "* La corriente dorsal, que lleva la información visual del lóbulo occipital a lóbulo parietal y cuyo objetivo es detectar movimiento en una escena. \n",
    "\n",
    "Aunque aún se cuestiona la supuesta independencia de las dos corrientes, la hipótesis se ha reivindicado en los últimos años y ha sido ampliamente aceptada, al menos en su idea general. De acuerdo a la hipótesis de las dos corrientes, el reconocimiento visual de patrones (forma y color) se puede esquematizar de la siguiente manera:\n",
    "\n",
    "1. La interpretación de información visual inicia con el sensado de la escena a través de células altamente especializadas en la **retina**: los conos y los bastones (importantes para la detección de movimiento). La percepción de la luz por los humanos se basa, de acuerdo con la teoría (tricromática) de Young–Helmholtz, en la respuesta de tres tipos de conos a diferentes rangos de longitud de onda:<br>&#9633; Los *conos S* que son particularmente sensibles a la luz en el rango 400–500 nm, con un pico de recepción en torno al rango 420–440 nm (en torno al color azul).<br>&#9633; Los *conos M*, sensibles fuertemente a la luz en el rango 450–630 nm, con un pico de recepción en torno al rango 534–555 nm (en la región de colores verde amarillento).<br>&#9633; Los *conos L* que son fuertemente sensibles a la luz en el rango 500–700 nm, con un pico de recepción en torno al rango 564–580 (la región del naranja).\n",
    "    <img src=\"images/colors_vision.png\" width=75%><br>\n",
    "La información proveniente de cada tipo de cono puede considerarse como un 'canal' de información. El estímulo visual es recibido por grupos de conos que muestrean porciones del campo visual de forma redundante. La luz recibida en los conos es convertida en una señal eléctrica y transmitida a las *células M* (neuronas) del sistema nervioso.<br><br>\n",
    "\n",
    "2. Los axones de las células-M se proyectan hacia el núcleo geniculado lateral (**NGL**), a través del nervio óptico (que se convierte en cintilla óptica a partir del quiasma óptico). El NGL es un núcleo en el tálamo que funciona como punto de relevo de la información visual. Consta de 6 capas, 4 de ellas formadas por *células-P* dedicadas a recibir las señales de las celulas-M (provenientes de los conos en la retina). La información recibida en el NGL es descompuesta en estas capas y reenviada a traves de la cintilla óptica a las corteza visual, específicamente a la corteza visual primaria y a la corteza visual secundaria (además, esta información es recombinada con la información proveniente de los bastones y enviada a *V3* para la detección de movimiento).<br><br>\n",
    "\n",
    "3. La mayor parte de los axones provenientes del NGL, a través de la cintilla óptica, terminan en la  corteza visual primaria **V1**. En la primera fase, entre los 40 y 100 ms después de recibir las señales prevenientes del NGL, V1 realiza actividades de detección de bordes. En este aspecto, la operación de las neuronas en V1 suele ser comparada con la de un filtro de Gabor (con el que es posible detectar bordes haciendo un análisis sobre múltiples frecuencias y/o direcciones). V1 mantiene una fuerte interconexión con V2 (y, al parecer con otras áres como V3, V4 e IT), enviando señales preliminares y recibiendo retroalimentación. Después de los primeros 100ms, V1 utiliza la retroalimentación de la corteza visual secundaria (y, posiblemente, de otras áreas) para realizar un procesamiento más detallado acerca de la organización de la escena. En V1 se inicia la corriente ventral.<br><br>\n",
    "\n",
    "4. **V2** es la corteza visual secundaria, rodea a la corteza visual primaria y conforma, con V3, el *área de asociación visual*. En gran medida repite las mismas funciones que V1, pero detectando características más complejas, como los contornos y la separación entre fondo y figura (segmentación de la imagen). También juega un papel importante en el enfoque de rasgos importantes en la imagen (a través de los núcleos pulvinares, responsables de la atención y los movimientos sacádicos), sin embargo, esta actividad no se asocia a la corriente ventral (aunque recientemente se ha insistido en la fuerte interrelación entre las dos corrientes).<br><br>\n",
    "\n",
    "5. **V4** se encuentra localizada por abajo de V2 y recibe la información proveniente de v2. Además, también llegan a V4 señales provenientes directamente de V1 y del NGL y señales provenientes de los núcleos pulvinares. V4 es capaz de reconocer características de complejidad media, como las figuras geométricas simples. <br><br>\n",
    "\n",
    "6. Desde V4, la información visual sigue su viaje a través de la corriente ventral hacia el **área inferotemporal** (IT), donde se lleva a cabo la identificación de patrones complejos, como el reconocimiento de rostros. Las neuronas en esta región preveen también del acceso a la memoria, con el fin de registrar y recuperar patrones para el reconocimiento de las formas complejas.<br><br>\n",
    "\n",
    "7. Finalmente, la clasificación de objetos (el juicio sobre el objeto) y la generación de la respuesta motora es realizada mediante circuitos neuronales en la **corteza prefrontal** (CPF).\n",
    "\n",
    "<img src=\"images/brain_1.png\" width=75%><br>\n",
    "\n",
    "<img src=\"images/brain_1b.png\" width=75%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolución sobre imágenes\n",
    "\n",
    "La forma en que se procesa la información visual en cada etapa del recorrido óptico, y particularmente en la retina, puede representarse mediante una operación de **convolución** sobre una imagen.\n",
    "\n",
    "La convolución es una operación matemática sobre dos funciones para producir una tercera función:\n",
    "\n",
    "$$h = f_1 \\ast f_2$$\n",
    "\n",
    "Aquí, $h$ describe la forma en que $f_1$ es modificada por $f_2$. \n",
    "\n",
    "En el caso de procesamiento de datos/imágenes, $f_1$ representa los datos de entrada y suele ser denominada *mapa de características*, mientras que $f_2$ es el *núcleo de convolución* y suele considerarse como un *filtro*. El kernel filtra el mapa de características para obtener un nuevo mapa con información de algún tipo (el conjunto de bordes, por ejemplo), o una versión mejorada del mapa original (después de la eliminación de ruido, por ejemplo). El resultado de la convolución es un *mapa transformado de características*. \n",
    "\n",
    "<img src=\"images/convol_1.png\" width=90%><br>\n",
    "\n",
    "En una imagen, la convolución consiste en recorrer (barrer) la imagen original completa con un *filtro* de dimensión $n\\times n$, con $n$ típicamente impar (usualmente 3, 5 o 7), calculando, a cada paso, el producto interior (de Frobenius) de la matriz que representa el núcleo de convolución por la matriz que se forma al seleccionar los pixeles bajo el filtro (*matriz subyacente*), como se muestra en la siguiente imagen:\n",
    "\n",
    "<img src=\"images/convol_2.png\" width=90%><br>\n",
    "\n",
    "El resultado de cada producto interior es asociado al pixel correspondiente al centro del filtro y registrado en el mapa transformado. Los pixeles en el borde de la imagen no coinciden con el centro del filtro en ningún paso del barrido, por lo que el mapa resultante, al eliminar los pixeles con valor indefinido, resultaría de tamaño menor al mapa original ($n-1$ pixeles menos). Usualmente se prefiere agregar valores arbitrarios para completar la matriz subyacente, de manera que se pueda calcular valores para los pixeles en el borde de la imagen y mantener el tamaño de la imagen original. Una opción frecuente es rellenar los valores 'faltantes' con cero:\n",
    "\n",
    "<img src=\"images/convol_3.png\" width=45%><br>\n",
    "\n",
    "El proceso humano de percepción tricromática de la luz hace natural describir los colores como una combinación de tres colores 'primarios', siendo uno de los modelos más conocidos el llamado RGB ('Red', 'Green', 'Blue'). Una imagen en color, entonces, suele representarse como la superposición de tres 'imágenes parciales', una para cada canal o color primario.\n",
    "\n",
    "<img src=\"images/Lenna_RGB.png\" width=75%><br>\n",
    "\n",
    "La convolución sobre una imagen en color se realiza utilizando un *kernel* con la misma profundidad; en el caso RGB se utiliza un *kernel* de 3 capas. Cada capa en el *kernel* recorre la capa correspondiente en el mapa original y produce un resultado parcial. Los resultados parciales se integran para generar el mapa de características transformado, de un sólo canal. \n",
    "\n",
    "<img src=\"images/convol_4.png\" width=50%><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitectura de la red neuronal convolucional\n",
    "\n",
    "Los procesos de percepción visual y reconocimiento de patrones por parte del cerebro, incluyendo los aspectos planteados aquí, es un tema aún en discusión por los especialistas en neurociencias. Este conocimiento, sin embargo, se tomó como base para desarrollar una arquitectura de red neuronal particularmente exitosa para el reconocimiento de  patrones en imágenes y que son responsables, en buena medida del nuevo impulso en el área de redes neuronales artificiales: las redes neuronales convolucionales.\n",
    "\n",
    "Una red neuronal presenta la siguiente arquitectura: \n",
    "\n",
    "<img src=\"images/neuron11.png\" width=80%><br>\n",
    "\n",
    "Aquí se distinguen dos secciones generales. La primera sección está formada, principalmente, por una o más capas de convolución y su objetivo es resaltar los rasgos en la imagen y reducir la dimensión de los datos. Esta sección corresponde a una una etapa de extracción automátizada y opaca de características. La segunda sección, es un perceptrón multicapa, utilizado para hacer la clasificación.\n",
    "\n",
    "Las diferentes funciones en la sección de extracción de características se modelan a través de capas de neuronas especializadas. El tipo de capa más importante en esta sección es la *capa de convolución*, sin embargo, resulta útil modelar otras tareas mediante capas (ese es el enfoque en TensorFlow, por ejemplo), incluso la función de activación suele describirse mediante una capa de neuronas especializada. A continuación se describen los principales tipos de capas utilizadas en una red convolucional.\n",
    "\n",
    "\n",
    "### Capa de convolución \n",
    "\n",
    "La *capa de convolución* es la componente principal de la sección de extracción de características en una red convolucional. Cada capa de convolución suele estar conformada por una serie de filtros destinados a identificar diferentes rasgos de un mismo mapa de características. Cada filtro $f_i$ procesa, de manera independiente cada uno de los canales de entrada e integr los resultados parciales ara generar un canal $c_i$ en el mapa de salida. \n",
    "\n",
    "<img src=\"images/neuron15.png\" width=50%><br>\n",
    "\n",
    "Desde un punto de vista neuronal, conviene visualizar la capa convolucional como si estuviera formada por un conjunto de neuronas organizadas en grupos, con cada grupo de neuronas $gn_i$ asociado a un filtro $f_i$. En este modelo, cada neurona en el grupo $gn_i$ utiliza como pesos sinápticos la matriz de valores del filtro $f_i$. Por lo tanto, sólo un subgrupo de señales de  entrada son capaces de estimular a la neurona, aquellas señales del mapa de entrada que coinciden con la cobertura del filtro, centrado en la posición de la neurona, como se muestra en la imagen.\n",
    "\n",
    "<img src=\"images/neuron14.png\" width=40%><br>\n",
    "\n",
    "Si $\\mathbf{x}$ es la matriz de señales de entrada bajo el filtro y $\\mathbf{w}$ la matriz de valores del filtro, entonces, la activación en la neurona está dada por \n",
    "\n",
    "$$a = \\mathbf{x}\\cdot \\mathbf{w}$$\n",
    "\n",
    "La activación alimenta la función de activación y la salida de esta función corresponde al valor en el mapa de salida en la misma posición que la neurona. Cada neurona en el grupo de neuronas genera una señal en el mapa de salida. Si la función de activación es la función identidad, entonces el mapa de salida coincide con la forma usual de la convolución sobre una imagen, como fue descrita anteriormente. Si el mapa de entrada contiene varios canales, las neuronas en cada grupo (filtro) procesan integran las señales de los diferentes canales de entrada en un sólo mapa de salida.\n",
    "\n",
    "La capa convolucional puede, entonces, esquematizarse como un conjunto de grupos especializados de neuronas, cada grupo asociado a un filtro (que le da la especialización a las neuronas), como se muestra en la siguiente figura:\n",
    "\n",
    "<img src=\"images/neuron16.png\" width=70%><br>\n",
    "\n",
    "En el modelo neuronal, cada grupo de neuronas $gn_i$ genera un canal de información $c_i$. Todos juntos, estos canales componen el mapa de salida.\n",
    "\n",
    "La función de activación más utilizada en las capas de convolución (y en redes neuronales profundas, en general) es el *rectificador*, definido como:\n",
    "\n",
    "$$g(x)=\\max(0,x)$$\n",
    "\n",
    "Una neurona que utiliza esta función de activación se conoce como *relu* (del término en inglés para *unidad lineal rectificada*).\n",
    "\n",
    "Hay que observar que las neuronas en una capa convolucional no se encuentran interconectadas; cada neurona recibe sus datos de entrada y proyecta su salida al mapa transformado. De esta mnera, la organización en el conjunto de neuronas es más parecido a la que se observa en el nervio óptico que a la mostrada en un circuito neuronal.\n",
    "\n",
    "#### Paso de barrido\n",
    "\n",
    "Si las neuronas están separadas entre sí a más de una unidad, es decir, si se define un paso de barrido mayor a uno, entonces la cobertura sobre el mapa original no será continua. \n",
    "\n",
    "<img src=\"images/convol_5.png\" width=75%><br>\n",
    "\n",
    "El resultado, en este caso, será un mapa de salida menor al mapa de entrada. El tamaño del mapa resultante de la convolución, para un mapa original de tamaño $w\\times h$, con un *kernel* de $n\\times n$, con $r$ pixeles de relleno y un paso de barrido $p$ es: \n",
    "\n",
    "$$\n",
    "[(w+2r-n)/p+1] \\times [(h+2r-n)/p+1] \n",
    "$$\n",
    "\n",
    "Considérese, por ejemplo, un mapa inicial de $100\\times 100$ pixeles que es convolucionado con un *kernel* de $3\\times 3$ pixeles con un paso $p=2$ y usando un relleno $r=1$. El tamaño del mapa transformado sería:\n",
    "\n",
    "$$\n",
    "[(100+2-3)/2 + 1] \\times [(100+2-3)/2 + 1] =  50\\times 50\n",
    "$$\n",
    "\n",
    "Esto equivale a un submuestreo de la salida, tomando, en el caso del ejemplo anterior, sólo la mitad de los datos. Para valores de paso mayores a $(n+1)/2$ habrá también un submuestreo de la entrada, es decir, habrá señales de entrada que no influyen en la salida.\n",
    "\n",
    "<img src=\"images/convol_6.png\" width=20%><br>\n",
    "\n",
    "### Capa de normalización por lotes\n",
    "\n",
    "La capa de normalización por lotes es un artefacto utilizado para modificar la salida de la capa de convolución. Se utiliza para aumentar la velocidad de entrenamiento de las redes neuronales profundas, siendo capaz de reducir a menos de una décima parte los pasos de entrenamiento. La razón por la que esta técnica funciona es confusa, sin embargo, esta capa es una componente común en las redes neuronales convolucionales. \n",
    "\n",
    "La operación que implementa una capa de normalización por lotes es simple: \n",
    "\n",
    "1. Normalizar la salida $z_i^{(l)}$ de cada neurona, utilizando la media y la varianza de los datos en el lote de entrenamiento, generando la nueva señal $z_i^{(l)}\\vert_{norm}$.\n",
    "\n",
    "2. Generar la salida normalizada $\\tilde{z}_i^{(l)} = \\gamma z_i^{(l)}\\vert_{norm} + \\beta$, siendo $\\gamma$ y $\\beta$ parámetros adicionales cuytos valores son determinados por entrenamiento.\n",
    "\n",
    "De esta manera, la capa de normalización mantiene las salidas de todas las neuronas a las que se aplica en un rango \"normal\" a los datos de entrenamiento.\n",
    "\n",
    "Entre los temas aún bajo discusión se encuentra dónde colocar la capa de normalización por lotes, con cierto consenso en que conviene colocarla antes de la función de activación.\n",
    "\n",
    "<img src=\"images/neuron17.png\" width=40%><br>\n",
    "\n",
    "De esta manera, aunque en la implementación se suele hablar de capas de normalización por lotes y de activación, en realidad habría que considerar estas capas como modificadores de la capa de convolución.\n",
    "\n",
    "### Capa de agrupación (*pooling*)\n",
    "\n",
    "La capa de agrupación ofrece otra opción de submuestreo (adicionalmente al uso de pasos de barrido mayor a uno discutido anteriormente) sobre la salida del proceso de convolución. El principal objetivo, en este caso, es reducir la dimensión de un mapa de características mediante algún proceso de selección o de agregación.\n",
    "\n",
    "Desde las perspectiva de procesamiento de imágenes/señales, esta forma de submuestreo se obtiene utilizando un filtro que recorre la imagen, típicamente sin traslape, y regresa un valor representativo de la ventana.\n",
    "\n",
    "<img src=\"images/convol_7.png\" width=50%><br>\n",
    "\n",
    "La opción más usual es utilizar un filtro de submuestreo de $2\\times 2$ pixeles con un paso de barrido de 2. El resultado es una reducción del mapa de entrada a una cuarta parte de su tamaño. El filtro se aplica a cada canal en el mapa de características, generando el mismo número de canales, pero submuestreados.\n",
    "\n",
    "El valor de salida del filtro puede ser cualquier estadístico, como puede ser el valor máximo, el mínimo, la moda, la mediana o la media. Las opciones disponibles en TensorFlow las opciones disponibles son el máximo y la media; en Deeplearnig4j se encuentra además la $p$-norma. El filtro de valor máximo realiza, además de la reducción de tamaño (asumiendo un paso de barrido adecuado), un filtrado no lineal de ruido que enfatiza los principales rasgos en el mapa; por su parte, el filtrado de media realiza un suavizado del mapa y es menos sensible a valores atípicos. Como en el caso de filtrado estandar sobre imágenes, el filtro más adecuado dependerá de cada caso específico. No obstante, el filtro más empleado en redes convolucionales es el filtro de valor máximo.\n",
    "\n",
    "Desde la perspectiva neuronal, la capa de muestreo se visualiza como un conjunto de neuronas que reciben como entrada un segmento de datos contiguos del mapa original y lo transforman en su salida. En el caso del filtro de media, los pesos para todas las entradas tienen el mismo valor ($\\frac{1}{n\\times m}$ con $n\\times m$ el tamaño del filtro). En el caso del filtro de valor máximo la metáfora es menos clara; los pesos para cada neurona son:\n",
    "\n",
    "$$\n",
    "w_{ij} = \\left\\{\n",
    "      \\begin{array}{ll}\n",
    "         1 & \\textrm{si } x_{ij} \\textrm{ es el valor máximo en } \\mathbf{x} \\\\\n",
    "         0 & \\textrm{cualquier otro caso}\n",
    "      \\end{array} \\right.\n",
    "$$\n",
    "\n",
    "Siendo $ij$ el índice sobre la matriz de señales de entrada $\\mathbf{x}$.\n",
    "\n",
    "Nuevamente, todas las neuronas en la capa de submuestro utilizan el mismo conjunto de pesos y lo aplican por separado a cada canal de entrada para producir, en conjunto, un mapa de salida con el mismo número de canales.\n",
    "\n",
    "Alternativamente, en lugar de utilizar una capa de agregación puede utilizarse un paso de barrido del *kernel* sobre la capa de convolución mayor a 1. Usualmente se emplea una u otra opción: paso de barrido mayor a uno o capa de agrupación. Hay que observar que, incrementar el paso de barrido para reducir el tamaño del mapa transformado es más económico que utilizar una capa de submuestreo.\n",
    "\n",
    "### Capa de aplanado (*flatten*)\n",
    "\n",
    "La transición entre la sección de extracción de características, que produce mapas 3D y la sección de clasificación, donde el multiperceptrón recibe datos en formato lineal, se realiza a través de una capa especial llamada de aplanado o *flatten*. La función de esta capa es reacomodar las señales en cada canal, apilándolas en un vector unidimensional.\n",
    "\n",
    "<img src=\"images/convol_8.png\" width=40%><br>\n",
    "\n",
    "### Capa de deserción (*dropout*)\n",
    "\n",
    "La sección de clasificación consiste, básicamente, en un perceptrón multicapa. El único rasgo adicional en su implementación en la red convolucional es el uso de un tipo de capa llamado de deserción o de *dropout*, que es común en las arquitecturas de redes neuronales para aprendizaje profundo. \n",
    "\n",
    "La deserción es una técnica de regularización cuyo objetivo es simplificar la complejidad de la red y, simultáneamente, reducir el problema de sobreentrenamiento para casos con datos insuficientes. En cada iteración de entrenamiento, un determinado porcentaje de nodos, tomados al azar, son eliminados del entrenamiento (sus activaciones son ignoradas). El resultado es, en ese paso de entrenamiento, equivalente a tener una red más pequeña: \n",
    "\n",
    "<img src=\"images/neuron18.png\" width=70%><br>\n",
    "\n",
    "Puesto que, en la siguiente iteración los nodos seleccionados para eliminación en el entrenamiento será diferente, el resultado final de entrenamiento será semejante a haber entrenado varias redes neuronales traslapadas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "NEW_TRY = False #True\n",
    "PATH = 'Data sets/fruits/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def set_training_data(rows, columns, channels):\n",
    "    images_path = PATH + 'images/'\n",
    "    directories = [d for d in os.listdir(images_path) \n",
    "                  if (os.path.isdir(os.path.join(images_path, d)))]\n",
    "\n",
    "    labels_idx = {}\n",
    "    images_dataset = {}\n",
    "    for i, d in zip(range(len(directories)), directories):\n",
    "        label_dir = os.path.join(images_path, d)\n",
    "        category = d.capitalize()\n",
    "        labels_idx[category] = i\n",
    "\n",
    "        names = [f for f in os.listdir(label_dir) \n",
    "                 if f.endswith(\".jpg\") or f.endswith(\".png\")]\n",
    "        \n",
    "        for n in names:\n",
    "            file_name = os.path.join(label_dir, n)\n",
    "            images_dataset[str(i) + \"_\" + n.split(\".\")[0]] = [file_name, category]\n",
    "\n",
    "    num_categories = len(labels_idx)\n",
    "\n",
    "    train_data, test_data = train_test_split(list(images_dataset.keys()), test_size=0.2)\n",
    "    test_y = np.asarray([labels_idx[images_dataset[x][1]] for x in test_data])\n",
    "    train_y = np.asarray([labels_idx[images_dataset[x][1]] for x in train_data])\n",
    "\n",
    "    image_size=(rows, columns)\n",
    "    image_shape=(rows, columns, channels)\n",
    "\n",
    "    training_data = {\n",
    "        'train_data': train_data,\n",
    "        'test_data': test_data,\n",
    "        'train_y': train_y,\n",
    "        'test_y': test_y,\n",
    "        'num_categories': num_categories,\n",
    "        'image_size': image_size,\n",
    "        'image_shape': image_shape,\n",
    "        'images_dataset': images_dataset\n",
    "    }\n",
    "    with open(PATH + 'chk/training_data.pickle', 'wb') as handle:\n",
    "        pickle.dump(training_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    return training_data\n",
    "\n",
    "def load_training_data():\n",
    "    with open(PATH + 'chk/training_data.pickle', 'rb') as handle:\n",
    "        training_data = pickle.load(handle)\n",
    "        \n",
    "    return training_data\n",
    "\n",
    "#_____________________________________________________________________________\n",
    "\n",
    "if NEW_TRY:\n",
    "    training_data = set_training_data(100, 100, 3)\n",
    "else:\n",
    "    training_data = load_training_data()\n",
    "    \n",
    "\n",
    "train_data = training_data['train_data']\n",
    "test_data = training_data['test_data']\n",
    "train_y = training_data['train_y']\n",
    "test_y = training_data['test_y']\n",
    "num_categories = training_data['num_categories']\n",
    "image_size = training_data['image_size']\n",
    "image_shape = training_data['image_shape']\n",
    "images_dataset = training_data['images_dataset']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redes convolucionales con Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 50, 50, 32)        2400      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 50, 50, 32)        200       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 50, 50, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 50, 50, 64)        18432     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 25, 25, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 25, 25, 64)        100       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 25, 25, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 25, 25, 128)       73728     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_2 (Average (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 12, 12, 128)       48        \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 12, 12, 256)       294912    \n",
      "_________________________________________________________________\n",
      "average_pooling2d_3 (Average (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 6, 6, 256)         24        \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2048)              18876416  \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 768)               1573632   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 103)               52839     \n",
      "=================================================================\n",
      "Total params: 21,286,459\n",
      "Trainable params: 21,286,273\n",
      "Non-trainable params: 186\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, model_from_json\n",
    "from keras import layers, regularizers as regs\n",
    "from keras.models import load_model\n",
    "\n",
    "def create_model():\n",
    "    tf_model = Sequential([\n",
    "        layers.Conv2D(32,             # Aplicar pocos filtros destinados a \n",
    "                      kernel_size=(5, 5), # ... identificar rasgos grandes\n",
    "                      input_shape=image_shape,\n",
    "                      padding='same', # Mantener tamaño original de la imagen\n",
    "                      strides=(2, 2), # Barrer la imagen en pasos de 2 pixeles\n",
    "                      use_bias=False  # Bias innecesario por 'BatchNormalization'\n",
    "                     ),\n",
    "        layers.BatchNormalization(axis = 1), # La normalización debe ser\n",
    "        layers.Activation(\"relu\"),           # anterior a la activación        \n",
    "        \n",
    "        layers.Conv2D(64, kernel_size=(3, 3), padding='same', use_bias=False),\n",
    "        layers.AveragePooling2D(pool_size=(2, 2)), # Reducir utilizando la media\n",
    "        layers.BatchNormalization(axis = 1),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Conv2D(128, kernel_size=(3, 3), padding='same', use_bias=False),\n",
    "        layers.AveragePooling2D(pool_size=(2, 2)),\n",
    "        layers.BatchNormalization(axis = 1),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Conv2D(256, kernel_size=(3, 3), padding='same', use_bias=False),\n",
    "        layers.AveragePooling2D(pool_size=(2, 2)),\n",
    "        layers.BatchNormalization(axis = 1),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Flatten(),    \n",
    "        layers.Dense(2048, activation='relu', kernel_regularizer=regs.l2(0.001)),\n",
    "        layers.Dropout(0.15),\n",
    "        layers.Dense(768, activation='relu', kernel_regularizer=regs.l2(0.001)),\n",
    "        layers.Dropout(0.15),\n",
    "        layers.Dense(512, activation='relu', kernel_regularizer=regs.l2(0.001)),\n",
    "        layers.Dense(num_categories, activation='softmax') #120\n",
    "        ])\n",
    "    tf_model.compile(optimizer='adam',\n",
    "                    loss='sparse_categorical_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "    tf_model.save(PATH + 'chk/tf_model.h5')\n",
    "    tf_model.summary()\n",
    "\n",
    "    return tf_model\n",
    "\n",
    "def reload_model():    \n",
    "    tf_model = load_model(PATH + 'chk/tf_model10.h5')\n",
    "    tf_model.summary()\n",
    "\n",
    "    return tf_model\n",
    "\n",
    "#_____________________________________________________________________________\n",
    "#NEW_TRY = True\n",
    "\n",
    "if NEW_TRY:\n",
    "    tf_model = create_model()\n",
    "else:\n",
    "    tf_model = reload_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 233s 23ms/step - loss: 4.8788 - acc: 0.1516 - val_loss: 3.0706 - val_acc: 0.2885\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 229s 22ms/step - loss: 2.6244 - acc: 0.3727 - val_loss: 2.0192 - val_acc: 0.5092\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 226s 22ms/step - loss: 2.0456 - acc: 0.4920 - val_loss: 1.5814 - val_acc: 0.6288\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 1.6684 - acc: 0.5917 - val_loss: 1.3514 - val_acc: 0.6992\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 226s 22ms/step - loss: 1.4385 - acc: 0.6631 - val_loss: 1.0497 - val_acc: 0.7951\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 230s 22ms/step - loss: 1.3245 - acc: 0.7065 - val_loss: 1.0907 - val_acc: 0.7757\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 228s 22ms/step - loss: 1.1423 - acc: 0.7583 - val_loss: 0.8262 - val_acc: 0.8619\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.9769 - acc: 0.8071 - val_loss: 0.7406 - val_acc: 0.8865\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 227s 22ms/step - loss: 0.9495 - acc: 0.8265 - val_loss: 0.7446 - val_acc: 0.8997\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.8518 - acc: 0.8528 - val_loss: 0.8043 - val_acc: 0.8742\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 223s 22ms/step - loss: 0.8153 - acc: 0.8625 - val_loss: 0.5881 - val_acc: 0.9340\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 227s 22ms/step - loss: 0.7545 - acc: 0.8801 - val_loss: 0.6288 - val_acc: 0.9217\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 226s 22ms/step - loss: 0.7210 - acc: 0.8875 - val_loss: 0.5749 - val_acc: 0.9428\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 229s 22ms/step - loss: 0.6755 - acc: 0.9025 - val_loss: 0.5415 - val_acc: 0.9428\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 230s 22ms/step - loss: 0.6450 - acc: 0.9007 - val_loss: 0.4863 - val_acc: 0.9578\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 230s 22ms/step - loss: 0.6500 - acc: 0.9046 - val_loss: 0.5275 - val_acc: 0.9631\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 222s 22ms/step - loss: 0.6584 - acc: 0.9146 - val_loss: 0.5631 - val_acc: 0.9270\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 220s 21ms/step - loss: 0.5969 - acc: 0.9151 - val_loss: 0.4726 - val_acc: 0.9560\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 219s 21ms/step - loss: 0.5712 - acc: 0.9207 - val_loss: 0.4512 - val_acc: 0.9604\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 2741s 268ms/step - loss: 0.5513 - acc: 0.9280 - val_loss: 0.4649 - val_acc: 0.9516\n",
      "\n",
      "Train on 10224 samples, validate on 1136 samples\n",
      "Epoch 1/5\n",
      "10224/10224 [==============================] - 2145s 210ms/step - loss: 0.5857 - acc: 0.9248 - val_loss: 0.4137 - val_acc: 0.9780\n",
      "Epoch 2/5\n",
      "10224/10224 [==============================] - 2737s 268ms/step - loss: 0.5338 - acc: 0.9281 - val_loss: 0.4267 - val_acc: 0.9665\n",
      "Epoch 3/5\n",
      "10224/10224 [==============================] - 3944s 386ms/step - loss: 0.5111 - acc: 0.9308 - val_loss: 0.4920 - val_acc: 0.9419\n",
      "Epoch 4/5\n",
      "10224/10224 [==============================] - 346s 34ms/step - loss: 0.5110 - acc: 0.9289 - val_loss: 0.4149 - val_acc: 0.9648\n",
      "Epoch 5/5\n",
      "10224/10224 [==============================] - 884s 86ms/step - loss: 0.4893 - acc: 0.9413 - val_loss: 0.3683 - val_acc: 0.9789\n",
      "\n",
      "EPOCH END\n",
      "\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 2855s 279ms/step - loss: 0.5104 - acc: 0.9344 - val_loss: 0.4232 - val_acc: 0.9736\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 3943s 386ms/step - loss: 0.4932 - acc: 0.9456 - val_loss: 0.4359 - val_acc: 0.9622\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 3944s 386ms/step - loss: 0.4627 - acc: 0.9452 - val_loss: 0.3585 - val_acc: 0.9824\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 3941s 385ms/step - loss: 0.4560 - acc: 0.9447 - val_loss: 0.4624 - val_acc: 0.9376\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 3945s 386ms/step - loss: 0.4391 - acc: 0.9488 - val_loss: 0.3838 - val_acc: 0.9631\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 2639s 258ms/step - loss: 0.4428 - acc: 0.9470 - val_loss: 0.3407 - val_acc: 0.9921\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 3942s 385ms/step - loss: 0.4190 - acc: 0.9559 - val_loss: 0.3305 - val_acc: 0.9824\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 2258s 221ms/step - loss: 0.4057 - acc: 0.9537 - val_loss: 0.3602 - val_acc: 0.9683\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 3940s 385ms/step - loss: 0.3962 - acc: 0.9584 - val_loss: 0.3800 - val_acc: 0.9613\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 3945s 386ms/step - loss: 0.4469 - acc: 0.9576 - val_loss: 0.3810 - val_acc: 0.9868\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 3942s 385ms/step - loss: 0.4856 - acc: 0.9459 - val_loss: 0.3678 - val_acc: 0.9719\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 3943s 386ms/step - loss: 0.3984 - acc: 0.9581 - val_loss: 0.2970 - val_acc: 0.9930\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 3944s 386ms/step - loss: 0.3825 - acc: 0.9592 - val_loss: 0.3056 - val_acc: 0.9798\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 2511s 245ms/step - loss: 0.3713 - acc: 0.9585 - val_loss: 0.2712 - val_acc: 0.9938\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 3944s 386ms/step - loss: 0.3753 - acc: 0.9621 - val_loss: 0.2842 - val_acc: 0.9877\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 3942s 385ms/step - loss: 0.3837 - acc: 0.9538 - val_loss: 0.2910 - val_acc: 0.9886\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 3945s 386ms/step - loss: 0.3502 - acc: 0.9645 - val_loss: 0.2822 - val_acc: 0.9868\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 3943s 386ms/step - loss: 0.3451 - acc: 0.9654 - val_loss: 0.3063 - val_acc: 0.9833\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 3942s 385ms/step - loss: 0.3637 - acc: 0.9599 - val_loss: 0.2694 - val_acc: 0.9894\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 2180s 213ms/step - loss: 0.3333 - acc: 0.9686 - val_loss: 0.3033 - val_acc: 0.9798\n",
      "\n",
      "Train on 10224 samples, validate on 1136 samples\n",
      "Epoch 1/5\n",
      "10224/10224 [==============================] - 9312s 911ms/step - loss: 0.3632 - acc: 0.9594 - val_loss: 0.2794 - val_acc: 0.9912\n",
      "Epoch 2/5\n",
      "10224/10224 [==============================] - 3957s 387ms/step - loss: 0.3419 - acc: 0.9647 - val_loss: 0.2770 - val_acc: 0.9886\n",
      "Epoch 3/5\n",
      "10224/10224 [==============================] - 3960s 387ms/step - loss: 0.3143 - acc: 0.9712 - val_loss: 0.2787 - val_acc: 0.9877\n",
      "Epoch 4/5\n",
      "10224/10224 [==============================] - 3959s 387ms/step - loss: 0.3184 - acc: 0.9703 - val_loss: 0.2652 - val_acc: 0.9894\n",
      "Epoch 5/5\n",
      "10224/10224 [==============================] - 3962s 388ms/step - loss: 0.4180 - acc: 0.9637 - val_loss: 0.3502 - val_acc: 0.9850\n",
      "\n",
      "EPOCH END\n",
      "\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 3963s 387ms/step - loss: 0.3369 - acc: 0.9699 - val_loss: 0.2573 - val_acc: 0.9930\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 3961s 387ms/step - loss: 0.3299 - acc: 0.9635 - val_loss: 0.3010 - val_acc: 0.9771\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 3960s 387ms/step - loss: 0.3173 - acc: 0.9706 - val_loss: 0.2583 - val_acc: 0.9868\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 3958s 387ms/step - loss: 0.3061 - acc: 0.9688 - val_loss: 0.2421 - val_acc: 0.9930\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 3960s 387ms/step - loss: 0.3144 - acc: 0.9686 - val_loss: 0.2392 - val_acc: 0.9965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 3950s 386ms/step - loss: 0.3186 - acc: 0.9657 - val_loss: 0.2621 - val_acc: 0.9886\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 3957s 387ms/step - loss: 0.3280 - acc: 0.9692 - val_loss: 0.2473 - val_acc: 0.9930\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 3958s 387ms/step - loss: 0.2979 - acc: 0.9700 - val_loss: 0.2291 - val_acc: 0.9947\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 3959s 387ms/step - loss: 0.2943 - acc: 0.9723 - val_loss: 0.2589 - val_acc: 0.9868\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 3960s 387ms/step - loss: 0.2786 - acc: 0.9773 - val_loss: 0.2962 - val_acc: 0.9719\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 3973s 388ms/step - loss: 0.2834 - acc: 0.9745 - val_loss: 0.2184 - val_acc: 0.9903\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 3957s 387ms/step - loss: 0.2760 - acc: 0.9742 - val_loss: 0.2077 - val_acc: 0.9982\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 3965s 388ms/step - loss: 0.3000 - acc: 0.9709 - val_loss: 0.2294 - val_acc: 0.9921\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 3974s 389ms/step - loss: 0.2608 - acc: 0.9823 - val_loss: 0.2042 - val_acc: 0.9956\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 3973s 389ms/step - loss: 0.2780 - acc: 0.9727 - val_loss: 0.2196 - val_acc: 0.9947\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 3953s 387ms/step - loss: 0.2719 - acc: 0.9758 - val_loss: 0.2430 - val_acc: 0.9956\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 3967s 388ms/step - loss: 0.3573 - acc: 0.9741 - val_loss: 0.2464 - val_acc: 0.9894\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 2753s 269ms/step - loss: 0.2697 - acc: 0.9765 - val_loss: 0.2171 - val_acc: 0.9956\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 3964s 388ms/step - loss: 0.2773 - acc: 0.9754 - val_loss: 0.2109 - val_acc: 0.9938\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 3983s 389ms/step - loss: 0.2576 - acc: 0.9775 - val_loss: 0.2229 - val_acc: 0.9877\n",
      "\n",
      "Train on 10224 samples, validate on 1136 samples\n",
      "Epoch 1/5\n",
      "10224/10224 [==============================] - 1057s 103ms/step - loss: 0.2761 - acc: 0.9714 - val_loss: 0.2476 - val_acc: 0.9859\n",
      "Epoch 2/5\n",
      "10224/10224 [==============================] - 4403s 431ms/step - loss: 0.2688 - acc: 0.9735 - val_loss: 0.2110 - val_acc: 0.9938\n",
      "Epoch 3/5\n",
      "10224/10224 [==============================] - 3958s 387ms/step - loss: 0.2674 - acc: 0.9756 - val_loss: 0.2334 - val_acc: 0.9842\n",
      "Epoch 4/5\n",
      "10224/10224 [==============================] - 3962s 388ms/step - loss: 0.2621 - acc: 0.9752 - val_loss: 0.2156 - val_acc: 0.9921\n",
      "Epoch 5/5\n",
      "10224/10224 [==============================] - 3963s 388ms/step - loss: 0.2459 - acc: 0.9802 - val_loss: 0.2298 - val_acc: 0.9859\n",
      "\n",
      "EPOCH END\n",
      "\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 3963s 387ms/step - loss: 0.2429 - acc: 0.9804 - val_loss: 0.2455 - val_acc: 0.9833\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 3964s 388ms/step - loss: 0.2712 - acc: 0.9735 - val_loss: 0.2157 - val_acc: 0.9956\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 3960s 387ms/step - loss: 0.2498 - acc: 0.9803 - val_loss: 0.2036 - val_acc: 0.9947\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 3961s 387ms/step - loss: 0.2404 - acc: 0.9814 - val_loss: 0.1889 - val_acc: 0.9974\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 3959s 387ms/step - loss: 0.2392 - acc: 0.9803 - val_loss: 0.1813 - val_acc: 0.9991\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 3954s 387ms/step - loss: 0.2504 - acc: 0.9768 - val_loss: 0.2396 - val_acc: 0.9824\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 3960s 387ms/step - loss: 0.2869 - acc: 0.9773 - val_loss: 0.2763 - val_acc: 0.9956\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 2492s 244ms/step - loss: 0.2687 - acc: 0.9777 - val_loss: 0.2077 - val_acc: 0.9921\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 229s 22ms/step - loss: 0.2462 - acc: 0.9786 - val_loss: 0.2559 - val_acc: 0.9807\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 220s 22ms/step - loss: 0.2334 - acc: 0.9798 - val_loss: 0.2058 - val_acc: 0.9868\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 219s 21ms/step - loss: 0.2592 - acc: 0.9720 - val_loss: 0.2009 - val_acc: 0.9921\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 217s 21ms/step - loss: 0.2221 - acc: 0.9827 - val_loss: 0.2098 - val_acc: 0.9859\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 2124s 208ms/step - loss: 0.2244 - acc: 0.9818 - val_loss: 0.1792 - val_acc: 0.9956\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 344s 34ms/step - loss: 0.2297 - acc: 0.9794 - val_loss: 0.2111 - val_acc: 0.9877\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 565s 55ms/step - loss: 0.2347 - acc: 0.9779 - val_loss: 0.1915 - val_acc: 0.9912\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 544s 53ms/step - loss: 0.2468 - acc: 0.9771 - val_loss: 0.1886 - val_acc: 0.9974\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 546s 53ms/step - loss: 0.2361 - acc: 0.9785 - val_loss: 0.1834 - val_acc: 0.9974\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.2339 - acc: 0.9802 - val_loss: 0.1769 - val_acc: 0.9991\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 227s 22ms/step - loss: 0.2318 - acc: 0.9806 - val_loss: 0.1861 - val_acc: 0.9956\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 228s 22ms/step - loss: 0.2108 - acc: 0.9851 - val_loss: 0.1679 - val_acc: 0.9974\n",
      "\n",
      "Train on 10224 samples, validate on 1136 samples\n",
      "Epoch 1/5\n",
      "10224/10224 [==============================] - 232s 23ms/step - loss: 0.2247 - acc: 0.9813 - val_loss: 0.1866 - val_acc: 0.9947\n",
      "Epoch 2/5\n",
      "10224/10224 [==============================] - 229s 22ms/step - loss: 0.2328 - acc: 0.9773 - val_loss: 0.1811 - val_acc: 0.9938\n",
      "Epoch 3/5\n",
      "10224/10224 [==============================] - 228s 22ms/step - loss: 0.2194 - acc: 0.9835 - val_loss: 0.1911 - val_acc: 0.9894\n",
      "Epoch 4/5\n",
      "10224/10224 [==============================] - 229s 22ms/step - loss: 0.2084 - acc: 0.9837 - val_loss: 0.1771 - val_acc: 0.9912\n",
      "Epoch 5/5\n",
      "10224/10224 [==============================] - 226s 22ms/step - loss: 0.2158 - acc: 0.9819 - val_loss: 0.1688 - val_acc: 0.9965\n",
      "\n",
      "EPOCH END\n",
      "\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 231s 23ms/step - loss: 0.2041 - acc: 0.9848 - val_loss: 0.1678 - val_acc: 0.9956\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 237s 23ms/step - loss: 0.2158 - acc: 0.9814 - val_loss: 0.1726 - val_acc: 0.9947\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 230s 22ms/step - loss: 0.2315 - acc: 0.9815 - val_loss: 0.1699 - val_acc: 0.9991\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 227s 22ms/step - loss: 0.2554 - acc: 0.9831 - val_loss: 0.2549 - val_acc: 0.9938\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 229s 22ms/step - loss: 0.2521 - acc: 0.9827 - val_loss: 0.1959 - val_acc: 0.9938\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 226s 22ms/step - loss: 0.2183 - acc: 0.9819 - val_loss: 0.1863 - val_acc: 0.9938\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.2198 - acc: 0.9795 - val_loss: 0.1748 - val_acc: 0.9938\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 229s 22ms/step - loss: 0.2202 - acc: 0.9795 - val_loss: 0.1760 - val_acc: 0.9965\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 230s 22ms/step - loss: 0.1985 - acc: 0.9854 - val_loss: 0.1600 - val_acc: 0.9982\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 232s 23ms/step - loss: 0.1992 - acc: 0.9843 - val_loss: 0.1763 - val_acc: 0.9912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 219s 21ms/step - loss: 0.2282 - acc: 0.9753 - val_loss: 0.1706 - val_acc: 0.9947\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 219s 21ms/step - loss: 0.2085 - acc: 0.9827 - val_loss: 0.1576 - val_acc: 0.9982\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 219s 21ms/step - loss: 0.1885 - acc: 0.9865 - val_loss: 0.1653 - val_acc: 0.9912\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.2128 - acc: 0.9816 - val_loss: 0.1711 - val_acc: 0.9930\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 227s 22ms/step - loss: 0.2129 - acc: 0.9827 - val_loss: 0.1611 - val_acc: 0.9956\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 222s 22ms/step - loss: 0.1943 - acc: 0.9866 - val_loss: 0.1620 - val_acc: 0.9938\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 228s 22ms/step - loss: 0.2174 - acc: 0.9777 - val_loss: 0.1929 - val_acc: 0.9868\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 228s 22ms/step - loss: 0.1957 - acc: 0.9866 - val_loss: 0.1535 - val_acc: 0.9982\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.2004 - acc: 0.9833 - val_loss: 0.1955 - val_acc: 0.9868\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.2030 - acc: 0.9864 - val_loss: 0.1651 - val_acc: 0.9947\n",
      "\n",
      "Train on 10224 samples, validate on 1136 samples\n",
      "Epoch 1/5\n",
      "10224/10224 [==============================] - 224s 22ms/step - loss: 0.2148 - acc: 0.9791 - val_loss: 0.1702 - val_acc: 0.9974\n",
      "Epoch 2/5\n",
      "10224/10224 [==============================] - 223s 22ms/step - loss: 0.1998 - acc: 0.9858 - val_loss: 0.1555 - val_acc: 0.9965\n",
      "Epoch 3/5\n",
      "10224/10224 [==============================] - 229s 22ms/step - loss: 0.1985 - acc: 0.9859 - val_loss: 0.1660 - val_acc: 0.9947\n",
      "Epoch 4/5\n",
      "10224/10224 [==============================] - 223s 22ms/step - loss: 0.2043 - acc: 0.9824 - val_loss: 0.1521 - val_acc: 0.9982\n",
      "Epoch 5/5\n",
      "10224/10224 [==============================] - 222s 22ms/step - loss: 0.1819 - acc: 0.9869 - val_loss: 0.1758 - val_acc: 0.9894\n",
      "\n",
      "EPOCH END\n",
      "\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.2375 - acc: 0.9830 - val_loss: 0.2417 - val_acc: 0.9921\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.2284 - acc: 0.9846 - val_loss: 0.1576 - val_acc: 0.9991\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 223s 22ms/step - loss: 0.1845 - acc: 0.9868 - val_loss: 0.1610 - val_acc: 0.9894\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 222s 22ms/step - loss: 0.1991 - acc: 0.9828 - val_loss: 0.1665 - val_acc: 0.9930\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 222s 22ms/step - loss: 0.2041 - acc: 0.9823 - val_loss: 0.1681 - val_acc: 0.9921\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 221s 22ms/step - loss: 0.2071 - acc: 0.9794 - val_loss: 0.1846 - val_acc: 0.9930\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 221s 22ms/step - loss: 0.1978 - acc: 0.9836 - val_loss: 0.1975 - val_acc: 0.9859\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 221s 22ms/step - loss: 0.2024 - acc: 0.9825 - val_loss: 0.1548 - val_acc: 0.9974\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 222s 22ms/step - loss: 0.1812 - acc: 0.9871 - val_loss: 0.1553 - val_acc: 0.9974\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 220s 22ms/step - loss: 0.1781 - acc: 0.9875 - val_loss: 0.1924 - val_acc: 0.9868\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 221s 22ms/step - loss: 0.2027 - acc: 0.9817 - val_loss: 0.1797 - val_acc: 0.9894\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 223s 22ms/step - loss: 0.1931 - acc: 0.9825 - val_loss: 0.1423 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 218s 21ms/step - loss: 0.1741 - acc: 0.9872 - val_loss: 0.1418 - val_acc: 0.9982\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 218s 21ms/step - loss: 0.1933 - acc: 0.9847 - val_loss: 0.1655 - val_acc: 0.9974\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 219s 21ms/step - loss: 0.1901 - acc: 0.9846 - val_loss: 0.1473 - val_acc: 1.0000\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 217s 21ms/step - loss: 0.2008 - acc: 0.9811 - val_loss: 0.1604 - val_acc: 0.9974\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 218s 21ms/step - loss: 0.1948 - acc: 0.9848 - val_loss: 0.1639 - val_acc: 0.9921\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 220s 22ms/step - loss: 0.1820 - acc: 0.9870 - val_loss: 0.1423 - val_acc: 0.9982\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 220s 22ms/step - loss: 0.1898 - acc: 0.9837 - val_loss: 0.1608 - val_acc: 0.9938\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 221s 22ms/step - loss: 0.1766 - acc: 0.9866 - val_loss: 0.2147 - val_acc: 0.9727\n",
      "\n",
      "Train on 10224 samples, validate on 1136 samples\n",
      "Epoch 1/5\n",
      "10224/10224 [==============================] - 222s 22ms/step - loss: 0.1841 - acc: 0.9861 - val_loss: 0.1373 - val_acc: 0.9982\n",
      "Epoch 2/5\n",
      "10224/10224 [==============================] - 221s 22ms/step - loss: 0.1855 - acc: 0.9851 - val_loss: 0.1471 - val_acc: 0.9947\n",
      "Epoch 3/5\n",
      "10224/10224 [==============================] - 224s 22ms/step - loss: 0.1715 - acc: 0.9881 - val_loss: 0.1641 - val_acc: 0.9877\n",
      "Epoch 4/5\n",
      "10224/10224 [==============================] - 231s 23ms/step - loss: 0.1783 - acc: 0.9849 - val_loss: 0.1452 - val_acc: 0.9947\n",
      "Epoch 5/5\n",
      "10224/10224 [==============================] - 226s 22ms/step - loss: 0.1673 - acc: 0.9888 - val_loss: 0.1449 - val_acc: 0.9965\n",
      "\n",
      "EPOCH END\n",
      "\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 226s 22ms/step - loss: 0.1922 - acc: 0.9805 - val_loss: 0.1804 - val_acc: 0.9903\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 226s 22ms/step - loss: 0.1877 - acc: 0.9863 - val_loss: 0.1431 - val_acc: 0.9991\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 221s 22ms/step - loss: 0.1769 - acc: 0.9874 - val_loss: 0.1806 - val_acc: 0.9912\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 222s 22ms/step - loss: 0.1827 - acc: 0.9869 - val_loss: 0.1443 - val_acc: 0.9991\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 221s 22ms/step - loss: 0.1866 - acc: 0.9851 - val_loss: 0.1477 - val_acc: 0.9956\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 222s 22ms/step - loss: 0.1870 - acc: 0.9838 - val_loss: 0.1584 - val_acc: 0.9921\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 221s 22ms/step - loss: 0.1899 - acc: 0.9843 - val_loss: 0.1627 - val_acc: 0.9938\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1605 - acc: 0.9914 - val_loss: 0.1433 - val_acc: 0.9956\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 232s 23ms/step - loss: 0.1754 - acc: 0.9868 - val_loss: 0.1589 - val_acc: 0.9930\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 230s 22ms/step - loss: 0.1892 - acc: 0.9831 - val_loss: 0.1787 - val_acc: 0.9894\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 228s 22ms/step - loss: 0.2564 - acc: 0.9809 - val_loss: 0.4107 - val_acc: 0.9938\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 231s 23ms/step - loss: 0.2627 - acc: 0.9892 - val_loss: 0.1676 - val_acc: 0.9991\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 231s 23ms/step - loss: 0.1631 - acc: 0.9941 - val_loss: 0.1381 - val_acc: 0.9991\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 220s 21ms/step - loss: 0.1702 - acc: 0.9861 - val_loss: 0.1845 - val_acc: 0.9859\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 217s 21ms/step - loss: 0.1790 - acc: 0.9853 - val_loss: 0.1322 - val_acc: 0.9982\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10227/10227 [==============================] - 219s 21ms/step - loss: 0.1644 - acc: 0.9894 - val_loss: 0.1260 - val_acc: 0.9991\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 2415s 236ms/step - loss: 0.1782 - acc: 0.9851 - val_loss: 0.1570 - val_acc: 0.9930\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 626s 61ms/step - loss: 0.1864 - acc: 0.9846 - val_loss: 0.1692 - val_acc: 0.9894\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 857s 84ms/step - loss: 0.1546 - acc: 0.9919 - val_loss: 0.1309 - val_acc: 0.9947\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 2412s 236ms/step - loss: 0.1702 - acc: 0.9849 - val_loss: 0.1571 - val_acc: 0.9894\n",
      "\n",
      "Train on 10224 samples, validate on 1136 samples\n",
      "Epoch 1/5\n",
      "10224/10224 [==============================] - 828s 81ms/step - loss: 0.1898 - acc: 0.9811 - val_loss: 0.1342 - val_acc: 0.9982\n",
      "Epoch 2/5\n",
      "10224/10224 [==============================] - 7949s 778ms/step - loss: 0.1587 - acc: 0.9886 - val_loss: 0.1993 - val_acc: 0.9789\n",
      "Epoch 3/5\n",
      "10224/10224 [==============================] - 2144s 210ms/step - loss: 0.1931 - acc: 0.9823 - val_loss: 0.1543 - val_acc: 0.9956\n",
      "Epoch 4/5\n",
      "10224/10224 [==============================] - 2144s 210ms/step - loss: 0.1643 - acc: 0.9886 - val_loss: 0.1551 - val_acc: 0.9912\n",
      "Epoch 5/5\n",
      "10224/10224 [==============================] - 2576s 252ms/step - loss: 0.1894 - acc: 0.9825 - val_loss: 0.1714 - val_acc: 0.9886\n",
      "\n",
      "EPOCH END\n",
      "\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 2700s 264ms/step - loss: 0.1697 - acc: 0.9890 - val_loss: 0.1329 - val_acc: 0.9991\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 3948s 386ms/step - loss: 0.1675 - acc: 0.9879 - val_loss: 0.1300 - val_acc: 0.9974\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 3944s 386ms/step - loss: 0.1572 - acc: 0.9893 - val_loss: 0.1263 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 3944s 386ms/step - loss: 0.1671 - acc: 0.9855 - val_loss: 0.1368 - val_acc: 0.9965\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 3945s 386ms/step - loss: 0.1747 - acc: 0.9853 - val_loss: 0.1411 - val_acc: 0.9938\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 3945s 386ms/step - loss: 0.1643 - acc: 0.9886 - val_loss: 0.1388 - val_acc: 0.9982\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 3943s 386ms/step - loss: 0.1667 - acc: 0.9869 - val_loss: 0.1639 - val_acc: 0.9894\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 2084s 204ms/step - loss: 0.1847 - acc: 0.9839 - val_loss: 0.1392 - val_acc: 0.9982\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 2143s 210ms/step - loss: 0.1482 - acc: 0.9931 - val_loss: 0.1460 - val_acc: 0.9956\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 3945s 386ms/step - loss: 0.1754 - acc: 0.9833 - val_loss: 0.1445 - val_acc: 0.9947\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 2440s 239ms/step - loss: 0.1721 - acc: 0.9888 - val_loss: 0.1311 - val_acc: 0.9982\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 3942s 385ms/step - loss: 0.1499 - acc: 0.9914 - val_loss: 0.1206 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 348s 34ms/step - loss: 0.1688 - acc: 0.9866 - val_loss: 0.1979 - val_acc: 0.9859\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 871s 85ms/step - loss: 0.2442 - acc: 0.9860 - val_loss: 0.1643 - val_acc: 0.9982\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 1414s 138ms/step - loss: 0.1633 - acc: 0.9892 - val_loss: 0.1334 - val_acc: 0.9974\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 230s 22ms/step - loss: 0.1811 - acc: 0.9851 - val_loss: 0.1348 - val_acc: 0.9956\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 239s 23ms/step - loss: 0.1551 - acc: 0.9901 - val_loss: 0.1345 - val_acc: 0.9947\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 236s 23ms/step - loss: 0.1571 - acc: 0.9877 - val_loss: 0.1333 - val_acc: 0.9965\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 230s 23ms/step - loss: 0.1588 - acc: 0.9870 - val_loss: 0.1508 - val_acc: 0.9912\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 227s 22ms/step - loss: 0.1767 - acc: 0.9852 - val_loss: 0.1360 - val_acc: 0.9974\n",
      "\n",
      "Train on 10224 samples, validate on 1136 samples\n",
      "Epoch 1/5\n",
      "10224/10224 [==============================] - 220s 22ms/step - loss: 0.1732 - acc: 0.9853 - val_loss: 0.1458 - val_acc: 0.9938\n",
      "Epoch 2/5\n",
      "10224/10224 [==============================] - 230s 23ms/step - loss: 0.1558 - acc: 0.9902 - val_loss: 0.1244 - val_acc: 0.9974\n",
      "Epoch 3/5\n",
      "10224/10224 [==============================] - 225s 22ms/step - loss: 0.1508 - acc: 0.9900 - val_loss: 0.1343 - val_acc: 0.9956\n",
      "Epoch 4/5\n",
      "10224/10224 [==============================] - 229s 22ms/step - loss: 0.1628 - acc: 0.9868 - val_loss: 0.1223 - val_acc: 0.9991\n",
      "Epoch 5/5\n",
      "10224/10224 [==============================] - 228s 22ms/step - loss: 0.1423 - acc: 0.9910 - val_loss: 0.1334 - val_acc: 0.9912\n",
      "\n",
      "EPOCH END\n",
      "\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 227s 22ms/step - loss: 0.1922 - acc: 0.9795 - val_loss: 0.1342 - val_acc: 0.9982\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 220s 22ms/step - loss: 0.1613 - acc: 0.9895 - val_loss: 0.1215 - val_acc: 0.9991\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 217s 21ms/step - loss: 0.1408 - acc: 0.9921 - val_loss: 0.1206 - val_acc: 0.9982\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1402 - acc: 0.9924 - val_loss: 0.1134 - val_acc: 0.9982\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1659 - acc: 0.9847 - val_loss: 0.1356 - val_acc: 0.9956\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 233s 23ms/step - loss: 0.1587 - acc: 0.9881 - val_loss: 0.1498 - val_acc: 0.9930\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 222s 22ms/step - loss: 0.1752 - acc: 0.9848 - val_loss: 0.1345 - val_acc: 0.9965\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 219s 21ms/step - loss: 0.1645 - acc: 0.9864 - val_loss: 0.1475 - val_acc: 0.9938\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 226s 22ms/step - loss: 0.1388 - acc: 0.9928 - val_loss: 0.1211 - val_acc: 0.9982\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 228s 22ms/step - loss: 0.1573 - acc: 0.9877 - val_loss: 0.1462 - val_acc: 0.9938\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 226s 22ms/step - loss: 0.1594 - acc: 0.9888 - val_loss: 0.1279 - val_acc: 0.9982\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1587 - acc: 0.9885 - val_loss: 0.1326 - val_acc: 0.9974\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 228s 22ms/step - loss: 0.1393 - acc: 0.9925 - val_loss: 0.1474 - val_acc: 0.9903\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 223s 22ms/step - loss: 0.1680 - acc: 0.9850 - val_loss: 0.1211 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 219s 21ms/step - loss: 0.1759 - acc: 0.9846 - val_loss: 0.1330 - val_acc: 0.9982\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 223s 22ms/step - loss: 0.1569 - acc: 0.9910 - val_loss: 0.1348 - val_acc: 0.9965\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 237s 23ms/step - loss: 0.1644 - acc: 0.9868 - val_loss: 0.1278 - val_acc: 0.9974\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 229s 22ms/step - loss: 0.1509 - acc: 0.9910 - val_loss: 0.1201 - val_acc: 0.9991\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 221s 22ms/step - loss: 0.1551 - acc: 0.9880 - val_loss: 0.1181 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 230s 22ms/step - loss: 0.1517 - acc: 0.9893 - val_loss: 0.1217 - val_acc: 0.9991\n",
      "\n",
      "Train on 10224 samples, validate on 1136 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10224/10224 [==============================] - 225s 22ms/step - loss: 0.1511 - acc: 0.9903 - val_loss: 0.1189 - val_acc: 0.9974\n",
      "Epoch 2/5\n",
      "10224/10224 [==============================] - 224s 22ms/step - loss: 0.1556 - acc: 0.9878 - val_loss: 0.1421 - val_acc: 0.9938\n",
      "Epoch 3/5\n",
      "10224/10224 [==============================] - 223s 22ms/step - loss: 0.1706 - acc: 0.9857 - val_loss: 0.1448 - val_acc: 0.9903\n",
      "Epoch 4/5\n",
      "10224/10224 [==============================] - 217s 21ms/step - loss: 0.1433 - acc: 0.9921 - val_loss: 0.1156 - val_acc: 0.9982\n",
      "Epoch 5/5\n",
      "10224/10224 [==============================] - 219s 21ms/step - loss: 0.1653 - acc: 0.9834 - val_loss: 0.1487 - val_acc: 0.9938\n",
      "\n",
      "EPOCH END\n",
      "\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 229s 22ms/step - loss: 0.1608 - acc: 0.9892 - val_loss: 0.1276 - val_acc: 0.9991\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 229s 22ms/step - loss: 0.1580 - acc: 0.9887 - val_loss: 0.1319 - val_acc: 0.9982\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 229s 22ms/step - loss: 0.1399 - acc: 0.9934 - val_loss: 0.1222 - val_acc: 0.9965\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 228s 22ms/step - loss: 0.1451 - acc: 0.9898 - val_loss: 0.1290 - val_acc: 0.9930\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 226s 22ms/step - loss: 0.1656 - acc: 0.9857 - val_loss: 0.1220 - val_acc: 0.9991\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 226s 22ms/step - loss: 0.1655 - acc: 0.9857 - val_loss: 0.1385 - val_acc: 0.9974\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 229s 22ms/step - loss: 0.1438 - acc: 0.9919 - val_loss: 0.1243 - val_acc: 0.9965\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 231s 23ms/step - loss: 0.1551 - acc: 0.9884 - val_loss: 0.2047 - val_acc: 0.9807\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 231s 23ms/step - loss: 0.1938 - acc: 0.9868 - val_loss: 0.1729 - val_acc: 0.9938\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1749 - acc: 0.9884 - val_loss: 0.1771 - val_acc: 0.9912\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 223s 22ms/step - loss: 0.1783 - acc: 0.9902 - val_loss: 0.1241 - val_acc: 0.9982\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 220s 22ms/step - loss: 0.1547 - acc: 0.9872 - val_loss: 0.1190 - val_acc: 0.9991\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 219s 21ms/step - loss: 0.1472 - acc: 0.9902 - val_loss: 0.1170 - val_acc: 0.9982\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 222s 22ms/step - loss: 0.1307 - acc: 0.9929 - val_loss: 0.1061 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 222s 22ms/step - loss: 0.1461 - acc: 0.9878 - val_loss: 0.1482 - val_acc: 0.9921\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 223s 22ms/step - loss: 0.1810 - acc: 0.9809 - val_loss: 0.1268 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1299 - acc: 0.9952 - val_loss: 0.1120 - val_acc: 0.9991\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 219s 21ms/step - loss: 0.1530 - acc: 0.9866 - val_loss: 0.1171 - val_acc: 0.9974\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 220s 21ms/step - loss: 0.1762 - acc: 0.9823 - val_loss: 0.1322 - val_acc: 0.9956\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 228s 22ms/step - loss: 0.1429 - acc: 0.9914 - val_loss: 0.1790 - val_acc: 0.9859\n",
      "\n",
      "Train on 10224 samples, validate on 1136 samples\n",
      "Epoch 1/5\n",
      "10224/10224 [==============================] - 224s 22ms/step - loss: 0.1534 - acc: 0.9889 - val_loss: 0.1147 - val_acc: 0.9991\n",
      "Epoch 2/5\n",
      "10224/10224 [==============================] - 224s 22ms/step - loss: 0.1531 - acc: 0.9871 - val_loss: 0.1251 - val_acc: 0.9991\n",
      "Epoch 3/5\n",
      "10224/10224 [==============================] - 222s 22ms/step - loss: 0.1415 - acc: 0.9912 - val_loss: 0.1147 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "10224/10224 [==============================] - 226s 22ms/step - loss: 0.1590 - acc: 0.9872 - val_loss: 0.1227 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "10224/10224 [==============================] - 227s 22ms/step - loss: 0.1405 - acc: 0.9922 - val_loss: 0.1112 - val_acc: 1.0000\n",
      "\n",
      "EPOCH END\n",
      "\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 228s 22ms/step - loss: 0.1338 - acc: 0.9923 - val_loss: 0.1120 - val_acc: 0.9982\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1519 - acc: 0.9863 - val_loss: 0.1312 - val_acc: 0.9921\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1438 - acc: 0.9898 - val_loss: 0.1304 - val_acc: 0.9903\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 223s 22ms/step - loss: 0.1486 - acc: 0.9880 - val_loss: 0.1352 - val_acc: 0.9956\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1315 - acc: 0.9932 - val_loss: 0.1183 - val_acc: 0.9965\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1716 - acc: 0.9835 - val_loss: 0.1316 - val_acc: 0.9965\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1408 - acc: 0.9905 - val_loss: 0.1255 - val_acc: 0.9982\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 223s 22ms/step - loss: 0.1364 - acc: 0.9919 - val_loss: 0.1203 - val_acc: 0.9974\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 223s 22ms/step - loss: 0.1687 - acc: 0.9853 - val_loss: 0.1321 - val_acc: 0.9974\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 222s 22ms/step - loss: 0.1336 - acc: 0.9922 - val_loss: 0.1460 - val_acc: 0.9903\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 236s 23ms/step - loss: 0.1455 - acc: 0.9889 - val_loss: 0.1187 - val_acc: 0.9982\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 232s 23ms/step - loss: 0.1513 - acc: 0.9879 - val_loss: 0.1214 - val_acc: 0.9974\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1476 - acc: 0.9898 - val_loss: 0.1156 - val_acc: 0.9982\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1473 - acc: 0.9896 - val_loss: 0.1173 - val_acc: 0.9991\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1518 - acc: 0.9876 - val_loss: 0.1220 - val_acc: 0.9947\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 228s 22ms/step - loss: 0.1441 - acc: 0.9903 - val_loss: 0.1137 - val_acc: 0.9982\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 227s 22ms/step - loss: 0.1326 - acc: 0.9920 - val_loss: 0.1056 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1420 - acc: 0.9892 - val_loss: 0.1149 - val_acc: 0.9965\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1253 - acc: 0.9934 - val_loss: 0.1189 - val_acc: 0.9930\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 228s 22ms/step - loss: 0.1559 - acc: 0.9865 - val_loss: 0.1345 - val_acc: 0.9974\n",
      "\n",
      "Train on 10224 samples, validate on 1136 samples\n",
      "Epoch 1/5\n",
      "10224/10224 [==============================] - 225s 22ms/step - loss: 0.1722 - acc: 0.9845 - val_loss: 0.1338 - val_acc: 0.9938\n",
      "Epoch 2/5\n",
      "10224/10224 [==============================] - 226s 22ms/step - loss: 0.1704 - acc: 0.9881 - val_loss: 0.1309 - val_acc: 0.9965\n",
      "Epoch 3/5\n",
      "10224/10224 [==============================] - 224s 22ms/step - loss: 0.1337 - acc: 0.9937 - val_loss: 0.1122 - val_acc: 0.9982\n",
      "Epoch 4/5\n",
      "10224/10224 [==============================] - 228s 22ms/step - loss: 0.1300 - acc: 0.9916 - val_loss: 0.1156 - val_acc: 0.9974\n",
      "Epoch 5/5\n",
      "10224/10224 [==============================] - 233s 23ms/step - loss: 0.1521 - acc: 0.9882 - val_loss: 0.1138 - val_acc: 1.0000\n",
      "\n",
      "EPOCH END\n",
      "\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10227/10227 [==============================] - 234s 23ms/step - loss: 0.1513 - acc: 0.9881 - val_loss: 0.1214 - val_acc: 0.9991\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 232s 23ms/step - loss: 0.1541 - acc: 0.9871 - val_loss: 0.1277 - val_acc: 0.9991\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1374 - acc: 0.9922 - val_loss: 0.1129 - val_acc: 0.9991\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1415 - acc: 0.9884 - val_loss: 0.1124 - val_acc: 0.9991\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 229s 22ms/step - loss: 0.1347 - acc: 0.9920 - val_loss: 0.1170 - val_acc: 0.9965\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 218s 21ms/step - loss: 0.1389 - acc: 0.9893 - val_loss: 0.1260 - val_acc: 0.9956\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 217s 21ms/step - loss: 0.1677 - acc: 0.9824 - val_loss: 0.1482 - val_acc: 0.9956\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 2152s 210ms/step - loss: 0.1496 - acc: 0.9879 - val_loss: 0.1275 - val_acc: 0.9991\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 2145s 210ms/step - loss: 0.1334 - acc: 0.9946 - val_loss: 0.1203 - val_acc: 0.9974\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 347s 34ms/step - loss: 0.1381 - acc: 0.9897 - val_loss: 0.1290 - val_acc: 0.9965\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 801s 78ms/step - loss: 0.1373 - acc: 0.9916 - val_loss: 0.1045 - val_acc: 0.9991\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 7946s 777ms/step - loss: 0.1559 - acc: 0.9843 - val_loss: 0.1298 - val_acc: 0.9982\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 272s 27ms/step - loss: 0.1479 - acc: 0.9907 - val_loss: 0.1208 - val_acc: 0.9974\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 218s 21ms/step - loss: 0.1283 - acc: 0.9943 - val_loss: 0.1036 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 219s 21ms/step - loss: 0.1316 - acc: 0.9905 - val_loss: 0.1089 - val_acc: 0.9982\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 218s 21ms/step - loss: 0.1344 - acc: 0.9909 - val_loss: 0.0985 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 220s 21ms/step - loss: 0.1369 - acc: 0.9893 - val_loss: 0.1153 - val_acc: 0.9965\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 221s 22ms/step - loss: 0.1393 - acc: 0.9906 - val_loss: 0.1151 - val_acc: 0.9965\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 221s 22ms/step - loss: 0.1573 - acc: 0.9869 - val_loss: 0.1166 - val_acc: 0.9991\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 7502s 734ms/step - loss: 0.1513 - acc: 0.9875 - val_loss: 0.1158 - val_acc: 0.9991\n",
      "\n",
      "Train on 10224 samples, validate on 1136 samples\n",
      "Epoch 1/5\n",
      "10224/10224 [==============================] - 3943s 386ms/step - loss: 0.1463 - acc: 0.9900 - val_loss: 0.1122 - val_acc: 0.9982\n",
      "Epoch 2/5\n",
      "10224/10224 [==============================] - 3943s 386ms/step - loss: 0.1352 - acc: 0.9928 - val_loss: 0.1226 - val_acc: 0.9938\n",
      "Epoch 3/5\n",
      "10224/10224 [==============================] - 3944s 386ms/step - loss: 0.1195 - acc: 0.9938 - val_loss: 0.0984 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "10224/10224 [==============================] - 3944s 386ms/step - loss: 0.1196 - acc: 0.9923 - val_loss: 0.0971 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "10224/10224 [==============================] - 3942s 386ms/step - loss: 0.1410 - acc: 0.9876 - val_loss: 0.1314 - val_acc: 0.9956\n",
      "\n",
      "EPOCH END\n",
      "\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 3951s 386ms/step - loss: 0.1741 - acc: 0.9841 - val_loss: 0.1291 - val_acc: 0.9974\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 2754s 269ms/step - loss: 0.1414 - acc: 0.9902 - val_loss: 0.1183 - val_acc: 0.9991\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 2631s 257ms/step - loss: 0.1415 - acc: 0.9897 - val_loss: 0.1215 - val_acc: 0.9947\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 2143s 210ms/step - loss: 0.1497 - acc: 0.9875 - val_loss: 0.1276 - val_acc: 0.9921\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 2160s 211ms/step - loss: 0.1241 - acc: 0.9950 - val_loss: 0.1411 - val_acc: 0.9877\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 3944s 386ms/step - loss: 0.1590 - acc: 0.9873 - val_loss: 0.1167 - val_acc: 0.9982\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 1649s 161ms/step - loss: 0.1263 - acc: 0.9932 - val_loss: 0.1175 - val_acc: 0.9947\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1401 - acc: 0.9894 - val_loss: 0.1137 - val_acc: 0.9982\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 229s 22ms/step - loss: 0.1275 - acc: 0.9921 - val_loss: 0.1139 - val_acc: 0.9965\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 232s 23ms/step - loss: 0.1204 - acc: 0.9930 - val_loss: 0.1095 - val_acc: 0.9974\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 229s 22ms/step - loss: 0.1362 - acc: 0.9880 - val_loss: 0.1151 - val_acc: 0.9965\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 227s 22ms/step - loss: 0.1604 - acc: 0.9859 - val_loss: 0.1192 - val_acc: 0.9982\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 227s 22ms/step - loss: 0.1268 - acc: 0.9947 - val_loss: 0.1029 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 219s 21ms/step - loss: 0.1242 - acc: 0.9927 - val_loss: 0.1214 - val_acc: 0.9903\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 234s 23ms/step - loss: 0.1682 - acc: 0.9839 - val_loss: 0.1228 - val_acc: 0.9982\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 230s 22ms/step - loss: 0.1389 - acc: 0.9917 - val_loss: 0.1123 - val_acc: 0.9991\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 228s 22ms/step - loss: 0.1190 - acc: 0.9947 - val_loss: 0.1000 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1360 - acc: 0.9894 - val_loss: 0.1142 - val_acc: 0.9965\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1297 - acc: 0.9931 - val_loss: 0.1040 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 223s 22ms/step - loss: 0.1382 - acc: 0.9890 - val_loss: 0.1066 - val_acc: 1.0000\n",
      "\n",
      "Train on 10224 samples, validate on 1136 samples\n",
      "Epoch 1/5\n",
      "10224/10224 [==============================] - 221s 22ms/step - loss: 0.1248 - acc: 0.9913 - val_loss: 0.1104 - val_acc: 0.9982\n",
      "Epoch 2/5\n",
      "10224/10224 [==============================] - 219s 21ms/step - loss: 0.1542 - acc: 0.9878 - val_loss: 0.1215 - val_acc: 0.9965\n",
      "Epoch 3/5\n",
      "10224/10224 [==============================] - 218s 21ms/step - loss: 0.1579 - acc: 0.9877 - val_loss: 0.1271 - val_acc: 0.9982\n",
      "Epoch 4/5\n",
      "10224/10224 [==============================] - 218s 21ms/step - loss: 0.1681 - acc: 0.9871 - val_loss: 0.1278 - val_acc: 0.9982\n",
      "Epoch 5/5\n",
      "10224/10224 [==============================] - 2151s 210ms/step - loss: 0.1310 - acc: 0.9942 - val_loss: 0.1129 - val_acc: 0.9991\n",
      "\n",
      "EPOCH END\n",
      "\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 442s 43ms/step - loss: 0.1223 - acc: 0.9935 - val_loss: 0.1029 - val_acc: 0.9991\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 227s 22ms/step - loss: 0.1768 - acc: 0.9819 - val_loss: 0.1504 - val_acc: 0.9912\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 237s 23ms/step - loss: 0.1476 - acc: 0.9926 - val_loss: 0.1112 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 235s 23ms/step - loss: 0.1278 - acc: 0.9929 - val_loss: 0.1053 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 235s 23ms/step - loss: 0.1422 - acc: 0.9889 - val_loss: 0.1101 - val_acc: 0.9991\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10227/10227 [==============================] - 232s 23ms/step - loss: 0.1206 - acc: 0.9944 - val_loss: 0.1115 - val_acc: 0.9974\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 227s 22ms/step - loss: 0.1352 - acc: 0.9891 - val_loss: 0.1291 - val_acc: 0.9912\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 226s 22ms/step - loss: 0.1299 - acc: 0.9909 - val_loss: 0.1560 - val_acc: 0.9868\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 226s 22ms/step - loss: 0.1322 - acc: 0.9919 - val_loss: 0.1032 - val_acc: 0.9991\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 237s 23ms/step - loss: 0.1208 - acc: 0.9926 - val_loss: 0.1042 - val_acc: 0.9991\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 229s 22ms/step - loss: 0.1285 - acc: 0.9895 - val_loss: 0.1001 - val_acc: 0.9991\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 228s 22ms/step - loss: 0.1665 - acc: 0.9840 - val_loss: 0.1280 - val_acc: 0.9982\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 220s 22ms/step - loss: 0.1264 - acc: 0.9956 - val_loss: 0.1023 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 222s 22ms/step - loss: 0.1252 - acc: 0.9919 - val_loss: 0.1071 - val_acc: 0.9974\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 229s 22ms/step - loss: 0.1436 - acc: 0.9885 - val_loss: 0.1119 - val_acc: 0.9991\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 229s 22ms/step - loss: 0.1257 - acc: 0.9929 - val_loss: 0.1380 - val_acc: 0.9903\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 228s 22ms/step - loss: 0.1220 - acc: 0.9933 - val_loss: 0.1116 - val_acc: 0.9965\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1569 - acc: 0.9845 - val_loss: 0.1155 - val_acc: 0.9991\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1294 - acc: 0.9926 - val_loss: 0.1085 - val_acc: 0.9982\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 231s 23ms/step - loss: 0.1410 - acc: 0.9897 - val_loss: 0.1052 - val_acc: 1.0000\n",
      "\n",
      "Train on 10224 samples, validate on 1136 samples\n",
      "Epoch 1/5\n",
      "10224/10224 [==============================] - 225s 22ms/step - loss: 0.1480 - acc: 0.9871 - val_loss: 0.1137 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "10224/10224 [==============================] - 227s 22ms/step - loss: 0.1639 - acc: 0.9871 - val_loss: 0.1379 - val_acc: 0.9947\n",
      "Epoch 3/5\n",
      "10224/10224 [==============================] - 232s 23ms/step - loss: 0.1376 - acc: 0.9941 - val_loss: 0.1099 - val_acc: 0.9991\n",
      "Epoch 4/5\n",
      "10224/10224 [==============================] - 225s 22ms/step - loss: 0.1229 - acc: 0.9940 - val_loss: 0.1135 - val_acc: 0.9974\n",
      "Epoch 5/5\n",
      "10224/10224 [==============================] - 220s 22ms/step - loss: 0.1454 - acc: 0.9897 - val_loss: 0.1226 - val_acc: 0.9956\n",
      "\n",
      "EPOCH END\n",
      "\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 230s 22ms/step - loss: 0.1438 - acc: 0.9891 - val_loss: 0.1163 - val_acc: 0.9982\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1305 - acc: 0.9937 - val_loss: 0.1060 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 222s 22ms/step - loss: 0.1185 - acc: 0.9946 - val_loss: 0.1040 - val_acc: 0.9982\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 221s 22ms/step - loss: 0.1282 - acc: 0.9913 - val_loss: 0.1106 - val_acc: 0.9965\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1593 - acc: 0.9863 - val_loss: 0.1276 - val_acc: 0.9956\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 227s 22ms/step - loss: 0.1369 - acc: 0.9915 - val_loss: 0.1184 - val_acc: 0.9974\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 235s 23ms/step - loss: 0.1495 - acc: 0.9873 - val_loss: 0.1282 - val_acc: 0.9982\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 232s 23ms/step - loss: 0.1346 - acc: 0.9928 - val_loss: 0.1140 - val_acc: 0.9982\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 226s 22ms/step - loss: 0.1198 - acc: 0.9942 - val_loss: 0.1006 - val_acc: 0.9991\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 227s 22ms/step - loss: 0.1403 - acc: 0.9865 - val_loss: 0.1213 - val_acc: 0.9982\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 229s 22ms/step - loss: 0.1690 - acc: 0.9874 - val_loss: 0.1145 - val_acc: 0.9982\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1244 - acc: 0.9942 - val_loss: 0.1030 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 226s 22ms/step - loss: 0.1245 - acc: 0.9927 - val_loss: 0.0994 - val_acc: 0.9991\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 234s 23ms/step - loss: 0.1224 - acc: 0.9909 - val_loss: 0.1008 - val_acc: 0.9991\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 235s 23ms/step - loss: 0.1274 - acc: 0.9921 - val_loss: 0.0977 - val_acc: 1.0000\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 236s 23ms/step - loss: 0.1202 - acc: 0.9924 - val_loss: 0.1020 - val_acc: 0.9982\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 229s 22ms/step - loss: 0.1346 - acc: 0.9887 - val_loss: 0.1153 - val_acc: 0.9956\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 228s 22ms/step - loss: 0.1590 - acc: 0.9860 - val_loss: 0.1184 - val_acc: 0.9991\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1320 - acc: 0.9933 - val_loss: 0.1080 - val_acc: 0.9982\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 236s 23ms/step - loss: 0.1240 - acc: 0.9927 - val_loss: 0.1020 - val_acc: 0.9991\n",
      "\n",
      "Train on 10224 samples, validate on 1136 samples\n",
      "Epoch 1/5\n",
      "10224/10224 [==============================] - 234s 23ms/step - loss: 0.1245 - acc: 0.9914 - val_loss: 0.1145 - val_acc: 0.9956\n",
      "Epoch 2/5\n",
      "10224/10224 [==============================] - 226s 22ms/step - loss: 0.1171 - acc: 0.9928 - val_loss: 0.1187 - val_acc: 0.9912\n",
      "Epoch 3/5\n",
      "10224/10224 [==============================] - 231s 23ms/step - loss: 0.1397 - acc: 0.9874 - val_loss: 0.1069 - val_acc: 0.9982\n",
      "Epoch 4/5\n",
      "10224/10224 [==============================] - 232s 23ms/step - loss: 0.1359 - acc: 0.9904 - val_loss: 0.1292 - val_acc: 0.9921\n",
      "Epoch 5/5\n",
      "10224/10224 [==============================] - 233s 23ms/step - loss: 0.1266 - acc: 0.9917 - val_loss: 0.1061 - val_acc: 0.9974\n",
      "\n",
      "EPOCH END\n",
      "\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 231s 23ms/step - loss: 0.1366 - acc: 0.9890 - val_loss: 0.1051 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 226s 22ms/step - loss: 0.1425 - acc: 0.9874 - val_loss: 0.1272 - val_acc: 0.9947\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1162 - acc: 0.9954 - val_loss: 0.0984 - val_acc: 0.9982\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 223s 22ms/step - loss: 0.1328 - acc: 0.9890 - val_loss: 0.1035 - val_acc: 0.9991\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 223s 22ms/step - loss: 0.1476 - acc: 0.9864 - val_loss: 0.1061 - val_acc: 1.0000\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 221s 22ms/step - loss: 0.1233 - acc: 0.9932 - val_loss: 0.1189 - val_acc: 0.9965\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1325 - acc: 0.9898 - val_loss: 0.1281 - val_acc: 0.9956\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 222s 22ms/step - loss: 0.1294 - acc: 0.9918 - val_loss: 0.1270 - val_acc: 0.9965\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1264 - acc: 0.9925 - val_loss: 0.1095 - val_acc: 0.9956\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 2274s 222ms/step - loss: 0.1192 - acc: 0.9938 - val_loss: 0.1133 - val_acc: 0.9938\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10227/10227 [==============================] - 2145s 210ms/step - loss: 0.1628 - acc: 0.9852 - val_loss: 0.1258 - val_acc: 0.9974\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 3947s 386ms/step - loss: 0.1261 - acc: 0.9939 - val_loss: 0.1018 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 3946s 386ms/step - loss: 0.1407 - acc: 0.9909 - val_loss: 0.1083 - val_acc: 0.9991\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 3945s 386ms/step - loss: 0.1140 - acc: 0.9950 - val_loss: 0.0952 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 346s 34ms/step - loss: 0.1233 - acc: 0.9919 - val_loss: 0.0962 - val_acc: 1.0000\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 921s 90ms/step - loss: 0.1489 - acc: 0.9890 - val_loss: 0.1033 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 7892s 772ms/step - loss: 0.1466 - acc: 0.9871 - val_loss: 0.1104 - val_acc: 0.9991\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 3962s 387ms/step - loss: 0.1429 - acc: 0.9892 - val_loss: 0.1163 - val_acc: 0.9982\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 3963s 387ms/step - loss: 0.1346 - acc: 0.9913 - val_loss: 0.1141 - val_acc: 0.9991\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 3964s 388ms/step - loss: 0.1382 - acc: 0.9913 - val_loss: 0.1039 - val_acc: 1.0000\n",
      "\n",
      "Train on 10224 samples, validate on 1136 samples\n",
      "Epoch 1/5\n",
      "10224/10224 [==============================] - 3961s 387ms/step - loss: 0.1215 - acc: 0.9942 - val_loss: 0.0949 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "10224/10224 [==============================] - 3961s 387ms/step - loss: 0.1031 - acc: 0.9961 - val_loss: 0.0915 - val_acc: 0.9991\n",
      "Epoch 3/5\n",
      "10224/10224 [==============================] - 3945s 386ms/step - loss: 0.1290 - acc: 0.9900 - val_loss: 0.0974 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "10224/10224 [==============================] - 3943s 386ms/step - loss: 0.1410 - acc: 0.9873 - val_loss: 0.1061 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "10224/10224 [==============================] - 3959s 387ms/step - loss: 0.1317 - acc: 0.9911 - val_loss: 0.1105 - val_acc: 0.9974\n",
      "\n",
      "EPOCH END\n",
      "\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 3973s 388ms/step - loss: 0.1375 - acc: 0.9892 - val_loss: 0.1118 - val_acc: 0.9991\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 3959s 387ms/step - loss: 0.1320 - acc: 0.9932 - val_loss: 0.1011 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 1788s 175ms/step - loss: 0.1086 - acc: 0.9955 - val_loss: 0.1036 - val_acc: 0.9947\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 236s 23ms/step - loss: 0.1153 - acc: 0.9922 - val_loss: 0.1624 - val_acc: 0.9859\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 231s 23ms/step - loss: 0.1423 - acc: 0.9857 - val_loss: 0.1121 - val_acc: 1.0000\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1341 - acc: 0.9919 - val_loss: 0.1101 - val_acc: 0.9982\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 230s 23ms/step - loss: 0.1284 - acc: 0.9908 - val_loss: 0.1050 - val_acc: 0.9991\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 229s 22ms/step - loss: 0.1219 - acc: 0.9931 - val_loss: 0.1373 - val_acc: 0.9903\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 222s 22ms/step - loss: 0.1404 - acc: 0.9882 - val_loss: 0.1277 - val_acc: 0.9921\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 222s 22ms/step - loss: 0.1322 - acc: 0.9917 - val_loss: 0.1122 - val_acc: 0.9974\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1598 - acc: 0.9843 - val_loss: 0.1155 - val_acc: 0.9991\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 223s 22ms/step - loss: 0.1171 - acc: 0.9967 - val_loss: 0.1178 - val_acc: 0.9965\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 234s 23ms/step - loss: 0.1375 - acc: 0.9892 - val_loss: 0.1106 - val_acc: 0.9991\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 232s 23ms/step - loss: 0.1231 - acc: 0.9936 - val_loss: 0.0995 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 223s 22ms/step - loss: 0.1371 - acc: 0.9893 - val_loss: 0.1128 - val_acc: 0.9965\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 223s 22ms/step - loss: 0.1323 - acc: 0.9912 - val_loss: 0.1193 - val_acc: 0.9965\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 222s 22ms/step - loss: 0.1322 - acc: 0.9905 - val_loss: 0.1043 - val_acc: 0.9982\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 226s 22ms/step - loss: 0.1146 - acc: 0.9948 - val_loss: 0.0990 - val_acc: 0.9991\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 230s 23ms/step - loss: 0.1413 - acc: 0.9872 - val_loss: 0.1098 - val_acc: 0.9982\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 226s 22ms/step - loss: 0.1346 - acc: 0.9910 - val_loss: 0.1066 - val_acc: 0.9991\n",
      "\n",
      "Train on 10224 samples, validate on 1136 samples\n",
      "Epoch 1/5\n",
      "10224/10224 [==============================] - 228s 22ms/step - loss: 0.1210 - acc: 0.9938 - val_loss: 0.1093 - val_acc: 0.9956\n",
      "Epoch 2/5\n",
      "10224/10224 [==============================] - 224s 22ms/step - loss: 0.1432 - acc: 0.9895 - val_loss: 0.1052 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "10224/10224 [==============================] - 224s 22ms/step - loss: 0.1156 - acc: 0.9951 - val_loss: 0.0950 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "10224/10224 [==============================] - 231s 23ms/step - loss: 0.1102 - acc: 0.9943 - val_loss: 0.0911 - val_acc: 0.9991\n",
      "Epoch 5/5\n",
      "10224/10224 [==============================] - 227s 22ms/step - loss: 0.1271 - acc: 0.9891 - val_loss: 0.1064 - val_acc: 0.9982\n",
      "\n",
      "EPOCH END\n",
      "\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 235s 23ms/step - loss: 0.1351 - acc: 0.9899 - val_loss: 0.1020 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 233s 23ms/step - loss: 0.1303 - acc: 0.9906 - val_loss: 0.1039 - val_acc: 0.9991\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 222s 22ms/step - loss: 0.1263 - acc: 0.9917 - val_loss: 0.1168 - val_acc: 0.9947\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1234 - acc: 0.9936 - val_loss: 0.1035 - val_acc: 0.9956\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1198 - acc: 0.9926 - val_loss: 0.1051 - val_acc: 0.9956\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 221s 22ms/step - loss: 0.1319 - acc: 0.9868 - val_loss: 0.1163 - val_acc: 0.9956\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 222s 22ms/step - loss: 0.1393 - acc: 0.9898 - val_loss: 0.1273 - val_acc: 0.9982\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1209 - acc: 0.9953 - val_loss: 0.1125 - val_acc: 0.9974\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1022 - acc: 0.9964 - val_loss: 0.1100 - val_acc: 0.9903\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 223s 22ms/step - loss: 0.1342 - acc: 0.9888 - val_loss: 0.1246 - val_acc: 0.9965\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 226s 22ms/step - loss: 0.1439 - acc: 0.9883 - val_loss: 0.1168 - val_acc: 0.9938\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 226s 22ms/step - loss: 0.1382 - acc: 0.9905 - val_loss: 0.1227 - val_acc: 0.9956\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 223s 22ms/step - loss: 0.1406 - acc: 0.9918 - val_loss: 0.1043 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 223s 22ms/step - loss: 0.1036 - acc: 0.9980 - val_loss: 0.0891 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 223s 22ms/step - loss: 0.1231 - acc: 0.9888 - val_loss: 0.1018 - val_acc: 0.9991\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10227/10227 [==============================] - 222s 22ms/step - loss: 0.1248 - acc: 0.9915 - val_loss: 0.1092 - val_acc: 0.9965\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 220s 22ms/step - loss: 0.1444 - acc: 0.9887 - val_loss: 0.1187 - val_acc: 0.9991\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 218s 21ms/step - loss: 0.1375 - acc: 0.9898 - val_loss: 0.1123 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 218s 21ms/step - loss: 0.1353 - acc: 0.9921 - val_loss: 0.1033 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 218s 21ms/step - loss: 0.1175 - acc: 0.9945 - val_loss: 0.0966 - val_acc: 1.0000\n",
      "\n",
      "Train on 10224 samples, validate on 1136 samples\n",
      "Epoch 1/5\n",
      "10224/10224 [==============================] - 221s 22ms/step - loss: 0.1229 - acc: 0.9919 - val_loss: 0.0939 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "10224/10224 [==============================] - 222s 22ms/step - loss: 0.1163 - acc: 0.9931 - val_loss: 0.0939 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "10224/10224 [==============================] - 234s 23ms/step - loss: 0.1304 - acc: 0.9903 - val_loss: 0.1013 - val_acc: 0.9991\n",
      "Epoch 4/5\n",
      "10224/10224 [==============================] - 232s 23ms/step - loss: 0.1283 - acc: 0.9909 - val_loss: 0.1195 - val_acc: 0.9947\n",
      "Epoch 5/5\n",
      "10224/10224 [==============================] - 234s 23ms/step - loss: 0.1471 - acc: 0.9885 - val_loss: 0.1074 - val_acc: 1.0000\n",
      "\n",
      "EPOCH END\n",
      "\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 232s 23ms/step - loss: 0.1243 - acc: 0.9936 - val_loss: 0.0991 - val_acc: 0.9991\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 228s 22ms/step - loss: 0.1105 - acc: 0.9939 - val_loss: 0.0955 - val_acc: 0.9974\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1198 - acc: 0.9918 - val_loss: 0.1016 - val_acc: 0.9956\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1342 - acc: 0.9886 - val_loss: 0.1006 - val_acc: 0.9991\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 223s 22ms/step - loss: 0.1358 - acc: 0.9903 - val_loss: 0.1129 - val_acc: 0.9956\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1376 - acc: 0.9900 - val_loss: 0.1059 - val_acc: 0.9991\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 226s 22ms/step - loss: 0.1032 - acc: 0.9973 - val_loss: 0.0921 - val_acc: 0.9991\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 223s 22ms/step - loss: 0.1094 - acc: 0.9929 - val_loss: 0.1083 - val_acc: 0.9938\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 221s 22ms/step - loss: 0.1390 - acc: 0.9861 - val_loss: 0.1200 - val_acc: 0.9965\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 221s 22ms/step - loss: 0.1390 - acc: 0.9911 - val_loss: 0.1135 - val_acc: 0.9965\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1461 - acc: 0.9910 - val_loss: 0.1112 - val_acc: 0.9965\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 222s 22ms/step - loss: 0.1161 - acc: 0.9961 - val_loss: 0.1009 - val_acc: 0.9974\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 223s 22ms/step - loss: 0.1312 - acc: 0.9894 - val_loss: 0.1003 - val_acc: 0.9982\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 223s 22ms/step - loss: 0.1156 - acc: 0.9936 - val_loss: 0.1023 - val_acc: 0.9974\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 229s 22ms/step - loss: 0.1215 - acc: 0.9917 - val_loss: 0.0991 - val_acc: 0.9982\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1029 - acc: 0.9961 - val_loss: 0.0903 - val_acc: 0.9991\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 222s 22ms/step - loss: 0.1256 - acc: 0.9893 - val_loss: 0.1080 - val_acc: 0.9965\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1448 - acc: 0.9871 - val_loss: 0.1078 - val_acc: 0.9982\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1240 - acc: 0.9932 - val_loss: 0.1005 - val_acc: 0.9982\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1090 - acc: 0.9949 - val_loss: 0.0926 - val_acc: 0.9991\n",
      "\n",
      "Train on 10224 samples, validate on 1136 samples\n",
      "Epoch 1/5\n",
      "10224/10224 [==============================] - 226s 22ms/step - loss: 0.1425 - acc: 0.9866 - val_loss: 0.1147 - val_acc: 0.9974\n",
      "Epoch 2/5\n",
      "10224/10224 [==============================] - 226s 22ms/step - loss: 0.1364 - acc: 0.9898 - val_loss: 0.1039 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "10224/10224 [==============================] - 227s 22ms/step - loss: 0.1185 - acc: 0.9941 - val_loss: 0.1000 - val_acc: 0.9991\n",
      "Epoch 4/5\n",
      "10224/10224 [==============================] - 221s 22ms/step - loss: 0.1261 - acc: 0.9907 - val_loss: 0.1273 - val_acc: 0.9930\n",
      "Epoch 5/5\n",
      "10224/10224 [==============================] - 223s 22ms/step - loss: 0.1183 - acc: 0.9931 - val_loss: 0.0974 - val_acc: 0.9991\n",
      "\n",
      "EPOCH END\n",
      "\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 233s 23ms/step - loss: 0.1486 - acc: 0.9850 - val_loss: 0.1342 - val_acc: 0.9930\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 231s 23ms/step - loss: 0.1239 - acc: 0.9945 - val_loss: 0.0951 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 229s 22ms/step - loss: 0.0966 - acc: 0.9983 - val_loss: 0.0887 - val_acc: 0.9974\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 230s 22ms/step - loss: 0.1059 - acc: 0.9932 - val_loss: 0.0886 - val_acc: 0.9982\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 227s 22ms/step - loss: 0.1304 - acc: 0.9890 - val_loss: 0.0981 - val_acc: 0.9991\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 235s 23ms/step - loss: 0.1128 - acc: 0.9930 - val_loss: 0.1278 - val_acc: 0.9894\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 231s 23ms/step - loss: 0.1254 - acc: 0.9913 - val_loss: 0.1106 - val_acc: 0.9965\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 236s 23ms/step - loss: 0.1268 - acc: 0.9903 - val_loss: 0.1253 - val_acc: 0.9930\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 235s 23ms/step - loss: 0.1250 - acc: 0.9914 - val_loss: 0.1687 - val_acc: 0.9833\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 223s 22ms/step - loss: 0.1378 - acc: 0.9894 - val_loss: 0.1142 - val_acc: 0.9965\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 228s 22ms/step - loss: 0.1247 - acc: 0.9909 - val_loss: 0.1580 - val_acc: 0.9807\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 226s 22ms/step - loss: 0.1256 - acc: 0.9920 - val_loss: 0.1019 - val_acc: 0.9991\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 223s 22ms/step - loss: 0.1218 - acc: 0.9940 - val_loss: 0.1046 - val_acc: 0.9974\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 223s 22ms/step - loss: 0.1177 - acc: 0.9928 - val_loss: 0.0948 - val_acc: 0.9991\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 222s 22ms/step - loss: 0.1159 - acc: 0.9943 - val_loss: 0.0930 - val_acc: 0.9991\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 230s 22ms/step - loss: 0.1323 - acc: 0.9888 - val_loss: 0.1109 - val_acc: 0.9956\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1364 - acc: 0.9910 - val_loss: 0.1303 - val_acc: 0.9930\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 222s 22ms/step - loss: 0.1069 - acc: 0.9955 - val_loss: 0.0998 - val_acc: 0.9982\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 220s 22ms/step - loss: 0.1281 - acc: 0.9909 - val_loss: 0.0958 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 223s 22ms/step - loss: 0.1288 - acc: 0.9914 - val_loss: 0.1095 - val_acc: 1.0000\n",
      "\n",
      "Train on 10224 samples, validate on 1136 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10224/10224 [==============================] - 222s 22ms/step - loss: 0.1592 - acc: 0.9866 - val_loss: 0.1175 - val_acc: 0.9991\n",
      "Epoch 2/5\n",
      "10224/10224 [==============================] - 223s 22ms/step - loss: 0.1339 - acc: 0.9915 - val_loss: 0.1025 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "10224/10224 [==============================] - 222s 22ms/step - loss: 0.1196 - acc: 0.9936 - val_loss: 0.0943 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "10224/10224 [==============================] - 223s 22ms/step - loss: 0.1179 - acc: 0.9923 - val_loss: 0.0950 - val_acc: 0.9991\n",
      "Epoch 5/5\n",
      "10224/10224 [==============================] - 230s 22ms/step - loss: 0.0960 - acc: 0.9970 - val_loss: 0.0849 - val_acc: 1.0000\n",
      "\n",
      "EPOCH END\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import skimage\n",
    "from skimage.color import rgba2rgb\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "def get_training_sample(start=0, length=1000):\n",
    "    train_images = []\n",
    "    end = min(start+length, len(train_data))\n",
    "    for name in train_data[start:end]:\n",
    "        image = skimage.data.imread(images_dataset[name][0])\n",
    "        if image.shape[0] > image.shape[1]:\n",
    "            image = skimage.transform.rotate(image, 90, resize=True)\n",
    "        image = skimage.transform.resize(image, image_size, mode='constant')\n",
    "        if image.shape != image_shape:\n",
    "            image = rgba2rgb(image)\n",
    "        train_images.append(image)\n",
    "    \n",
    "    return np.asarray(train_images), train_y[start:end]\n",
    "\n",
    "def train_network():\n",
    "    segments = 5\n",
    "    length = int(len(train_data) / segments + 1)\n",
    "    for j in range(20):\n",
    "        start = 0\n",
    "        for i in range(segments):\n",
    "            train_images, train_targets = get_training_sample(start, length)\n",
    "            result = tf_model.fit(train_images, train_targets, epochs = 5, \n",
    "                                  validation_split=0.1, verbose=1)\n",
    "            start += length\n",
    "            print()\n",
    "        tf_model.save(PATH + 'chk/tf_model' + str(j) + '.h5')\n",
    "        print(\"EPOCH END\\n\\n\")\n",
    "\n",
    "train_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calidad de la red:\n",
      "Loss: 0.08739986653659008\n",
      "Acc: 0.9996480112636396\n"
     ]
    }
   ],
   "source": [
    "def get_test_data():\n",
    "    test_images = []\n",
    "    for name in test_data:\n",
    "        image = skimage.data.imread(images_dataset[name][0])\n",
    "        if image.shape[0] > image.shape[1]:\n",
    "            image = skimage.transform.rotate(image, 90, resize=True)\n",
    "        image = skimage.transform.resize(image, image_size, mode='constant')\n",
    "        test_images.append(image)\n",
    "    test_images = np.asarray(test_images)\n",
    "    return test_images\n",
    "    \n",
    "def evalute_accuracy():\n",
    "    test_images = get_test_data()\n",
    "    \n",
    "    loss, acc = tf_model.evaluate(test_images, test_y, verbose=0)\n",
    "    return loss, acc\n",
    "\n",
    "loss, acc = evalute_accuracy()\n",
    "print(\"Calidad de la red:\\nLoss: {}\\nAcc: {}\".format(loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calidad de la red (Max Pool):<br>\n",
    "Loss: 0.09856902201466894<br>\n",
    "Acc: 0.9994368180218233<br><br>\n",
    "\n",
    "Calidad de la red (Average Pool):<br>\n",
    "Loss: 0.08739986653659008<br>\n",
    "Acc: 0.9996480112636396<br><br>\n",
    "\n",
    "Calidad de la red (Average Pool):<br>\n",
    "Loss: 0.09177039901016884<br>\n",
    "Acc: 0.9992256247800071<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
