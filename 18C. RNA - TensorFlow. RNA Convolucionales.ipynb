{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/header_18.png)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes neuronales convolucionales\n",
    "\n",
    "Las redes convolucionales son una combinación de la red neuronal *feedforward*, con capas de pre-procesamiento de los datos de entrada. Estas capas de entrada mejoran y simplifican los datos que serán presentados a la red neuronal *feedforward*. Esta arquitectura es particularmente útil para reconocimiento de patrones en imágenes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La inspiración biológica\n",
    "\n",
    "La red neuronal *feedforward* es un buen clasificador y método de aprendizaje automático basado en una metafora fisiológica. Sin embargo, la arquitectura que presenta este modelo es muy simplista: La red neuronal se conforma mediante un conjunto, típicamente grande, de neuronas básicamente idénticas, organizadas en capas secuenciales que tampoco se diferencian entre sí. La forma de resolver un problema es, básicamente, la misma sin importar el tipo de problema de que se trate. Las redes neuronales convolucionales se derivan de una metáfora más amplia de la estructura cerebral, específicamente aquella especializada en la comprensión de la información visual.\n",
    "\n",
    "### Estructura cerebral en el humano\n",
    "\n",
    "Actualmente, no existe una teoría unificada acerca de la organización y funcionamiento del cerebro que sea universalmente aceptada. Sin embargo, existe un consenso amplio en torno a la idea de que las diferentes regiones en el cerebro se especializan funcionalmente. Anatómicamente, el cerebro se describe en dos hemisferios (derecho e izquierdo), divididos en 6 lóbulos, cuatro de ellos fácilmente identificables (el lóbulo frontal, el lóbulo temporal, el lóbulo parietal y el lóbulo occipital) y otros dos (el lóbulo límbico y el lóbulo insular) localizados más al interior del cerebro.<br><br>\n",
    "\n",
    "<img src=\"images/brain_2.png\" width=50%><br>\n",
    "\n",
    "Estas regiones anatómicas han sido asociadas a funciones específicas, una interpretación sujeta a controversia, pero que sigue siendo ampliamente aceptada:\n",
    "\n",
    "* Hemisferio derecho: Se considera responsable de controlar el movimiento de lado izquierdo del cuerpo, así como asiento de la creatividad, la intuición y el pensamiento holístico.\n",
    "\n",
    "* Hemisferio izquierdo: Se considera responsable de controlar el movimiento de lado derecho del cuerpo, así como asiento del pensamiento lógico-matemático y el lenguaje.\n",
    "\n",
    "* Lóbulo frontal: Se encuentra localizado al frente del cerbro y delimitado posteriormente por el surco central y en la parte inferior por el surco lateral. Se considera responsable de controlar el movimiento voluntario de partes específicas del cuerpo, así como de funciones mentales superiores como la planificación y la toma de decisiones. También se considera responsable de la articulación fluida del habla.\n",
    "\n",
    "* Lóbulo parietal: Se localiza en la parte central superior del cerebro. Se asume responsable de procesar información sensorial relacionada con el gusto, la temperatura, el tacto y la propiocepción. También se le responsabiliza (de acuerdo a la hipótesis de las dos corrientes de la visión) de la interpretación visual del movimiento (trayectoria dorsal). \n",
    "\n",
    "* Lóbulo occipital: Localizado en la parte posterior del cerebro, contiene la mayor parte de la corteza visual en los mamíferos y se le considera responsable del procesamiento de la información visual proveniente de los ojos.\n",
    "\n",
    "* Lóbulo temporal: Se encuentra localizado por abajo del surco lateral. Se le considera responsable de interpretar la información proveniente de los sentidos, particularmente del oído y la vista, comprensión del lenguaje y la asociación de emociones.\n",
    "\n",
    "* Lóbulo límbico: Se encuentra localizado sobre las caras interiores de cada hemisferio cerebral, en la parte más interna y abarca porciones de los lóbulos frontal, parietal y temporal. Se le responsabiliza del olfato y de influir fuertemente en las emociones.\n",
    "\n",
    "* Lóbulo insular: Consiste de la corteza cerebral en la parte profunda del surco lateral. Se le relaciona con la conciencia y se asume juega un papel importante en las emociones y el control de la homeostasis del cuerpo.\n",
    "\n",
    "\n",
    "### Comprensión de la información visual\n",
    "\n",
    "Una de las teorías de especialización local más influyentes y más cuestionadas en los últimos 30 años es la relacionada con la hipótesis de las dos trayectorías. De acuerdo con esta hipótesis, la información visual (al igual que la auditiva) viaja por dos trayectorias independientes:\n",
    "\n",
    "* La corriente ventral, que lleva la información visual desde el lóbulo occipital al lóbulo tenporal con el fin de identificar patrones (formas), y \n",
    "* La corriente dorsal, que lleva la información visual del lóbulo occipital a lóbulo parietal y cuyo objetivo es detectar movimiento en una escena. \n",
    "\n",
    "Aunque aún se cuestiona la supuesta independencia de las dos corrientes, la hipótesis se ha reivindicado en los últimos años y ha sido ampliamente aceptada, al menos en su idea general. De acuerdo a la hipótesis de las dos corrientes, el reconocimiento visual de patrones (forma y color) se puede esquematizar de la siguiente manera:\n",
    "\n",
    "1. La interpretación de información visual inicia con el sensado de la escena a través de células altamente especializadas en la **retina**: los conos y los bastones (importantes para la detección de movimiento). La percepción de la luz por los humanos se basa, de acuerdo con la teoría (tricromática) de Young–Helmholtz, en la respuesta de tres tipos de conos a diferentes rangos de longitud de onda:<br>&#9633; Los *conos S* que son particularmente sensibles a la luz en el rango 400–500 nm, con un pico de recepción en torno al rango 420–440 nm (en torno al color azul).<br>&#9633; Los *conos M*, sensibles fuertemente a la luz en el rango 450–630 nm, con un pico de recepción en torno al rango 534–555 nm (en la región de colores verde amarillento).<br>&#9633; Los *conos L* que son fuertemente sensibles a la luz en el rango 500–700 nm, con un pico de recepción en torno al rango 564–580 (la región del naranja).\n",
    "    <img src=\"images/colors_vision.png\" width=75%><br>\n",
    "La información proveniente de cada tipo de cono puede considerarse como un 'canal' de información. El estímulo visual es recibido por grupos de conos que muestrean porciones del campo visual de forma redundante. La luz recibida en los conos es convertida en una señal eléctrica y transmitida a las *células M* (neuronas) del sistema nervioso.<br><br>\n",
    "\n",
    "2. Los axones de las células-M se proyectan hacia el núcleo geniculado lateral (**NGL**), a través del nervio óptico (que se convierte en cintilla óptica a partir del quiasma óptico). El NGL es un núcleo en el tálamo que funciona como punto de relevo de la información visual. Consta de 6 capas, 4 de ellas formadas por *células-P* dedicadas a recibir las señales de las celulas-M (provenientes de los conos en la retina). La información recibida en el NGL es descompuesta en estas capas y reenviada a traves de la cintilla óptica a las corteza visual, específicamente a la corteza visual primaria y a la corteza visual secundaria (además, esta información es recombinada con la información proveniente de los bastones y enviada a *V3* para la detección de movimiento).<br><br>\n",
    "\n",
    "3. La mayor parte de los axones provenientes del NGL, a través de la cintilla óptica, terminan en la  corteza visual primaria **V1**. En la primera fase, entre los 40 y 100 ms después de recibir las señales prevenientes del NGL, V1 realiza actividades de detección de bordes. En este aspecto, la operación de las neuronas en V1 suele ser comparada con la de un filtro de Gabor (con el que es posible detectar bordes haciendo un análisis sobre múltiples frecuencias y/o direcciones). V1 mantiene una fuerte interconexión con V2 (y, al parecer con otras áres como V3, V4 e IT), enviando señales preliminares y recibiendo retroalimentación. Después de los primeros 100ms, V1 utiliza la retroalimentación de la corteza visual secundaria (y, posiblemente, de otras áreas) para realizar un procesamiento más detallado acerca de la organización de la escena. En V1 se inicia la corriente ventral.<br><br>\n",
    "\n",
    "4. **V2** es la corteza visual secundaria, rodea a la corteza visual primaria y conforma, con V3, el *área de asociación visual*. En gran medida repite las mismas funciones que V1, pero detectando características más complejas, como los contornos y la separación entre fondo y figura (segmentación de la imagen). También juega un papel importante en el enfoque de rasgos importantes en la imagen (a través de los núcleos pulvinares, responsables de la atención y los movimientos sacádicos), sin embargo, esta actividad no se asocia a la corriente ventral (aunque recientemente se ha insistido en la fuerte interrelación entre las dos corrientes).<br><br>\n",
    "\n",
    "5. **V4** se encuentra localizada por abajo de V2 y recibe la información proveniente de v2. Además, también llegan a V4 señales provenientes directamente de V1 y del NGL y señales provenientes de los núcleos pulvinares. V4 es capaz de reconocer características de complejidad media, como las figuras geométricas simples. <br><br>\n",
    "\n",
    "6. Desde V4, la información visual sigue su viaje a través de la corriente ventral hacia el **área inferotemporal** (IT), donde se lleva a cabo la identificación de patrones complejos, como el reconocimiento de rostros. Las neuronas en esta región preveen también del acceso a la memoria, con el fin de registrar y recuperar patrones para el reconocimiento de las formas complejas.<br><br>\n",
    "\n",
    "7. Finalmente, la clasificación de objetos (el juicio sobre el objeto) y la generación de la respuesta motora es realizada mediante circuitos neuronales en la **corteza prefrontal** (CPF).\n",
    "\n",
    "<img src=\"images/brain_1.png\" width=75%><br>\n",
    "\n",
    "<img src=\"images/brain_1b.png\" width=75%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolución sobre imágenes\n",
    "\n",
    "La forma en que se procesa la información visual en cada etapa del recorrido óptico, y particularmente en la retina, puede representarse mediante una operación de **convolución** sobre una imagen.\n",
    "\n",
    "La convolución es una operación matemática sobre dos funciones para producir una tercera función:\n",
    "\n",
    "$$h = f_1 \\ast f_2$$\n",
    "\n",
    "Aquí, $h$ describe la forma en que $f_1$ es modificada por $f_2$. \n",
    "\n",
    "En el caso de procesamiento de datos/imágenes, $f_1$ representa los datos de entrada y suele ser denominada *mapa de características*, mientras que $f_2$ es el *núcleo de convolución* y suele considerarse como un *filtro*. El kernel filtra el mapa de características para obtener un nuevo mapa con información de algún tipo (el conjunto de bordes, por ejemplo), o una versión mejorada del mapa original (después de la eliminación de ruido, por ejemplo). El resultado de la convolución es un *mapa transformado de características*. \n",
    "\n",
    "<img src=\"images/convol_1.png\" width=90%><br>\n",
    "\n",
    "En una imagen, la convolución consiste en recorrer (barrer) la imagen original completa con un *filtro* de dimensión $n\\times n$, con $n$ típicamente impar (usualmente 3, 5 o 7), calculando, a cada paso, el producto interior (de Frobenius) de la matriz que representa el núcleo de convolución por la matriz que se forma al seleccionar los pixeles bajo el filtro (*matriz subyacente*), como se muestra en la siguiente imagen:\n",
    "\n",
    "<img src=\"images/convol_2.png\" width=90%><br>\n",
    "\n",
    "El resultado de cada producto interior es asociado al pixel correspondiente al centro del filtro y registrado en el mapa transformado. Los pixeles en el borde de la imagen no coinciden con el centro del filtro en ningún paso del barrido, por lo que el mapa resultante, al eliminar los pixeles con valor indefinido, resultaría de tamaño menor al mapa original ($n-1$ pixeles menos). Usualmente se prefiere agregar valores arbitrarios para completar la matriz subyacente, de manera que se pueda calcular valores para los pixeles en el borde de la imagen y mantener el tamaño de la imagen original. Una opción frecuente es rellenar los valores 'faltantes' con cero:\n",
    "\n",
    "<img src=\"images/convol_3.png\" width=45%><br>\n",
    "\n",
    "El proceso humano de percepción tricromática de la luz hace natural describir los colores como una combinación de tres colores 'primarios', siendo uno de los modelos más conocidos el llamado RGB ('Red', 'Green', 'Blue'). Una imagen en color, entonces, suele representarse como la superposición de tres 'imágenes parciales', una para cada canal o color primario.\n",
    "\n",
    "<img src=\"images/Lenna_RGB.png\" width=75%><br>\n",
    "\n",
    "La convolución sobre una imagen en color se realiza utilizando un *kernel* con la misma profundidad; en el caso RGB se utiliza un *kernel* de 3 capas. Cada capa en el *kernel* recorre la capa correspondiente en el mapa original y produce un resultado parcial. Los resultados parciales se integran para generar el mapa de características transformado, de un sólo canal. \n",
    "\n",
    "<img src=\"images/convol_4.png\" width=50%><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitectura de la red neuronal convolucional\n",
    "\n",
    "Los procesos de percepción visual y reconocimiento de patrones por parte del cerebro, incluyendo los aspectos planteados aquí, es un tema aún en discusión por los especialistas en neurociencias. Este conocimiento, sin embargo, se tomó como base para desarrollar una arquitectura de red neuronal particularmente exitosa para el reconocimiento de  patrones en imágenes y que son responsables, en buena medida del nuevo impulso en el área de redes neuronales artificiales: las redes neuronales convolucionales.\n",
    "\n",
    "Una red neuronal presenta la siguiente arquitectura: \n",
    "\n",
    "<img src=\"images/neuron11.png\" width=80%><br>\n",
    "\n",
    "Aquí se distinguen dos secciones generales. La primera sección está formada, principalmente, por una o más capas de convolución y su objetivo es resaltar los rasgos en la imagen y reducir la dimensión de los datos. Esta sección corresponde a una una etapa de extracción automátizada y opaca de características. La segunda sección, es un perceptrón multicapa, utilizado para hacer la clasificación.\n",
    "\n",
    "Las diferentes funciones en la sección de extracción de características se modelan a través de capas de neuronas especializadas. El tipo de capa más importante en esta sección es la *capa de convolución*, sin embargo, resulta útil modelar otras tareas mediante capas (ese es el enfoque en TensorFlow, por ejemplo), incluso la función de activación suele describirse mediante una capa de neuronas especializada. A continuación se describen los principales tipos de capas utilizadas en una red convolucional.\n",
    "\n",
    "\n",
    "### Capa de convolución \n",
    "\n",
    "La *capa de convolución* es la componente principal de la sección de extracción de características en una red convolucional. Cada capa de convolución suele estar conformada por una serie de filtros destinados a identificar diferentes rasgos de un mismo mapa de características. Cada filtro $f_i$ genera un canal $c_i$ en el mapa de salida.\n",
    "\n",
    "<img src=\"images/neuron15.png\" width=50%><br>\n",
    "\n",
    "En total, si el mapa de entrada tiene $n_c$ canales y la red convolucional emplea $n_f$ filtros, entonces el mapa de salida contendrá $n_c\\times n_f$ canales.\n",
    "\n",
    "Desde un punto de vista neuronal, conviene visualizar la capa convolucional si estuviera formada por un conjunto de neuronas organizadas en grupos, con cada grupo de neuronas $gn_i$ asociado a un filtro $f_i$. En este modelo, cada neurona en el grupo $gn_i$ utiliza como pesos sinápticos la matriz de valores del filtro $f_i$. Por lo tanto, sólo un subgrupo de señales de  entrada son capaces de estimular a la neurona, aquellas señales del mapa de entrada que coinciden con la cobertura del filtro, centrado en la posición de la neurona, como se muestra en la imagen.\n",
    "\n",
    "<img src=\"images/neuron14.png\" width=40%><br>\n",
    "\n",
    "Si $\\mathbf{x}$ es la matriz de señales de entrada bajo el filtro y $\\mathbf{w}$ la matriz de valores del filtro, entonces, la activación en la neurona está dada por \n",
    "\n",
    "$$a = \\mathbf{x}\\cdot \\mathbf{w}$$\n",
    "\n",
    "La activación alimenta la función de activación y la salida de esta función corresponde al valor en el mapa de salida en la misma posición que la neurona. Cada neurona en el grupo de neuronas genera una señal en el mapa de salida. Si la función de activación es la función identidad, entonces el mapa de salida coincide con la forma usual de la convolución sobre una imagen descrita anteriormente. \n",
    "\n",
    "La capa convolucional puede, entonces, esquematizarse como un conjunto de grupos especializados de neuronas, cada grupo asociado a un filtro (que le da la espeialización a las neuronas), como se muestra en la siguiente figura:\n",
    "\n",
    "<img src=\"images/neuron16.png\" width=70%><br>\n",
    "\n",
    "En el modelo neuronal, cada grupo de neuronas $gn_i$ genera un canal de información $c_i$. Todos juntos, estos canales componen el mapa de salida.\n",
    "\n",
    "La función de activación más utilizada en las capas de convolución (y en redes neuronales profundas, en general) es el *rectificador*, definido como:\n",
    "\n",
    "$$g(x)=\\max(0,x)$$\n",
    "\n",
    "Una neurona que utiliza esta función de activación se conoce como *relu* (del término en inglés para *unidad lineal rectificada*).\n",
    "\n",
    "Hay que observar que las neuronas en una capa convolucional no se encuentran interconectadas; cada neurona recibe sus datos de entrada y proyecta su salida al mapa transformado. De esta mnera, la organización en el conjunto de neuronas es más parecido a la que se observa en el nervio óptico que a la mostrada en un circuito neuronal.\n",
    "\n",
    "#### Paso de barrido\n",
    "\n",
    "Si las neuronas están separadas entre sí a más de una unidad, es decir, si se define un paso de barrido mayor a uno, entonces la cobertura sobre el mapa original no será continua. \n",
    "\n",
    "<img src=\"images/convol_5.png\" width=75%><br>\n",
    "\n",
    "El resultado, en este caso, será un mapa de salida menor al mapa de entrada. El tamaño del mapa resultante de la convolución, para un mapa original de tamaño $w\\times h$, con un *kernel* de $n\\times n$, con $r$ pixeles de relleno y un paso de barrido $p$ es: \n",
    "\n",
    "$$\n",
    "[(w+2r-n)/p+1] \\times [(h+2r-n)/p+1] \n",
    "$$\n",
    "\n",
    "Considérese, por ejemplo, un mapa inicial de $100\\times 100$ pixeles que es convolucionado con un *kernel* de $3\\times 3$ pixeles con un paso $p=2$ y usando un relleno $r=1$. El tamaño del mapa transformado sería:\n",
    "\n",
    "$$\n",
    "[(100+2-3)/2 + 1] \\times [(100+2-3)/2 + 1] =  50\\times 50\n",
    "$$\n",
    "\n",
    "Esto equivale a un submuestreo de la salida, tomando, en el caso del ejemplo anterior, sólo la mitad de los datos. Para valores de paso mayores a $(n+1)/2$ habrá también un submuestreo de la entrada, es decir, habrá señales de entrada que no influyen en la salida.\n",
    "\n",
    "<img src=\"images/convol_6.png\" width=20%><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capa de normalización por lotes\n",
    "\n",
    "La capa de normalización por lotes es un artefacto utilizado para modificar la salida de la capa de convolución. Se utiliza para aumentar la velocidad de entrenamiento de las redes neuronales profundas, siendo capaz de reducir a menos de una décima parte los pasos de entrenamiento. La razón por la que esta técnica funciona es confusa, sin embargo, esta capa es una componente común en las redes neuronales convolucionales. \n",
    "\n",
    "La operación que implementa una capa de normalización por lotes es simple: \n",
    "\n",
    "1. Normalizar la salida $z_i^{(l)}$ de cada neurona, utilizando la media y la varianza de los datos en el lote de entrenamiento, generando la nueva señal $z_i^{(l)}\\vert_{norm}$.\n",
    "\n",
    "2. Generar la salida normalizada $\\tilde{z}_i^{(l)} = \\gamma z_i^{(l)}\\vert_{norm} + \\beta$, siendo $\\gamma$ y $\\beta$ parámetros adicionales cuytos valores son determinados por entrenamiento.\n",
    "\n",
    "De esta manera, la capa de normalización mantiene las salidas de todas las neuronas a las que se aplica en un rango \"normal\" a los datos de entrenamiento.\n",
    "\n",
    "Entre los temas aún bajo discusión se encuentra dónde colocar la capa de normalización por lotes, con cierto consenso en que conviene colocarla antes de la función de activación.\n",
    "\n",
    "<img src=\"images/neuron17.png\" width=70%><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capa de agrupación (*pooling*)\n",
    "\n",
    "La capa de agrupación ofrece otra opción de submuestreo (adicionalmente al uso de pasos de barrido mayor a uno discutido anteriormente) sobre la salida del proceso de convolución. El principal objetivo, en este caso, es reducir la dimensión de un mapa de características mediante algún proceso de selección o de agregación.\n",
    "\n",
    "Desde las perspectiva de procesamiento de imágenes/señales, esta forma de submuestreo se obtiene utilizando un filtro que recorre la imagen, típicamente sin traslape, y regresa un valor representativo de la ventana.\n",
    "\n",
    "<img src=\"images/convol_7.png\" width=50%><br>\n",
    "\n",
    "La opción más usual es utilizar un filtro de submuestreo de $2\\times 2$ pixeles con un paso de barrido de 2. El resultado es una reducción del mapa de entrada a una cuarta parte de su tamaño. El filtro se aplica a cada canal en el mapa de características, generando el mismo número de canales, pero submuestreados.\n",
    "\n",
    "El valor de salida del filtro puede ser cualquier estadístico, como puede ser el valor máximo, el mínimo, la moda, la mediana o la media. Las opciones disponibles en TensorFlow las opciones disponibles son el máximo y la media; en Deeplearnig4j se encuentra además la $p$-norma. El filtro de valor máximo realiza, además de la reducción de tamaño (asumiendo un paso de barrido adecuado), un filtrado no lineal de ruido que enfatiza los principales rasgos en el mapa; por su parte, el filtrado de media realiza un suavizado del mapa y es menos sensible a valores atípicos. Como en el caso de filtrado estandar sobre imágenes, el filtro más adecuado dependerá de cada caso específico. No obstante, el filtro más empleado en redes convolucionales es el filtro de valor máximo.\n",
    "\n",
    "Desde la perspectiva neuronal, la capa de muestreo se visualiza como un conjunto de neuronas que reciben como entrada un segmento de datos contiguos del mapa original y lo transforman en su salida. En el caso del filtro de media, los pesos para todas las entradas tienen el mismo valor ($\\frac{1}{n\\times m}$ con $n\\times m$ el tamaño del filtro). En el caso del filtro de valor máximo la metáfora es menos clara; los pesos para cada neurona son:\n",
    "\n",
    "$$\n",
    "w_{ij} = \\left\\{\n",
    "      \\begin{array}{ll}\n",
    "         1 & \\textrm{si } x_{ij} \\textrm{ es el valor máximo en } \\mathbf{x} \\\\\n",
    "         0 & \\textrm{cualquier otro caso}\n",
    "      \\end{array} \\right.\n",
    "$$\n",
    "\n",
    "Siendo $ij$ el índice sobre la matriz de señales de entrada $\\mathbf{x}$.\n",
    "\n",
    "Nuevamente, todas las neuronas en la capa de submuestro utilizan el mismo conjunto de pesos y lo aplican por separado a cada canal de entrada para producir, en conjunto, un mapa de salida con el mismo número de canales.\n",
    "\n",
    "Alternativamente, en lugar de utilizar una capa de agregación puede utilizarse un paso de barrido del *kernel* sobre la capa de convolución mayor a 1. Usualmente se emplea una u otra opción: paso de barrido mayor a uno o capa de agrupación. Hay que observar que, incrementar el paso de barrido para reducir el tamaño del mapa transformado es más económico que utilizar una capa de submuestreo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capa de deserción (*dropout*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "NEW_TRY = True\n",
    "PATH = 'Data sets/fruits/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def set_training_data(rows, columns, channels):\n",
    "    images_path = PATH + 'images/'\n",
    "    directories = [d for d in os.listdir(images_path) \n",
    "                  if (os.path.isdir(os.path.join(images_path, d)))]\n",
    "\n",
    "    labels_idx = {}\n",
    "    images_dataset = {}\n",
    "    for i, d in zip(range(len(directories)), directories):\n",
    "        label_dir = os.path.join(images_path, d)\n",
    "        category = d.capitalize()\n",
    "        labels_idx[category] = i\n",
    "\n",
    "        names = [f for f in os.listdir(label_dir) \n",
    "                 if f.endswith(\".jpg\") or f.endswith(\".png\")]\n",
    "        \n",
    "        for n in names:\n",
    "            file_name = os.path.join(label_dir, n)\n",
    "            images_dataset[str(i) + \"_\" + n.split(\".\")[0]] = [file_name, category]\n",
    "\n",
    "    num_categories = len(labels_idx)\n",
    "\n",
    "    train_data, test_data = train_test_split(list(images_dataset.keys()), test_size=0.2)\n",
    "    test_y = np.asarray([labels_idx[images_dataset[x][1]] for x in test_data])\n",
    "    train_y = np.asarray([labels_idx[images_dataset[x][1]] for x in train_data])\n",
    "\n",
    "    image_size=(rows, columns)\n",
    "    image_shape=(rows, columns, channels)\n",
    "\n",
    "    training_data = {\n",
    "        'train_data': train_data,\n",
    "        'test_data': test_data,\n",
    "        'train_y': train_y,\n",
    "        'test_y': test_y,\n",
    "        'num_categories': num_categories,\n",
    "        'image_size': image_size,\n",
    "        'image_shape': image_shape,\n",
    "        'images_dataset': images_dataset\n",
    "    }\n",
    "    with open(PATH + 'chk/training_data.pickle', 'wb') as handle:\n",
    "        pickle.dump(training_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    return training_data\n",
    "\n",
    "def load_training_data():\n",
    "    with open(PATH + 'chk/training_data.pickle', 'rb') as handle:\n",
    "        training_data = pickle.load(handle)\n",
    "        \n",
    "    return training_data\n",
    "\n",
    "#_____________________________________________________________________________\n",
    "\n",
    "if NEW_TRY:\n",
    "    training_data = set_training_data(100, 100, 3)\n",
    "else:\n",
    "    training_data = load_training_data()\n",
    "    \n",
    "\n",
    "train_data = training_data['train_data']\n",
    "test_data = training_data['test_data']\n",
    "train_y = training_data['train_y']\n",
    "test_y = training_data['test_y']\n",
    "num_categories = training_data['num_categories']\n",
    "image_size = training_data['image_size']\n",
    "image_shape = training_data['image_shape']\n",
    "images_dataset = training_data['images_dataset']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 50, 50, 32)        2400      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 50, 50, 32)        200       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 50, 50, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 50, 50, 64)        18432     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 25, 25, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 25, 25, 64)        100       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 25, 25, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 25, 25, 128)       73728     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_2 (Average (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 12, 12, 128)       48        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 12, 12, 256)       294912    \n",
      "_________________________________________________________________\n",
      "average_pooling2d_3 (Average (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 6, 6, 256)         24        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2048)              18876416  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 768)               1573632   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 103)               52839     \n",
      "=================================================================\n",
      "Total params: 21,286,459\n",
      "Trainable params: 21,286,273\n",
      "Non-trainable params: 186\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, model_from_json\n",
    "from keras import layers, regularizers as regs\n",
    "from keras.models import load_model\n",
    "\n",
    "def create_model():\n",
    "    tf_model = Sequential([\n",
    "        layers.Conv2D(32,             # Aplicar pocos filtros destinados a \n",
    "                      kernel_size=(5, 5), # ... identificar rasgos grandes\n",
    "                      input_shape=image_shape,\n",
    "                      padding='same', # Mantener tamaño original de la imagen\n",
    "                      strides=(2, 2), # Barrer la imagen en pasos de 2 pixeles\n",
    "                      use_bias=False  # Bias innecesario por 'BatchNormalization'\n",
    "                     ),\n",
    "        layers.BatchNormalization(axis = 1), # La normalización debe ser\n",
    "        layers.Activation(\"relu\"),           # anterior a la activación        \n",
    "        \n",
    "        layers.Conv2D(64, kernel_size=(3, 3), padding='same', use_bias=False),\n",
    "        layers.AveragePooling2D(pool_size=(2, 2)), # Reducir utilizando la media\n",
    "        layers.BatchNormalization(axis = 1),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Conv2D(128, kernel_size=(3, 3), padding='same', use_bias=False),\n",
    "        layers.AveragePooling2D(pool_size=(2, 2)),\n",
    "        layers.BatchNormalization(axis = 1),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Conv2D(256, kernel_size=(3, 3), padding='same', use_bias=False),\n",
    "        layers.AveragePooling2D(pool_size=(2, 2)),\n",
    "        layers.BatchNormalization(axis = 1),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Flatten(),    \n",
    "        layers.Dense(2048, activation='relu', kernel_regularizer=regs.l2(0.001)),\n",
    "        layers.Dropout(0.15),\n",
    "        layers.Dense(768, activation='relu', kernel_regularizer=regs.l2(0.001)),\n",
    "        layers.Dropout(0.15),\n",
    "        layers.Dense(512, activation='relu', kernel_regularizer=regs.l2(0.001)),\n",
    "        layers.Dense(num_categories, activation='softmax') #120\n",
    "        ])\n",
    "    tf_model.compile(optimizer='adam',\n",
    "                    loss='sparse_categorical_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "    tf_model.save(PATH + 'chk/tf_model.h5')\n",
    "    tf_model.summary()\n",
    "\n",
    "    return tf_model\n",
    "\n",
    "def reload_model():    \n",
    "    tf_model = load_model(PATH + 'chk/tf_model9.h5')\n",
    "    tf_model.summary()\n",
    "\n",
    "    return tf_model\n",
    "\n",
    "#_____________________________________________________________________________\n",
    "# NEW_TRY = False\n",
    "\n",
    "if NEW_TRY:\n",
    "    tf_model = create_model()\n",
    "else:\n",
    "    tf_model = reload_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 249s 24ms/step - loss: 5.0550 - acc: 0.1358 - val_loss: 3.1670 - val_acc: 0.3263\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 233s 23ms/step - loss: 2.7588 - acc: 0.3499 - val_loss: 2.8969 - val_acc: 0.3325\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 274s 27ms/step - loss: 2.0957 - acc: 0.5036 - val_loss: 1.6991 - val_acc: 0.5893\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 267s 26ms/step - loss: 1.7312 - acc: 0.5924 - val_loss: 1.5384 - val_acc: 0.6544\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 228s 22ms/step - loss: 1.5076 - acc: 0.6563 - val_loss: 1.1705 - val_acc: 0.7722\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 230s 23ms/step - loss: 1.3459 - acc: 0.7086 - val_loss: 1.0022 - val_acc: 0.8303\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 230s 23ms/step - loss: 1.2318 - acc: 0.7481 - val_loss: 0.9394 - val_acc: 0.8434\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 228s 22ms/step - loss: 1.1461 - acc: 0.7771 - val_loss: 0.9669 - val_acc: 0.8461\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 233s 23ms/step - loss: 1.0465 - acc: 0.7972 - val_loss: 0.8670 - val_acc: 0.8716\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 240s 24ms/step - loss: 0.9865 - acc: 0.8164 - val_loss: 0.7775 - val_acc: 0.8909\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.9645 - acc: 0.8306 - val_loss: 0.7930 - val_acc: 0.8997\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.9095 - acc: 0.8523 - val_loss: 0.7218 - val_acc: 0.9244\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 2285s 223ms/step - loss: 0.8539 - acc: 0.8618 - val_loss: 0.6520 - val_acc: 0.9226\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 347s 34ms/step - loss: 0.8153 - acc: 0.8709 - val_loss: 0.6493 - val_acc: 0.9279\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 570s 56ms/step - loss: 0.7682 - acc: 0.8850 - val_loss: 0.6053 - val_acc: 0.9305\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 756s 74ms/step - loss: 0.8011 - acc: 0.8783 - val_loss: 0.7198 - val_acc: 0.9208\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 2671s 261ms/step - loss: 0.7677 - acc: 0.8971 - val_loss: 0.5854 - val_acc: 0.9437\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 585s 57ms/step - loss: 0.7114 - acc: 0.8972 - val_loss: 0.5801 - val_acc: 0.9464\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 790s 77ms/step - loss: 0.6631 - acc: 0.9088 - val_loss: 0.5128 - val_acc: 0.9613\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 6395s 625ms/step - loss: 0.6490 - acc: 0.9101 - val_loss: 0.5188 - val_acc: 0.9516\n",
      "\n",
      "Train on 10224 samples, validate on 1136 samples\n",
      "Epoch 1/5\n",
      "10224/10224 [==============================] - 491s 48ms/step - loss: 0.6692 - acc: 0.9055 - val_loss: 0.5454 - val_acc: 0.9533\n",
      "Epoch 2/5\n",
      "10224/10224 [==============================] - 762s 75ms/step - loss: 0.6219 - acc: 0.9190 - val_loss: 0.4708 - val_acc: 0.9736\n",
      "Epoch 3/5\n",
      "10224/10224 [==============================] - 7609s 744ms/step - loss: 0.5903 - acc: 0.9242 - val_loss: 0.4753 - val_acc: 0.9560\n",
      "Epoch 4/5\n",
      "10224/10224 [==============================] - 448s 44ms/step - loss: 0.7288 - acc: 0.9080 - val_loss: 0.5653 - val_acc: 0.9577\n",
      "Epoch 5/5\n",
      "10224/10224 [==============================] - 728s 71ms/step - loss: 0.5844 - acc: 0.9334 - val_loss: 0.4546 - val_acc: 0.9674\n",
      "\n",
      "EPOCH END\n",
      "\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 7771s 760ms/step - loss: 0.5858 - acc: 0.9227 - val_loss: 0.4275 - val_acc: 0.9798\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 398s 39ms/step - loss: 0.5356 - acc: 0.9367 - val_loss: 0.4149 - val_acc: 0.9771\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 702s 69ms/step - loss: 0.5437 - acc: 0.9303 - val_loss: 0.4383 - val_acc: 0.9666\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 7854s 768ms/step - loss: 0.5382 - acc: 0.9331 - val_loss: 0.4262 - val_acc: 0.9719\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 350s 34ms/step - loss: 0.5045 - acc: 0.9398 - val_loss: 0.4443 - val_acc: 0.9631\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 642s 63ms/step - loss: 0.5215 - acc: 0.9356 - val_loss: 0.3788 - val_acc: 0.9824\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 7056s 690ms/step - loss: 0.5070 - acc: 0.9401 - val_loss: 0.3852 - val_acc: 0.9807\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 2141s 209ms/step - loss: 0.4862 - acc: 0.9457 - val_loss: 0.4051 - val_acc: 0.9719\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 411s 40ms/step - loss: 0.5103 - acc: 0.9361 - val_loss: 0.4004 - val_acc: 0.9824\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 692s 68ms/step - loss: 0.6156 - acc: 0.9343 - val_loss: 0.4206 - val_acc: 0.9745\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 2854s 279ms/step - loss: 0.4887 - acc: 0.9486 - val_loss: 0.3695 - val_acc: 0.9798\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 357s 35ms/step - loss: 0.4432 - acc: 0.9550 - val_loss: 0.3645 - val_acc: 0.9807\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 741s 72ms/step - loss: 0.4461 - acc: 0.9514 - val_loss: 0.3647 - val_acc: 0.9727\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 2943s 288ms/step - loss: 0.4530 - acc: 0.9461 - val_loss: 0.3959 - val_acc: 0.9683\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 350s 34ms/step - loss: 0.4532 - acc: 0.9488 - val_loss: 0.3889 - val_acc: 0.9622\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 708s 69ms/step - loss: 0.4744 - acc: 0.9438 - val_loss: 0.3392 - val_acc: 0.9859\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 4468s 437ms/step - loss: 0.4355 - acc: 0.9487 - val_loss: 0.3338 - val_acc: 0.9877\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 227s 22ms/step - loss: 0.4277 - acc: 0.9530 - val_loss: 0.3214 - val_acc: 0.9886\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 231s 23ms/step - loss: 0.4160 - acc: 0.9579 - val_loss: 0.3097 - val_acc: 0.9921\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 229s 22ms/step - loss: 0.3996 - acc: 0.9594 - val_loss: 0.3258 - val_acc: 0.9859\n",
      "\n",
      "Train on 10224 samples, validate on 1136 samples\n",
      "Epoch 1/5\n",
      "10224/10224 [==============================] - 228s 22ms/step - loss: 0.4205 - acc: 0.9518 - val_loss: 0.3161 - val_acc: 0.9833\n",
      "Epoch 2/5\n",
      "10224/10224 [==============================] - 225s 22ms/step - loss: 0.4066 - acc: 0.9557 - val_loss: 0.3223 - val_acc: 0.9842\n",
      "Epoch 3/5\n",
      "10224/10224 [==============================] - 226s 22ms/step - loss: 0.4073 - acc: 0.9564 - val_loss: 0.3128 - val_acc: 0.9877\n",
      "Epoch 4/5\n",
      "10224/10224 [==============================] - 227s 22ms/step - loss: 0.3916 - acc: 0.9589 - val_loss: 0.3192 - val_acc: 0.9877\n",
      "Epoch 5/5\n",
      "10224/10224 [==============================] - 224s 22ms/step - loss: 0.5004 - acc: 0.9556 - val_loss: 0.3484 - val_acc: 0.9894\n",
      "\n",
      "EPOCH END\n",
      "\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 227s 22ms/step - loss: 0.3978 - acc: 0.9609 - val_loss: 0.3471 - val_acc: 0.9666\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.3912 - acc: 0.9579 - val_loss: 0.2820 - val_acc: 0.9956\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.3788 - acc: 0.9603 - val_loss: 0.3008 - val_acc: 0.9877\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.3836 - acc: 0.9574 - val_loss: 0.3221 - val_acc: 0.9815\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.3718 - acc: 0.9625 - val_loss: 0.3037 - val_acc: 0.9886\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 2606s 255ms/step - loss: 0.3816 - acc: 0.9574 - val_loss: 0.2862 - val_acc: 0.9877\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 352s 34ms/step - loss: 0.3708 - acc: 0.9609 - val_loss: 0.2845 - val_acc: 0.9894\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 602s 59ms/step - loss: 0.3576 - acc: 0.9658 - val_loss: 0.2924 - val_acc: 0.9850\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 645s 63ms/step - loss: 0.3490 - acc: 0.9652 - val_loss: 0.2793 - val_acc: 0.9894\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 606s 59ms/step - loss: 0.3553 - acc: 0.9634 - val_loss: 0.2734 - val_acc: 0.9912\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.3532 - acc: 0.9635 - val_loss: 0.2672 - val_acc: 0.9912\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.3536 - acc: 0.9625 - val_loss: 0.2709 - val_acc: 0.9921\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 226s 22ms/step - loss: 0.3318 - acc: 0.9675 - val_loss: 0.2605 - val_acc: 0.9938\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 232s 23ms/step - loss: 0.3413 - acc: 0.9638 - val_loss: 0.2685 - val_acc: 0.9894\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.3225 - acc: 0.9729 - val_loss: 0.2576 - val_acc: 0.9912\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 226s 22ms/step - loss: 0.4268 - acc: 0.9554 - val_loss: 0.4400 - val_acc: 0.9859\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 229s 22ms/step - loss: 0.3639 - acc: 0.9704 - val_loss: 0.2551 - val_acc: 0.9965\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 226s 22ms/step - loss: 0.3270 - acc: 0.9680 - val_loss: 0.2883 - val_acc: 0.9807\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.3305 - acc: 0.9651 - val_loss: 0.2659 - val_acc: 0.9868\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.3065 - acc: 0.9730 - val_loss: 0.2427 - val_acc: 0.9956\n",
      "\n",
      "Train on 10224 samples, validate on 1136 samples\n",
      "Epoch 1/5\n",
      "10224/10224 [==============================] - 226s 22ms/step - loss: 0.3315 - acc: 0.9653 - val_loss: 0.2533 - val_acc: 0.9903\n",
      "Epoch 2/5\n",
      "10224/10224 [==============================] - 228s 22ms/step - loss: 0.3060 - acc: 0.9703 - val_loss: 0.2413 - val_acc: 0.9921\n",
      "Epoch 3/5\n",
      "10224/10224 [==============================] - 225s 22ms/step - loss: 0.3195 - acc: 0.9675 - val_loss: 0.2424 - val_acc: 0.9903\n",
      "Epoch 4/5\n",
      "10224/10224 [==============================] - 225s 22ms/step - loss: 0.3149 - acc: 0.9693 - val_loss: 0.2342 - val_acc: 0.9965\n",
      "Epoch 5/5\n",
      "10224/10224 [==============================] - 225s 22ms/step - loss: 0.2964 - acc: 0.9732 - val_loss: 0.2559 - val_acc: 0.9850\n",
      "\n",
      "EPOCH END\n",
      "\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 227s 22ms/step - loss: 0.3242 - acc: 0.9665 - val_loss: 0.2473 - val_acc: 0.9877\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 228s 22ms/step - loss: 0.3185 - acc: 0.9653 - val_loss: 0.2368 - val_acc: 0.9930\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 226s 22ms/step - loss: 0.2981 - acc: 0.9724 - val_loss: 0.2438 - val_acc: 0.9930\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.3062 - acc: 0.9706 - val_loss: 0.2441 - val_acc: 0.9938\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.3011 - acc: 0.9697 - val_loss: 0.2458 - val_acc: 0.9894\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.3100 - acc: 0.9702 - val_loss: 0.2305 - val_acc: 0.9938\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 226s 22ms/step - loss: 0.3016 - acc: 0.9717 - val_loss: 0.2591 - val_acc: 0.9859\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 230s 23ms/step - loss: 0.3038 - acc: 0.9719 - val_loss: 0.2523 - val_acc: 0.9868\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 232s 23ms/step - loss: 0.2797 - acc: 0.9771 - val_loss: 0.2540 - val_acc: 0.9886\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 228s 22ms/step - loss: 0.3313 - acc: 0.9745 - val_loss: 0.2657 - val_acc: 0.9938\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 230s 22ms/step - loss: 0.3238 - acc: 0.9682 - val_loss: 0.2535 - val_acc: 0.9921\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 236s 23ms/step - loss: 0.2882 - acc: 0.9711 - val_loss: 0.2742 - val_acc: 0.9798\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 233s 23ms/step - loss: 0.2925 - acc: 0.9726 - val_loss: 0.2180 - val_acc: 0.9947\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 231s 23ms/step - loss: 0.2780 - acc: 0.9741 - val_loss: 0.2267 - val_acc: 0.9903\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 227s 22ms/step - loss: 0.2901 - acc: 0.9729 - val_loss: 0.2189 - val_acc: 0.9947\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 230s 22ms/step - loss: 0.3017 - acc: 0.9664 - val_loss: 0.2259 - val_acc: 0.9903\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 232s 23ms/step - loss: 0.2723 - acc: 0.9779 - val_loss: 0.2283 - val_acc: 0.9930\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 238s 23ms/step - loss: 0.2654 - acc: 0.9790 - val_loss: 0.2077 - val_acc: 0.9956\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 235s 23ms/step - loss: 0.2810 - acc: 0.9714 - val_loss: 0.2268 - val_acc: 0.9894\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 239s 23ms/step - loss: 0.2903 - acc: 0.9690 - val_loss: 0.2148 - val_acc: 0.9965\n",
      "\n",
      "Train on 10224 samples, validate on 1136 samples\n",
      "Epoch 1/5\n",
      "10224/10224 [==============================] - 236s 23ms/step - loss: 0.2783 - acc: 0.9729 - val_loss: 0.2114 - val_acc: 0.9947\n",
      "Epoch 2/5\n",
      "10224/10224 [==============================] - 231s 23ms/step - loss: 0.2656 - acc: 0.9759 - val_loss: 0.2243 - val_acc: 0.9921\n",
      "Epoch 3/5\n",
      "10224/10224 [==============================] - 230s 23ms/step - loss: 0.2689 - acc: 0.9739 - val_loss: 0.2296 - val_acc: 0.9912\n",
      "Epoch 4/5\n",
      "10224/10224 [==============================] - 225s 22ms/step - loss: 0.2623 - acc: 0.9788 - val_loss: 0.2071 - val_acc: 0.9912\n",
      "Epoch 5/5\n",
      "10224/10224 [==============================] - 227s 22ms/step - loss: 0.2646 - acc: 0.9748 - val_loss: 0.2141 - val_acc: 0.9947\n",
      "\n",
      "EPOCH END\n",
      "\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 239s 23ms/step - loss: 0.2702 - acc: 0.9747 - val_loss: 0.2052 - val_acc: 0.9947\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 241s 24ms/step - loss: 0.2389 - acc: 0.9830 - val_loss: 0.2000 - val_acc: 0.9921\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 240s 23ms/step - loss: 0.2628 - acc: 0.9741 - val_loss: 0.1983 - val_acc: 0.9982\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 235s 23ms/step - loss: 0.2774 - acc: 0.9744 - val_loss: 0.3235 - val_acc: 0.9938\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 259s 25ms/step - loss: 0.3240 - acc: 0.9806 - val_loss: 0.2120 - val_acc: 0.9938\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 238s 23ms/step - loss: 0.2680 - acc: 0.9744 - val_loss: 0.1898 - val_acc: 0.9991\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 240s 23ms/step - loss: 0.2465 - acc: 0.9775 - val_loss: 0.2020 - val_acc: 0.9921\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 232s 23ms/step - loss: 0.2563 - acc: 0.9745 - val_loss: 0.1911 - val_acc: 0.9982\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10227/10227 [==============================] - 226s 22ms/step - loss: 0.2378 - acc: 0.9826 - val_loss: 0.1890 - val_acc: 0.9965\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.2553 - acc: 0.9741 - val_loss: 0.1906 - val_acc: 0.9982\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.2610 - acc: 0.9761 - val_loss: 0.1853 - val_acc: 0.9982\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 557s 54ms/step - loss: 0.2441 - acc: 0.9787 - val_loss: 0.2006 - val_acc: 0.9921\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 233s 23ms/step - loss: 0.2459 - acc: 0.9801 - val_loss: 0.1947 - val_acc: 0.9912\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 229s 22ms/step - loss: 0.2333 - acc: 0.9784 - val_loss: 0.1783 - val_acc: 0.9982\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.2445 - acc: 0.9771 - val_loss: 0.1982 - val_acc: 0.9912\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.2433 - acc: 0.9768 - val_loss: 0.1960 - val_acc: 0.9930\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.2496 - acc: 0.9771 - val_loss: 0.1874 - val_acc: 0.9930\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 2304s 225ms/step - loss: 0.2410 - acc: 0.9801 - val_loss: 0.1820 - val_acc: 0.9974\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 390s 38ms/step - loss: 0.2386 - acc: 0.9784 - val_loss: 0.1904 - val_acc: 0.9921\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 851s 83ms/step - loss: 0.2323 - acc: 0.9817 - val_loss: 0.1759 - val_acc: 0.9982\n",
      "\n",
      "Train on 10224 samples, validate on 1136 samples\n",
      "Epoch 1/5\n",
      "10224/10224 [==============================] - 7829s 766ms/step - loss: 0.2563 - acc: 0.9730 - val_loss: 0.1852 - val_acc: 0.9982\n",
      "Epoch 2/5\n",
      "10224/10224 [==============================] - 435s 42ms/step - loss: 0.2279 - acc: 0.9818 - val_loss: 0.1941 - val_acc: 0.9938\n",
      "Epoch 3/5\n",
      "10224/10224 [==============================] - 808s 79ms/step - loss: 0.2206 - acc: 0.9827 - val_loss: 0.1716 - val_acc: 0.9982\n",
      "Epoch 4/5\n",
      "10224/10224 [==============================] - 7807s 764ms/step - loss: 0.2448 - acc: 0.9766 - val_loss: 0.2403 - val_acc: 0.9868\n",
      "Epoch 5/5\n",
      "10224/10224 [==============================] - 449s 44ms/step - loss: 0.2135 - acc: 0.9851 - val_loss: 0.1795 - val_acc: 0.9965\n",
      "\n",
      "EPOCH END\n",
      "\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 826s 81ms/step - loss: 0.2493 - acc: 0.9743 - val_loss: 0.2025 - val_acc: 0.9894\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 7763s 759ms/step - loss: 0.2666 - acc: 0.9767 - val_loss: 0.2467 - val_acc: 0.9938\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 480s 47ms/step - loss: 0.2296 - acc: 0.9851 - val_loss: 0.1883 - val_acc: 0.9965\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 815s 80ms/step - loss: 0.2383 - acc: 0.9807 - val_loss: 0.2095 - val_acc: 0.9912\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 7742s 757ms/step - loss: 0.2426 - acc: 0.9796 - val_loss: 0.1868 - val_acc: 0.9974\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 476s 47ms/step - loss: 0.2373 - acc: 0.9797 - val_loss: 0.1848 - val_acc: 0.9947\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 821s 80ms/step - loss: 0.2321 - acc: 0.9802 - val_loss: 0.1779 - val_acc: 0.9982\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 7712s 754ms/step - loss: 0.2250 - acc: 0.9817 - val_loss: 0.2096 - val_acc: 0.9868\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 463s 45ms/step - loss: 0.2271 - acc: 0.9812 - val_loss: 0.1745 - val_acc: 0.9991\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 780s 76ms/step - loss: 0.2212 - acc: 0.9825 - val_loss: 0.1905 - val_acc: 0.9894\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 2343s 229ms/step - loss: 0.2322 - acc: 0.9793 - val_loss: 0.1707 - val_acc: 0.9956\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 573s 56ms/step - loss: 0.2088 - acc: 0.9829 - val_loss: 0.1732 - val_acc: 0.9974\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 891s 87ms/step - loss: 0.2091 - acc: 0.9828 - val_loss: 0.1892 - val_acc: 0.9886\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 7643s 747ms/step - loss: 0.2330 - acc: 0.9775 - val_loss: 0.1871 - val_acc: 0.9947\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 510s 50ms/step - loss: 0.2399 - acc: 0.9773 - val_loss: 0.2092 - val_acc: 0.9859\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 786s 77ms/step - loss: 0.2163 - acc: 0.9837 - val_loss: 0.1677 - val_acc: 0.9974\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 2615s 256ms/step - loss: 0.2258 - acc: 0.9800 - val_loss: 0.2014 - val_acc: 0.9842\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 229s 22ms/step - loss: 0.2224 - acc: 0.9813 - val_loss: 0.1796 - val_acc: 0.9965\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 231s 23ms/step - loss: 0.2239 - acc: 0.9828 - val_loss: 0.1697 - val_acc: 0.9974\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 230s 22ms/step - loss: 0.2226 - acc: 0.9843 - val_loss: 0.1739 - val_acc: 0.9982\n",
      "\n",
      "Train on 10224 samples, validate on 1136 samples\n",
      "Epoch 1/5\n",
      "10224/10224 [==============================] - 228s 22ms/step - loss: 0.2274 - acc: 0.9807 - val_loss: 0.1834 - val_acc: 0.9974\n",
      "Epoch 2/5\n",
      "10224/10224 [==============================] - 227s 22ms/step - loss: 0.2435 - acc: 0.9824 - val_loss: 0.2265 - val_acc: 0.9956\n",
      "Epoch 3/5\n",
      "10224/10224 [==============================] - 227s 22ms/step - loss: 0.2144 - acc: 0.9900 - val_loss: 0.1539 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "10224/10224 [==============================] - 227s 22ms/step - loss: 0.2029 - acc: 0.9839 - val_loss: 0.1605 - val_acc: 0.9947\n",
      "Epoch 5/5\n",
      "10224/10224 [==============================] - 225s 22ms/step - loss: 0.2242 - acc: 0.9799 - val_loss: 0.1654 - val_acc: 0.9974\n",
      "\n",
      "EPOCH END\n",
      "\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 240s 23ms/step - loss: 0.2242 - acc: 0.9778 - val_loss: 0.1662 - val_acc: 0.9991\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 230s 22ms/step - loss: 0.2207 - acc: 0.9810 - val_loss: 0.1636 - val_acc: 0.9991\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 235s 23ms/step - loss: 0.1997 - acc: 0.9860 - val_loss: 0.1624 - val_acc: 0.9930\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 236s 23ms/step - loss: 0.1922 - acc: 0.9836 - val_loss: 0.1551 - val_acc: 0.9974\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 236s 23ms/step - loss: 0.1972 - acc: 0.9841 - val_loss: 0.1552 - val_acc: 0.9982\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 233s 23ms/step - loss: 0.2135 - acc: 0.9799 - val_loss: 0.1692 - val_acc: 0.9982\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 230s 22ms/step - loss: 0.2035 - acc: 0.9840 - val_loss: 0.1638 - val_acc: 0.9965\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.2095 - acc: 0.9821 - val_loss: 0.1690 - val_acc: 0.9965\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 236s 23ms/step - loss: 0.2094 - acc: 0.9828 - val_loss: 0.1601 - val_acc: 0.9965\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 235s 23ms/step - loss: 0.2000 - acc: 0.9848 - val_loss: 0.1636 - val_acc: 0.9956\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 228s 22ms/step - loss: 0.2137 - acc: 0.9806 - val_loss: 0.1598 - val_acc: 0.9982\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 226s 22ms/step - loss: 0.2064 - acc: 0.9831 - val_loss: 0.1562 - val_acc: 0.9991\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 231s 23ms/step - loss: 0.2184 - acc: 0.9789 - val_loss: 0.1872 - val_acc: 0.9903\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 234s 23ms/step - loss: 0.2052 - acc: 0.9847 - val_loss: 0.1655 - val_acc: 0.9982\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 232s 23ms/step - loss: 0.1997 - acc: 0.9857 - val_loss: 0.1568 - val_acc: 0.9974\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 233s 23ms/step - loss: 0.2158 - acc: 0.9789 - val_loss: 0.1611 - val_acc: 0.9974\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 226s 22ms/step - loss: 0.1991 - acc: 0.9850 - val_loss: 0.1631 - val_acc: 0.9965\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 231s 23ms/step - loss: 0.1919 - acc: 0.9856 - val_loss: 0.1665 - val_acc: 0.9921\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 236s 23ms/step - loss: 0.1983 - acc: 0.9852 - val_loss: 0.1661 - val_acc: 0.9938\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 227s 22ms/step - loss: 0.1838 - acc: 0.9866 - val_loss: 0.1600 - val_acc: 0.9938\n",
      "\n",
      "Train on 10224 samples, validate on 1136 samples\n",
      "Epoch 1/5\n",
      "10224/10224 [==============================] - 235s 23ms/step - loss: 0.2294 - acc: 0.9760 - val_loss: 0.1666 - val_acc: 0.9974\n",
      "Epoch 2/5\n",
      "10224/10224 [==============================] - 225s 22ms/step - loss: 0.2054 - acc: 0.9853 - val_loss: 0.1693 - val_acc: 0.9965\n",
      "Epoch 3/5\n",
      "10224/10224 [==============================] - 230s 22ms/step - loss: 0.2026 - acc: 0.9844 - val_loss: 0.1525 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "10224/10224 [==============================] - 231s 23ms/step - loss: 0.1997 - acc: 0.9851 - val_loss: 0.1612 - val_acc: 0.9982\n",
      "Epoch 5/5\n",
      "10224/10224 [==============================] - 225s 22ms/step - loss: 0.1843 - acc: 0.9888 - val_loss: 0.1488 - val_acc: 0.9982\n",
      "\n",
      "EPOCH END\n",
      "\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 239s 23ms/step - loss: 0.1974 - acc: 0.9825 - val_loss: 0.1966 - val_acc: 0.9842\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 239s 23ms/step - loss: 0.2067 - acc: 0.9850 - val_loss: 0.1499 - val_acc: 0.9982\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 232s 23ms/step - loss: 0.2222 - acc: 0.9818 - val_loss: 0.2062 - val_acc: 0.9938\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 226s 22ms/step - loss: 0.2113 - acc: 0.9861 - val_loss: 0.1673 - val_acc: 0.9991\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.2128 - acc: 0.9819 - val_loss: 0.1995 - val_acc: 0.9850\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 229s 22ms/step - loss: 0.2117 - acc: 0.9842 - val_loss: 0.1527 - val_acc: 0.9991\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 226s 22ms/step - loss: 0.1942 - acc: 0.9837 - val_loss: 0.1736 - val_acc: 0.9912\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 226s 22ms/step - loss: 0.1874 - acc: 0.9867 - val_loss: 0.1389 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 229s 22ms/step - loss: 0.1686 - acc: 0.9890 - val_loss: 0.1697 - val_acc: 0.9912\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 230s 23ms/step - loss: 0.1930 - acc: 0.9811 - val_loss: 0.1691 - val_acc: 0.9912\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1946 - acc: 0.9850 - val_loss: 0.1587 - val_acc: 0.9912\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1977 - acc: 0.9830 - val_loss: 0.1534 - val_acc: 0.9956\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1646 - acc: 0.9910 - val_loss: 0.1339 - val_acc: 0.9982\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1802 - acc: 0.9846 - val_loss: 0.1627 - val_acc: 0.9921\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1871 - acc: 0.9846 - val_loss: 0.1615 - val_acc: 0.9930\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1977 - acc: 0.9829 - val_loss: 0.1555 - val_acc: 0.9956\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1863 - acc: 0.9848 - val_loss: 0.1522 - val_acc: 0.9974\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1797 - acc: 0.9856 - val_loss: 0.1552 - val_acc: 0.9956\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 2370s 232ms/step - loss: 0.1648 - acc: 0.9894 - val_loss: 0.1436 - val_acc: 0.9956\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 560s 55ms/step - loss: 0.1937 - acc: 0.9819 - val_loss: 0.1635 - val_acc: 0.9938\n",
      "\n",
      "Train on 10224 samples, validate on 1136 samples\n",
      "Epoch 1/5\n",
      "10224/10224 [==============================] - 753s 74ms/step - loss: 0.1718 - acc: 0.9881 - val_loss: 0.1568 - val_acc: 0.9886\n",
      "Epoch 2/5\n",
      "10224/10224 [==============================] - 7652s 748ms/step - loss: 0.1837 - acc: 0.9846 - val_loss: 0.1564 - val_acc: 0.9965\n",
      "Epoch 3/5\n",
      "10224/10224 [==============================] - 465s 45ms/step - loss: 0.1922 - acc: 0.9829 - val_loss: 0.1448 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "10224/10224 [==============================] - 712s 70ms/step - loss: 0.1860 - acc: 0.9837 - val_loss: 0.1441 - val_acc: 0.9982\n",
      "Epoch 5/5\n",
      "10224/10224 [==============================] - 7765s 760ms/step - loss: 0.1792 - acc: 0.9858 - val_loss: 0.1455 - val_acc: 0.9982\n",
      "\n",
      "EPOCH END\n",
      "\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 402s 39ms/step - loss: 0.1840 - acc: 0.9854 - val_loss: 0.1556 - val_acc: 0.9947\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 728s 71ms/step - loss: 0.1891 - acc: 0.9846 - val_loss: 0.1553 - val_acc: 0.9930\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 7822s 765ms/step - loss: 0.1809 - acc: 0.9851 - val_loss: 0.1614 - val_acc: 0.9930\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 348s 34ms/step - loss: 0.1897 - acc: 0.9853 - val_loss: 0.1644 - val_acc: 0.9938\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 671s 66ms/step - loss: 0.2000 - acc: 0.9841 - val_loss: 0.1575 - val_acc: 0.9991\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 2825s 276ms/step - loss: 0.2015 - acc: 0.9858 - val_loss: 0.1646 - val_acc: 0.9965\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 352s 34ms/step - loss: 0.1995 - acc: 0.9870 - val_loss: 0.1502 - val_acc: 0.9974\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 716s 70ms/step - loss: 0.1661 - acc: 0.9914 - val_loss: 0.1402 - val_acc: 0.9982\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 7965s 779ms/step - loss: 0.1816 - acc: 0.9852 - val_loss: 0.1442 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 347s 34ms/step - loss: 0.1970 - acc: 0.9842 - val_loss: 0.1690 - val_acc: 0.9965\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 548s 54ms/step - loss: 0.1792 - acc: 0.9891 - val_loss: 0.1350 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 753s 74ms/step - loss: 0.1917 - acc: 0.9835 - val_loss: 0.1767 - val_acc: 0.9912\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 7605s 744ms/step - loss: 0.1916 - acc: 0.9854 - val_loss: 0.1518 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 466s 46ms/step - loss: 0.1763 - acc: 0.9871 - val_loss: 0.1918 - val_acc: 0.9859\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 691s 68ms/step - loss: 0.1666 - acc: 0.9890 - val_loss: 0.1591 - val_acc: 0.9868\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 7724s 755ms/step - loss: 0.1924 - acc: 0.9800 - val_loss: 0.1884 - val_acc: 0.9868\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 349s 34ms/step - loss: 0.1689 - acc: 0.9900 - val_loss: 0.1387 - val_acc: 0.9974\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 657s 64ms/step - loss: 0.1788 - acc: 0.9861 - val_loss: 0.1387 - val_acc: 0.9991\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10227/10227 [==============================] - 3101s 303ms/step - loss: 0.1591 - acc: 0.9903 - val_loss: 0.1642 - val_acc: 0.9894\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 350s 34ms/step - loss: 0.1870 - acc: 0.9843 - val_loss: 0.1482 - val_acc: 0.9956\n",
      "\n",
      "Train on 10224 samples, validate on 1136 samples\n",
      "Epoch 1/5\n",
      "10224/10224 [==============================] - 582s 57ms/step - loss: 0.1691 - acc: 0.9886 - val_loss: 0.1295 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "10224/10224 [==============================] - 757s 74ms/step - loss: 0.1893 - acc: 0.9844 - val_loss: 0.1497 - val_acc: 0.9974\n",
      "Epoch 3/5\n",
      "10224/10224 [==============================] - 7582s 742ms/step - loss: 0.1693 - acc: 0.9905 - val_loss: 0.1422 - val_acc: 0.9965\n",
      "Epoch 4/5\n",
      "10224/10224 [==============================] - 461s 45ms/step - loss: 0.1822 - acc: 0.9848 - val_loss: 0.1420 - val_acc: 0.9974\n",
      "Epoch 5/5\n",
      "10224/10224 [==============================] - 679s 66ms/step - loss: 0.1590 - acc: 0.9918 - val_loss: 0.1365 - val_acc: 0.9965\n",
      "\n",
      "EPOCH END\n",
      "\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 2371s 232ms/step - loss: 0.1881 - acc: 0.9836 - val_loss: 0.1843 - val_acc: 0.9850\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 468s 46ms/step - loss: 0.1916 - acc: 0.9834 - val_loss: 0.1441 - val_acc: 0.9947\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 752s 74ms/step - loss: 0.1698 - acc: 0.9883 - val_loss: 0.1419 - val_acc: 0.9947\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 2682s 262ms/step - loss: 0.1855 - acc: 0.9865 - val_loss: 0.1748 - val_acc: 0.9974\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 226s 22ms/step - loss: 0.1800 - acc: 0.9889 - val_loss: 0.1432 - val_acc: 0.9982\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1737 - acc: 0.9881 - val_loss: 0.1385 - val_acc: 0.9974\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1682 - acc: 0.9879 - val_loss: 0.1372 - val_acc: 0.9965\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1618 - acc: 0.9881 - val_loss: 0.1327 - val_acc: 0.9982\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1696 - acc: 0.9868 - val_loss: 0.1268 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1735 - acc: 0.9865 - val_loss: 0.1317 - val_acc: 0.9991\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1695 - acc: 0.9875 - val_loss: 0.1396 - val_acc: 0.9974\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1665 - acc: 0.9881 - val_loss: 0.1418 - val_acc: 0.9947\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 226s 22ms/step - loss: 0.1558 - acc: 0.9899 - val_loss: 0.1239 - val_acc: 0.9965\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1580 - acc: 0.9886 - val_loss: 0.1458 - val_acc: 0.9912\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1841 - acc: 0.9837 - val_loss: 0.1469 - val_acc: 0.9947\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 230s 22ms/step - loss: 0.1758 - acc: 0.9874 - val_loss: 0.1298 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 227s 22ms/step - loss: 0.1671 - acc: 0.9878 - val_loss: 0.1408 - val_acc: 0.9947\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1743 - acc: 0.9873 - val_loss: 0.1399 - val_acc: 0.9982\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 226s 22ms/step - loss: 0.1581 - acc: 0.9899 - val_loss: 0.1411 - val_acc: 0.9938\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1814 - acc: 0.9846 - val_loss: 0.1613 - val_acc: 0.9921\n",
      "\n",
      "Train on 10224 samples, validate on 1136 samples\n",
      "Epoch 1/5\n",
      "10224/10224 [==============================] - 224s 22ms/step - loss: 0.1823 - acc: 0.9846 - val_loss: 0.1359 - val_acc: 0.9991\n",
      "Epoch 2/5\n",
      "10224/10224 [==============================] - 227s 22ms/step - loss: 0.1730 - acc: 0.9877 - val_loss: 0.1302 - val_acc: 0.9991\n",
      "Epoch 3/5\n",
      "10224/10224 [==============================] - 227s 22ms/step - loss: 0.1452 - acc: 0.9935 - val_loss: 0.1175 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "10224/10224 [==============================] - 225s 22ms/step - loss: 0.1396 - acc: 0.9909 - val_loss: 0.2416 - val_acc: 0.9727\n",
      "Epoch 5/5\n",
      "10224/10224 [==============================] - 224s 22ms/step - loss: 0.1927 - acc: 0.9820 - val_loss: 0.1567 - val_acc: 0.9930\n",
      "\n",
      "EPOCH END\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import skimage\n",
    "from skimage.color import rgba2rgb\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "def get_training_sample(start=0, length=1000):\n",
    "    train_images = []\n",
    "    end = min(start+length, len(train_data))\n",
    "    for name in train_data[start:end]:\n",
    "        image = skimage.data.imread(images_dataset[name][0])\n",
    "        if image.shape[0] > image.shape[1]:\n",
    "            image = skimage.transform.rotate(image, 90, resize=True)\n",
    "        image = skimage.transform.resize(image, image_size, mode='constant')\n",
    "        if image.shape != image_shape:\n",
    "            image = rgba2rgb(image)\n",
    "        train_images.append(image)\n",
    "    \n",
    "    return np.asarray(train_images), train_y[start:end]\n",
    "\n",
    "def train_network():\n",
    "    segments = 5\n",
    "    length = int(len(train_data) / segments + 1)\n",
    "    for j in range(10):\n",
    "        start = 0\n",
    "        for i in range(segments):\n",
    "            train_images, train_targets = get_training_sample(start, length)\n",
    "            result = tf_model.fit(train_images, train_targets, epochs = 5, \n",
    "                                  validation_split=0.1, verbose=1)\n",
    "            start += length\n",
    "            print()\n",
    "        tf_model.save(PATH + 'chk/tf_dogs_model' + str(j) + '.h5')\n",
    "        print(\"EPOCH END\\n\\n\")\n",
    "\n",
    "train_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 227s 22ms/step - loss: 0.2047 - acc: 0.9818 - val_loss: 0.1605 - val_acc: 0.9956\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1687 - acc: 0.9899 - val_loss: 0.1346 - val_acc: 0.9991\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1679 - acc: 0.9884 - val_loss: 0.1338 - val_acc: 0.9991\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1980 - acc: 0.9858 - val_loss: 0.1616 - val_acc: 0.9921\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 229s 22ms/step - loss: 0.1595 - acc: 0.9914 - val_loss: 0.1364 - val_acc: 0.9991\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 229s 22ms/step - loss: 0.1552 - acc: 0.9910 - val_loss: 0.1260 - val_acc: 0.9956\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1619 - acc: 0.9855 - val_loss: 0.1334 - val_acc: 0.9974\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1553 - acc: 0.9907 - val_loss: 0.1173 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 227s 22ms/step - loss: 0.1417 - acc: 0.9919 - val_loss: 0.1343 - val_acc: 0.9947\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 230s 22ms/step - loss: 0.1645 - acc: 0.9887 - val_loss: 0.1199 - val_acc: 0.9991\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 226s 22ms/step - loss: 0.1694 - acc: 0.9840 - val_loss: 0.1447 - val_acc: 0.9903\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 226s 22ms/step - loss: 0.2087 - acc: 0.9787 - val_loss: 0.1562 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1684 - acc: 0.9915 - val_loss: 0.1507 - val_acc: 0.9956\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1663 - acc: 0.9883 - val_loss: 0.1827 - val_acc: 0.9842\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1568 - acc: 0.9908 - val_loss: 0.1232 - val_acc: 1.0000\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1592 - acc: 0.9882 - val_loss: 0.1423 - val_acc: 0.9947\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1621 - acc: 0.9885 - val_loss: 0.1232 - val_acc: 0.9991\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1448 - acc: 0.9916 - val_loss: 0.1164 - val_acc: 0.9991\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1675 - acc: 0.9844 - val_loss: 0.1271 - val_acc: 0.9965\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1382 - acc: 0.9933 - val_loss: 0.1226 - val_acc: 0.9974\n",
      "\n",
      "Train on 10224 samples, validate on 1136 samples\n",
      "Epoch 1/5\n",
      "10224/10224 [==============================] - 229s 22ms/step - loss: 0.1765 - acc: 0.9823 - val_loss: 0.1456 - val_acc: 0.9938\n",
      "Epoch 2/5\n",
      "10224/10224 [==============================] - 224s 22ms/step - loss: 0.1784 - acc: 0.9851 - val_loss: 0.1364 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "10224/10224 [==============================] - 224s 22ms/step - loss: 0.1708 - acc: 0.9888 - val_loss: 0.1288 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "10224/10224 [==============================] - 225s 22ms/step - loss: 0.1414 - acc: 0.9933 - val_loss: 0.1250 - val_acc: 0.9938\n",
      "Epoch 5/5\n",
      "10224/10224 [==============================] - 225s 22ms/step - loss: 0.1455 - acc: 0.9902 - val_loss: 0.1141 - val_acc: 0.9982\n",
      "\n",
      "EPOCH END\n",
      "\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 230s 22ms/step - loss: 0.1730 - acc: 0.9816 - val_loss: 0.1516 - val_acc: 0.9947\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 227s 22ms/step - loss: 0.1500 - acc: 0.9919 - val_loss: 0.1244 - val_acc: 0.9974\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1414 - acc: 0.9921 - val_loss: 0.1150 - val_acc: 0.9982\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 226s 22ms/step - loss: 0.1345 - acc: 0.9921 - val_loss: 0.1394 - val_acc: 0.9912\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 226s 22ms/step - loss: 0.1754 - acc: 0.9849 - val_loss: 0.1309 - val_acc: 0.9982\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1597 - acc: 0.9882 - val_loss: 0.1259 - val_acc: 0.9991\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1749 - acc: 0.9848 - val_loss: 0.1485 - val_acc: 0.9991\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 2578s 252ms/step - loss: 0.1727 - acc: 0.9893 - val_loss: 0.1544 - val_acc: 0.9974\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 437s 43ms/step - loss: 0.1615 - acc: 0.9916 - val_loss: 0.1254 - val_acc: 0.9982\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 749s 73ms/step - loss: 0.1436 - acc: 0.9917 - val_loss: 0.1231 - val_acc: 0.9965\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 2947s 288ms/step - loss: 0.1639 - acc: 0.9874 - val_loss: 0.1286 - val_acc: 0.9982\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 488s 48ms/step - loss: 0.1592 - acc: 0.9863 - val_loss: 0.1173 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 854s 84ms/step - loss: 0.1578 - acc: 0.9898 - val_loss: 0.1292 - val_acc: 0.9974\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 7741s 757ms/step - loss: 0.1532 - acc: 0.9900 - val_loss: 0.1418 - val_acc: 0.9938\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 446s 44ms/step - loss: 0.1863 - acc: 0.9853 - val_loss: 0.1381 - val_acc: 0.9974\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 762s 74ms/step - loss: 0.1723 - acc: 0.9855 - val_loss: 0.2140 - val_acc: 0.9894\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 7766s 759ms/step - loss: 0.1635 - acc: 0.9882 - val_loss: 0.1374 - val_acc: 0.9965\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 427s 42ms/step - loss: 0.1597 - acc: 0.9896 - val_loss: 0.1834 - val_acc: 0.9833\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 749s 73ms/step - loss: 0.1358 - acc: 0.9936 - val_loss: 0.1502 - val_acc: 0.9894\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 7827s 765ms/step - loss: 0.1247 - acc: 0.9954 - val_loss: 0.1151 - val_acc: 0.9965\n",
      "\n",
      "Train on 10224 samples, validate on 1136 samples\n",
      "Epoch 1/5\n",
      "10224/10224 [==============================] - 414s 41ms/step - loss: 0.1520 - acc: 0.9858 - val_loss: 0.1219 - val_acc: 0.9974\n",
      "Epoch 2/5\n",
      "10224/10224 [==============================] - 791s 77ms/step - loss: 0.1612 - acc: 0.9874 - val_loss: 0.1245 - val_acc: 0.9982\n",
      "Epoch 3/5\n",
      "10224/10224 [==============================] - 3730s 365ms/step - loss: 0.1423 - acc: 0.9921 - val_loss: 0.1128 - val_acc: 0.9991\n",
      "Epoch 4/5\n",
      "10224/10224 [==============================] - 474s 46ms/step - loss: 0.1536 - acc: 0.9865 - val_loss: 0.1197 - val_acc: 0.9991\n",
      "Epoch 5/5\n",
      "10224/10224 [==============================] - 845s 83ms/step - loss: 0.1589 - acc: 0.9885 - val_loss: 0.1493 - val_acc: 0.9930\n",
      "\n",
      "EPOCH END\n",
      "\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 7725s 755ms/step - loss: 0.1466 - acc: 0.9931 - val_loss: 0.1728 - val_acc: 0.9842\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 463s 45ms/step - loss: 0.1461 - acc: 0.9895 - val_loss: 0.1880 - val_acc: 0.9824\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 788s 77ms/step - loss: 0.1704 - acc: 0.9852 - val_loss: 0.1725 - val_acc: 0.9886\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 7756s 758ms/step - loss: 0.1497 - acc: 0.9904 - val_loss: 0.1367 - val_acc: 0.9982\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 430s 42ms/step - loss: 0.1674 - acc: 0.9867 - val_loss: 0.1452 - val_acc: 0.9947\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10227/10227 [==============================] - 748s 73ms/step - loss: 0.1725 - acc: 0.9858 - val_loss: 0.1327 - val_acc: 0.9974\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 7800s 763ms/step - loss: 0.1516 - acc: 0.9915 - val_loss: 0.1222 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 406s 40ms/step - loss: 0.1200 - acc: 0.9970 - val_loss: 0.1039 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 742s 73ms/step - loss: 0.1414 - acc: 0.9896 - val_loss: 0.1527 - val_acc: 0.9877\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 7511s 734ms/step - loss: 0.1445 - acc: 0.9919 - val_loss: 0.1421 - val_acc: 0.9912\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 388s 38ms/step - loss: 0.1564 - acc: 0.9880 - val_loss: 0.1327 - val_acc: 0.9965\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 789s 77ms/step - loss: 0.1417 - acc: 0.9907 - val_loss: 0.1276 - val_acc: 0.9956\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 2572s 251ms/step - loss: 0.1678 - acc: 0.9853 - val_loss: 0.1302 - val_acc: 0.9991\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 534s 52ms/step - loss: 0.1638 - acc: 0.9886 - val_loss: 0.1562 - val_acc: 0.9930\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 938s 92ms/step - loss: 0.1535 - acc: 0.9922 - val_loss: 0.1322 - val_acc: 0.9982\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 14125s 1s/step - loss: 0.1792 - acc: 0.9859 - val_loss: 0.1301 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 462s 45ms/step - loss: 0.1530 - acc: 0.9906 - val_loss: 0.1206 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 799s 78ms/step - loss: 0.1401 - acc: 0.9919 - val_loss: 0.1133 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 7755s 758ms/step - loss: 0.1383 - acc: 0.9920 - val_loss: 0.1105 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 436s 43ms/step - loss: 0.1476 - acc: 0.9884 - val_loss: 0.1136 - val_acc: 1.0000\n",
      "\n",
      "Train on 10224 samples, validate on 1136 samples\n",
      "Epoch 1/5\n",
      "10224/10224 [==============================] - 762s 74ms/step - loss: 0.1577 - acc: 0.9886 - val_loss: 0.1277 - val_acc: 0.9982\n",
      "Epoch 2/5\n",
      "10224/10224 [==============================] - 7771s 760ms/step - loss: 0.1529 - acc: 0.9887 - val_loss: 0.1347 - val_acc: 0.9965\n",
      "Epoch 3/5\n",
      "10224/10224 [==============================] - 434s 42ms/step - loss: 0.1333 - acc: 0.9940 - val_loss: 0.1076 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "10224/10224 [==============================] - 763s 75ms/step - loss: 0.1481 - acc: 0.9880 - val_loss: 0.1168 - val_acc: 0.9991\n",
      "Epoch 5/5\n",
      "10224/10224 [==============================] - 7804s 763ms/step - loss: 0.1653 - acc: 0.9869 - val_loss: 0.1335 - val_acc: 0.9938\n",
      "\n",
      "EPOCH END\n",
      "\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 391s 38ms/step - loss: 0.1561 - acc: 0.9886 - val_loss: 0.1175 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 746s 73ms/step - loss: 0.1267 - acc: 0.9949 - val_loss: 0.1150 - val_acc: 0.9974\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 7819s 765ms/step - loss: 0.1543 - acc: 0.9858 - val_loss: 0.1131 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 351s 34ms/step - loss: 0.1440 - acc: 0.9907 - val_loss: 0.1184 - val_acc: 0.9982\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 719s 70ms/step - loss: 0.1608 - acc: 0.9870 - val_loss: 0.1253 - val_acc: 0.9991\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 7667s 750ms/step - loss: 0.1522 - acc: 0.9909 - val_loss: 0.1266 - val_acc: 0.9956\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 2409s 236ms/step - loss: 0.1551 - acc: 0.9893 - val_loss: 0.1168 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 464s 45ms/step - loss: 0.1394 - acc: 0.9915 - val_loss: 0.1162 - val_acc: 0.9991\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 733s 72ms/step - loss: 0.1452 - acc: 0.9886 - val_loss: 0.1262 - val_acc: 0.9956\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 2747s 269ms/step - loss: 0.1296 - acc: 0.9928 - val_loss: 0.1257 - val_acc: 0.9947\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 490s 48ms/step - loss: 0.1421 - acc: 0.9896 - val_loss: 0.1200 - val_acc: 0.9974\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 846s 83ms/step - loss: 0.1439 - acc: 0.9911 - val_loss: 0.1144 - val_acc: 0.9982\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 3384s 331ms/step - loss: 0.1385 - acc: 0.9919 - val_loss: 0.1135 - val_acc: 0.9991\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 512s 50ms/step - loss: 0.1579 - acc: 0.9869 - val_loss: 0.1310 - val_acc: 0.9982\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 818s 80ms/step - loss: 0.1459 - acc: 0.9914 - val_loss: 0.1218 - val_acc: 0.9965\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 4183s 409ms/step - loss: 0.1527 - acc: 0.9888 - val_loss: 0.1413 - val_acc: 0.9912\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 226s 22ms/step - loss: 0.1369 - acc: 0.9930 - val_loss: 0.1183 - val_acc: 0.9974\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 227s 22ms/step - loss: 0.1460 - acc: 0.9885 - val_loss: 0.1395 - val_acc: 0.9974\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 227s 22ms/step - loss: 0.1362 - acc: 0.9925 - val_loss: 0.1122 - val_acc: 0.9991\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 226s 22ms/step - loss: 0.1715 - acc: 0.9854 - val_loss: 0.1290 - val_acc: 0.9965\n",
      "\n",
      "Train on 10224 samples, validate on 1136 samples\n",
      "Epoch 1/5\n",
      "10224/10224 [==============================] - 228s 22ms/step - loss: 0.1630 - acc: 0.9886 - val_loss: 0.1267 - val_acc: 0.9991\n",
      "Epoch 2/5\n",
      "10224/10224 [==============================] - 227s 22ms/step - loss: 0.1328 - acc: 0.9954 - val_loss: 0.1124 - val_acc: 0.9974\n",
      "Epoch 3/5\n",
      "10224/10224 [==============================] - 227s 22ms/step - loss: 0.1482 - acc: 0.9901 - val_loss: 0.1322 - val_acc: 0.9956\n",
      "Epoch 4/5\n",
      "10224/10224 [==============================] - 231s 23ms/step - loss: 0.1500 - acc: 0.9901 - val_loss: 0.1167 - val_acc: 0.9974\n",
      "Epoch 5/5\n",
      "10224/10224 [==============================] - 225s 22ms/step - loss: 0.1171 - acc: 0.9965 - val_loss: 0.0963 - val_acc: 1.0000\n",
      "\n",
      "EPOCH END\n",
      "\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 230s 22ms/step - loss: 0.1526 - acc: 0.9852 - val_loss: 0.1277 - val_acc: 0.9974\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 229s 22ms/step - loss: 0.1523 - acc: 0.9909 - val_loss: 0.1192 - val_acc: 0.9982\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 226s 22ms/step - loss: 0.1360 - acc: 0.9915 - val_loss: 0.1067 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 234s 23ms/step - loss: 0.1652 - acc: 0.9857 - val_loss: 0.1428 - val_acc: 0.9974\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 228s 22ms/step - loss: 0.1512 - acc: 0.9923 - val_loss: 0.1177 - val_acc: 0.9991\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 234s 23ms/step - loss: 0.1389 - acc: 0.9919 - val_loss: 0.1209 - val_acc: 0.9938\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 233s 23ms/step - loss: 0.1369 - acc: 0.9907 - val_loss: 0.1376 - val_acc: 0.9930\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1773 - acc: 0.9825 - val_loss: 0.1752 - val_acc: 0.9886\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 231s 23ms/step - loss: 0.1555 - acc: 0.9905 - val_loss: 0.1242 - val_acc: 0.9991\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 234s 23ms/step - loss: 0.1408 - acc: 0.9915 - val_loss: 0.1137 - val_acc: 1.0000\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 243s 24ms/step - loss: 0.1472 - acc: 0.9898 - val_loss: 0.1325 - val_acc: 0.9938\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 231s 23ms/step - loss: 0.1583 - acc: 0.9873 - val_loss: 0.1182 - val_acc: 0.9974\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1202 - acc: 0.9953 - val_loss: 0.1013 - val_acc: 0.9991\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 227s 22ms/step - loss: 0.1378 - acc: 0.9907 - val_loss: 0.1047 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 230s 22ms/step - loss: 0.1111 - acc: 0.9962 - val_loss: 0.0978 - val_acc: 0.9991\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1474 - acc: 0.9853 - val_loss: 0.1767 - val_acc: 0.9894\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 226s 22ms/step - loss: 0.1592 - acc: 0.9880 - val_loss: 0.1153 - val_acc: 0.9991\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1190 - acc: 0.9959 - val_loss: 0.2462 - val_acc: 0.9657\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1432 - acc: 0.9899 - val_loss: 0.1074 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 230s 22ms/step - loss: 0.1220 - acc: 0.9951 - val_loss: 0.1165 - val_acc: 0.9947\n",
      "\n",
      "Train on 10224 samples, validate on 1136 samples\n",
      "Epoch 1/5\n",
      "10224/10224 [==============================] - 234s 23ms/step - loss: 0.1630 - acc: 0.9853 - val_loss: 0.1832 - val_acc: 0.9886\n",
      "Epoch 2/5\n",
      "10224/10224 [==============================] - 233s 23ms/step - loss: 0.1468 - acc: 0.9913 - val_loss: 0.1118 - val_acc: 0.9991\n",
      "Epoch 3/5\n",
      "10224/10224 [==============================] - 226s 22ms/step - loss: 0.1304 - acc: 0.9933 - val_loss: 0.1065 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "10224/10224 [==============================] - 227s 22ms/step - loss: 0.1334 - acc: 0.9913 - val_loss: 0.1108 - val_acc: 0.9956\n",
      "Epoch 5/5\n",
      "10224/10224 [==============================] - 224s 22ms/step - loss: 0.1452 - acc: 0.9894 - val_loss: 0.1286 - val_acc: 0.9956\n",
      "\n",
      "EPOCH END\n",
      "\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1712 - acc: 0.9857 - val_loss: 0.1483 - val_acc: 0.9938\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1462 - acc: 0.9911 - val_loss: 0.1237 - val_acc: 0.9974\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1376 - acc: 0.9919 - val_loss: 0.1206 - val_acc: 0.9956\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1621 - acc: 0.9875 - val_loss: 0.1356 - val_acc: 0.9947\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 227s 22ms/step - loss: 0.1277 - acc: 0.9955 - val_loss: 0.1063 - val_acc: 1.0000\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1328 - acc: 0.9914 - val_loss: 0.1134 - val_acc: 0.9965\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1510 - acc: 0.9883 - val_loss: 0.1554 - val_acc: 0.9877\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1514 - acc: 0.9904 - val_loss: 0.1211 - val_acc: 0.9974\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1182 - acc: 0.9965 - val_loss: 0.1045 - val_acc: 0.9965\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1383 - acc: 0.9888 - val_loss: 0.1427 - val_acc: 0.9930\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1462 - acc: 0.9895 - val_loss: 0.1131 - val_acc: 0.9991\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1449 - acc: 0.9900 - val_loss: 0.1135 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1490 - acc: 0.9889 - val_loss: 0.1326 - val_acc: 0.9921\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1366 - acc: 0.9928 - val_loss: 0.1099 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1582 - acc: 0.9880 - val_loss: 0.1291 - val_acc: 0.9982\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1395 - acc: 0.9907 - val_loss: 0.1086 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1490 - acc: 0.9896 - val_loss: 0.1201 - val_acc: 0.9991\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1456 - acc: 0.9901 - val_loss: 0.1195 - val_acc: 0.9982\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 232s 23ms/step - loss: 0.1478 - acc: 0.9899 - val_loss: 0.1291 - val_acc: 0.9938\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 245s 24ms/step - loss: 0.1300 - acc: 0.9936 - val_loss: 0.1042 - val_acc: 1.0000\n",
      "\n",
      "Train on 10224 samples, validate on 1136 samples\n",
      "Epoch 1/5\n",
      "10224/10224 [==============================] - 243s 24ms/step - loss: 0.1417 - acc: 0.9897 - val_loss: 0.1196 - val_acc: 0.9974\n",
      "Epoch 2/5\n",
      "10224/10224 [==============================] - 2397s 234ms/step - loss: 0.1467 - acc: 0.9902 - val_loss: 0.1324 - val_acc: 0.9938\n",
      "Epoch 3/5\n",
      "10224/10224 [==============================] - 481s 47ms/step - loss: 0.1397 - acc: 0.9922 - val_loss: 0.1447 - val_acc: 0.9894\n",
      "Epoch 4/5\n",
      "10224/10224 [==============================] - 782s 76ms/step - loss: 0.1586 - acc: 0.9865 - val_loss: 0.1223 - val_acc: 0.9956\n",
      "Epoch 5/5\n",
      "10224/10224 [==============================] - 2403s 235ms/step - loss: 0.1432 - acc: 0.9912 - val_loss: 0.1129 - val_acc: 0.9991\n",
      "\n",
      "EPOCH END\n",
      "\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 275s 27ms/step - loss: 0.1363 - acc: 0.9917 - val_loss: 0.1073 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 267s 26ms/step - loss: 0.1325 - acc: 0.9920 - val_loss: 0.1097 - val_acc: 0.9991\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 256s 25ms/step - loss: 0.1341 - acc: 0.9922 - val_loss: 0.1113 - val_acc: 0.9982\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 253s 25ms/step - loss: 0.1359 - acc: 0.9908 - val_loss: 0.1135 - val_acc: 0.9974\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 247s 24ms/step - loss: 0.1604 - acc: 0.9869 - val_loss: 0.1418 - val_acc: 0.9965\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 245s 24ms/step - loss: 0.1360 - acc: 0.9934 - val_loss: 0.1108 - val_acc: 0.9991\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 2207s 216ms/step - loss: 0.1334 - acc: 0.9928 - val_loss: 0.1090 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 571s 56ms/step - loss: 0.1364 - acc: 0.9910 - val_loss: 0.1110 - val_acc: 0.9974\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 798s 78ms/step - loss: 0.1496 - acc: 0.9892 - val_loss: 0.1111 - val_acc: 0.9991\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 5440s 532ms/step - loss: 0.1256 - acc: 0.9940 - val_loss: 0.1031 - val_acc: 1.0000\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 473s 46ms/step - loss: 0.1381 - acc: 0.9900 - val_loss: 0.1227 - val_acc: 0.9938\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 753s 74ms/step - loss: 0.1646 - acc: 0.9846 - val_loss: 0.1276 - val_acc: 0.9982\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 7735s 756ms/step - loss: 0.1502 - acc: 0.9902 - val_loss: 0.1284 - val_acc: 0.9982\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 418s 41ms/step - loss: 0.1286 - acc: 0.9934 - val_loss: 0.1156 - val_acc: 0.9982\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 709s 69ms/step - loss: 0.1210 - acc: 0.9944 - val_loss: 0.1114 - val_acc: 0.9965\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10227/10227 [==============================] - 7810s 764ms/step - loss: 0.1174 - acc: 0.9935 - val_loss: 0.0955 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 348s 34ms/step - loss: 0.1352 - acc: 0.9875 - val_loss: 0.1170 - val_acc: 0.9974\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 696s 68ms/step - loss: 0.1484 - acc: 0.9891 - val_loss: 0.1117 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 7897s 772ms/step - loss: 0.1521 - acc: 0.9889 - val_loss: 0.1235 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 345s 34ms/step - loss: 0.1266 - acc: 0.9947 - val_loss: 0.1208 - val_acc: 0.9938\n",
      "\n",
      "Train on 10224 samples, validate on 1136 samples\n",
      "Epoch 1/5\n",
      "10224/10224 [==============================] - 595s 58ms/step - loss: 0.1477 - acc: 0.9882 - val_loss: 0.1247 - val_acc: 0.9982\n",
      "Epoch 2/5\n",
      "10224/10224 [==============================] - 775s 76ms/step - loss: 0.1508 - acc: 0.9908 - val_loss: 0.1138 - val_acc: 0.9982\n",
      "Epoch 3/5\n",
      "10224/10224 [==============================] - 7550s 738ms/step - loss: 0.1220 - acc: 0.9948 - val_loss: 0.1033 - val_acc: 0.9991\n",
      "Epoch 4/5\n",
      "10224/10224 [==============================] - 522s 51ms/step - loss: 0.1296 - acc: 0.9920 - val_loss: 0.1090 - val_acc: 0.9991\n",
      "Epoch 5/5\n",
      "10224/10224 [==============================] - 738s 72ms/step - loss: 0.1578 - acc: 0.9867 - val_loss: 0.1182 - val_acc: 0.9982\n",
      "\n",
      "EPOCH END\n",
      "\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 7664s 749ms/step - loss: 0.1253 - acc: 0.9940 - val_loss: 0.1030 - val_acc: 0.9991\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 436s 43ms/step - loss: 0.1273 - acc: 0.9921 - val_loss: 0.1138 - val_acc: 0.9982\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 687s 67ms/step - loss: 0.1169 - acc: 0.9953 - val_loss: 0.1083 - val_acc: 0.9956\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 7798s 762ms/step - loss: 0.1367 - acc: 0.9877 - val_loss: 0.1230 - val_acc: 0.9956\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 348s 34ms/step - loss: 0.1614 - acc: 0.9872 - val_loss: 0.1229 - val_acc: 0.9991\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 630s 62ms/step - loss: 0.1384 - acc: 0.9923 - val_loss: 0.1052 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 1857s 182ms/step - loss: 0.1153 - acc: 0.9957 - val_loss: 0.0962 - val_acc: 0.9991\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1174 - acc: 0.9931 - val_loss: 0.1114 - val_acc: 0.9930\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 226s 22ms/step - loss: 0.1411 - acc: 0.9876 - val_loss: 0.1276 - val_acc: 0.9930\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 226s 22ms/step - loss: 0.1323 - acc: 0.9914 - val_loss: 0.1072 - val_acc: 0.9991\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 226s 22ms/step - loss: 0.1321 - acc: 0.9910 - val_loss: 0.1102 - val_acc: 0.9991\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1561 - acc: 0.9849 - val_loss: 0.1258 - val_acc: 0.9947\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 230s 22ms/step - loss: 0.1390 - acc: 0.9919 - val_loss: 0.1093 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 234s 23ms/step - loss: 0.1253 - acc: 0.9934 - val_loss: 0.1090 - val_acc: 0.9982\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 234s 23ms/step - loss: 0.1240 - acc: 0.9934 - val_loss: 0.0974 - val_acc: 1.0000\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 229s 22ms/step - loss: 0.1447 - acc: 0.9875 - val_loss: 0.1152 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1417 - acc: 0.9924 - val_loss: 0.1059 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1375 - acc: 0.9921 - val_loss: 0.1378 - val_acc: 0.9938\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 233s 23ms/step - loss: 0.1310 - acc: 0.9930 - val_loss: 0.1011 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 231s 23ms/step - loss: 0.1319 - acc: 0.9921 - val_loss: 0.1082 - val_acc: 0.9974\n",
      "\n",
      "Train on 10224 samples, validate on 1136 samples\n",
      "Epoch 1/5\n",
      "10224/10224 [==============================] - 225s 22ms/step - loss: 0.1552 - acc: 0.9867 - val_loss: 0.1218 - val_acc: 0.9982\n",
      "Epoch 2/5\n",
      "10224/10224 [==============================] - 226s 22ms/step - loss: 0.1463 - acc: 0.9911 - val_loss: 0.1257 - val_acc: 0.9991\n",
      "Epoch 3/5\n",
      "10224/10224 [==============================] - 225s 22ms/step - loss: 0.1200 - acc: 0.9961 - val_loss: 0.0976 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "10224/10224 [==============================] - 225s 22ms/step - loss: 0.1096 - acc: 0.9950 - val_loss: 0.0986 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "10224/10224 [==============================] - 225s 22ms/step - loss: 0.1302 - acc: 0.9895 - val_loss: 0.1155 - val_acc: 0.9930\n",
      "\n",
      "EPOCH END\n",
      "\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 229s 22ms/step - loss: 0.1429 - acc: 0.9906 - val_loss: 0.1052 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 228s 22ms/step - loss: 0.1274 - acc: 0.9925 - val_loss: 0.1106 - val_acc: 0.9965\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1153 - acc: 0.9952 - val_loss: 0.1002 - val_acc: 0.9974\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1277 - acc: 0.9908 - val_loss: 0.1166 - val_acc: 0.9947\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1646 - acc: 0.9861 - val_loss: 0.1187 - val_acc: 0.9991\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1496 - acc: 0.9889 - val_loss: 0.1690 - val_acc: 0.9868\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1390 - acc: 0.9916 - val_loss: 0.1110 - val_acc: 0.9982\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1359 - acc: 0.9903 - val_loss: 0.1097 - val_acc: 0.9991\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1335 - acc: 0.9925 - val_loss: 0.1214 - val_acc: 0.9965\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 228s 22ms/step - loss: 0.1275 - acc: 0.9922 - val_loss: 0.1108 - val_acc: 0.9982\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1550 - acc: 0.9871 - val_loss: 0.1183 - val_acc: 0.9982\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 230s 22ms/step - loss: 0.1323 - acc: 0.9935 - val_loss: 0.1143 - val_acc: 0.9965\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1304 - acc: 0.9923 - val_loss: 0.1038 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 227s 22ms/step - loss: 0.1376 - acc: 0.9897 - val_loss: 0.1074 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1186 - acc: 0.9945 - val_loss: 0.0991 - val_acc: 0.9991\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1245 - acc: 0.9926 - val_loss: 0.1064 - val_acc: 0.9965\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1323 - acc: 0.9896 - val_loss: 0.1061 - val_acc: 0.9991\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1321 - acc: 0.9920 - val_loss: 0.0997 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 228s 22ms/step - loss: 0.1355 - acc: 0.9903 - val_loss: 0.1006 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 231s 23ms/step - loss: 0.1247 - acc: 0.9923 - val_loss: 0.1026 - val_acc: 0.9991\n",
      "\n",
      "Train on 10224 samples, validate on 1136 samples\n",
      "Epoch 1/5\n",
      "10224/10224 [==============================] - 225s 22ms/step - loss: 0.1420 - acc: 0.9894 - val_loss: 0.1121 - val_acc: 0.9982\n",
      "Epoch 2/5\n",
      "10224/10224 [==============================] - 225s 22ms/step - loss: 0.1776 - acc: 0.9834 - val_loss: 0.1216 - val_acc: 0.9982\n",
      "Epoch 3/5\n",
      "10224/10224 [==============================] - 224s 22ms/step - loss: 0.1244 - acc: 0.9957 - val_loss: 0.1057 - val_acc: 0.9991\n",
      "Epoch 4/5\n",
      "10224/10224 [==============================] - 225s 22ms/step - loss: 0.1188 - acc: 0.9948 - val_loss: 0.0960 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "10224/10224 [==============================] - 225s 22ms/step - loss: 0.1164 - acc: 0.9945 - val_loss: 0.0933 - val_acc: 0.9982\n",
      "\n",
      "EPOCH END\n",
      "\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 231s 23ms/step - loss: 0.1111 - acc: 0.9932 - val_loss: 0.0967 - val_acc: 0.9965\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 242s 24ms/step - loss: 0.1355 - acc: 0.9881 - val_loss: 0.1103 - val_acc: 0.9974\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 268s 26ms/step - loss: 0.1192 - acc: 0.9937 - val_loss: 0.1041 - val_acc: 0.9956\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 261s 26ms/step - loss: 0.1291 - acc: 0.9913 - val_loss: 0.0962 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 249s 24ms/step - loss: 0.1265 - acc: 0.9902 - val_loss: 0.0995 - val_acc: 1.0000\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 236s 23ms/step - loss: 0.1341 - acc: 0.9900 - val_loss: 0.1089 - val_acc: 0.9982\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 242s 24ms/step - loss: 0.1150 - acc: 0.9943 - val_loss: 0.0927 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 236s 23ms/step - loss: 0.1128 - acc: 0.9938 - val_loss: 0.0950 - val_acc: 0.9982\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 230s 22ms/step - loss: 0.1142 - acc: 0.9930 - val_loss: 0.0993 - val_acc: 0.9974\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1636 - acc: 0.9829 - val_loss: 0.1317 - val_acc: 0.9974\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 224s 22ms/step - loss: 0.1505 - acc: 0.9905 - val_loss: 0.1229 - val_acc: 0.9991\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 2198s 215ms/step - loss: 0.1448 - acc: 0.9932 - val_loss: 0.1087 - val_acc: 0.9991\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 2149s 210ms/step - loss: 0.1187 - acc: 0.9954 - val_loss: 0.0986 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 447s 44ms/step - loss: 0.1220 - acc: 0.9933 - val_loss: 0.0965 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 750s 73ms/step - loss: 0.1283 - acc: 0.9897 - val_loss: 0.1100 - val_acc: 0.9965\n",
      "\n",
      "Train on 10227 samples, validate on 1137 samples\n",
      "Epoch 1/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1530 - acc: 0.9864 - val_loss: 0.1121 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "10227/10227 [==============================] - 225s 22ms/step - loss: 0.1295 - acc: 0.9943 - val_loss: 0.1016 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "10227/10227 [==============================] - 7531s 736ms/step - loss: 0.1318 - acc: 0.9899 - val_loss: 0.1022 - val_acc: 0.9991\n",
      "Epoch 4/5\n",
      "10227/10227 [==============================] - 555s 54ms/step - loss: 0.1149 - acc: 0.9946 - val_loss: 0.0951 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "10227/10227 [==============================] - 793s 78ms/step - loss: 0.1098 - acc: 0.9963 - val_loss: 0.0934 - val_acc: 0.9991\n",
      "\n",
      "Train on 10224 samples, validate on 1136 samples\n",
      "Epoch 1/5\n",
      "10224/10224 [==============================] - 7622s 746ms/step - loss: 0.1235 - acc: 0.9912 - val_loss: 0.2109 - val_acc: 0.9815\n",
      "Epoch 2/5\n",
      "10224/10224 [==============================] - 529s 52ms/step - loss: 0.1304 - acc: 0.9910 - val_loss: 0.1213 - val_acc: 0.9965\n",
      "Epoch 3/5\n",
      "10224/10224 [==============================] - 789s 77ms/step - loss: 0.1378 - acc: 0.9892 - val_loss: 0.1189 - val_acc: 0.9956\n",
      "Epoch 4/5\n",
      "10224/10224 [==============================] - 7690s 752ms/step - loss: 0.1168 - acc: 0.9946 - val_loss: 0.1000 - val_acc: 0.9982\n",
      "Epoch 5/5\n",
      "10224/10224 [==============================] - 513s 50ms/step - loss: 0.1097 - acc: 0.9951 - val_loss: 0.0904 - val_acc: 1.0000\n",
      "\n",
      "EPOCH END\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calidad de la red:\n",
      "Loss: 0.09177039901016884\n",
      "Acc: 0.9992256247800071\n"
     ]
    }
   ],
   "source": [
    "def get_test_data():\n",
    "    test_images = []\n",
    "    for name in test_data:\n",
    "        image = skimage.data.imread(images_dataset[name][0])\n",
    "        if image.shape[0] > image.shape[1]:\n",
    "            image = skimage.transform.rotate(image, 90, resize=True)\n",
    "        image = skimage.transform.resize(image, image_size, mode='constant')\n",
    "        test_images.append(image)\n",
    "    test_images = np.asarray(test_images)\n",
    "    return test_images\n",
    "    \n",
    "def evalute_accuracy():\n",
    "    test_images = get_test_data()\n",
    "    \n",
    "    loss, acc = tf_model.evaluate(test_images, test_y, verbose=0)\n",
    "    return loss, acc\n",
    "\n",
    "loss, acc = evalute_accuracy()\n",
    "print(\"Calidad de la red:\\nLoss: {}\\nAcc: {}\".format(loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calidad de la red:<br>\n",
    "Loss: 0.09177039901016884<br>\n",
    "Acc: 0.9992256247800071<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
